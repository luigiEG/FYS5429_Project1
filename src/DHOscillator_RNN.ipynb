{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy, scipy, and matplotlib\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "#%matplotlib widget\n",
    "\n",
    "import os\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumped Harmonic Oscillator RNN model\n",
    "In this notebook, we will train a rNN with the loss function given by the MSE on the train points. The architecture, number of data point, epochs and hyperparamenters, all shared with the ../models/PINN_baseline_model.pt, will be the baseline for further improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Number of epochs\n",
    "n_epochs = 25000\n",
    "\n",
    "# Learning rate and scheduler\n",
    "lr = 0.01\n",
    "factor = 0.85 #\n",
    "patience = 200 #\n",
    "\n",
    "# Assumed values for the example\n",
    "n_layers = 3 # Number of LSTM layers\n",
    "n_neurons = 20 # Number of neurons in each LSTM layer\n",
    "input_size = 1 # The input feature size, here it's 1 because we only input time\n",
    "seq_len = 25 #50 # Sequence lenghts\n",
    "batch_size = batch_size = 600 #250 # Batch size\n",
    "\n",
    "# Model class\n",
    "class RNN_net(torch.nn.Module):\n",
    "    def __init__(self, n_layers, n_neurons, input_size):\n",
    "        super(RNN_net, self).__init__()\n",
    "        self.n_neurons = n_neurons\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Initializing the LSTM layer\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=n_neurons, num_layers=n_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer that outputs two values: position and velocity\n",
    "        self.fc = torch.nn.Linear(n_neurons, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialise hidden state and cell state with dimensions (num_layers, batch_size, n_neurons)\n",
    "        batch_size = x.size(0)\n",
    "        h0 = torch.zeros(self.n_layers, batch_size, self.n_neurons).to(x.device)\n",
    "        c0 = torch.zeros(self.n_layers, batch_size, self.n_neurons).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # Reshape output from (batch_size, seq_len, n_neurons) to (batch_size * seq_len, n_neurons)\n",
    "        out = out.contiguous().view(-1, self.n_neurons)\n",
    "        # Forward pass through the fully connected layer.\n",
    "        out = self.fc(out)\n",
    "        # Reshape back to (batch_size, seq_len, 2)\n",
    "        out = out.view(batch_size, -1, 2)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RNN_net model is designed to process sequential data and make predictions about the position and velocity of the data points. It accomplishes this by utilizing a recurrent neural network (RNN) architecture, specifically the Long Short-Term Memory (LSTM) layer.\n",
    "\n",
    "The model takes as input three parameters: the number of layers (n_layers), the number of neurons in each layer (n_neurons), and the input size (input_size). These parameters are used to initialize the LSTM layer and the fully connected layer (fc).\n",
    "\n",
    "During the forward pass, the input tensor x is fed into the LSTM layer. The LSTM layer makes use of hidden and cell state tensors to capture the temporal dependencies in the sequential data. These hidden and cell states are initialized based on the device of the input tensor.\n",
    "\n",
    "The output of the LSTM layer is reshaped into a 2D tensor to match the input shape of the fully connected layer. This allows the fully connected layer to process the output of the LSTM layer and generate predictions. The fully connected layer maps the hidden representations from the LSTM layer to two output values: position and velocity.\n",
    "\n",
    "The final step involves reshaping the output tensor back into its original shape, representing predictions for each input data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import data\n",
    "data = np.load('../data/DHOscillator_data.npy')\n",
    "X = data[:,0]\n",
    "Y = data[:,1:]\n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a step-by-step breakdown and code to prepare the data:\n",
    "\n",
    "Normalize or preprocess the data if necessary.\n",
    "Split the data into training, validation, and testing sets.\n",
    "Create input sequences (X_batch) and corresponding outputs (Y_batch) for each set.\n",
    "Convert the data into PyTorch tensors and create DataLoader instances for batch processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def create_sequences(X, Y, seq_len):\n",
    "    X_seq, Y_seq = [], []\n",
    "    for i in range(len(X) - seq_len + 1):   \n",
    "        X_seq.append(X[i:i+seq_len])\n",
    "        Y_seq.append(Y[i:i+seq_len])    \n",
    "    \n",
    "    return np.array(X_seq), np.array(Y_seq)\n",
    "\n",
    "total_time = 30\n",
    "cutoff_time = 0.85 * total_time  \n",
    "cutoff_index = np.argmax(X >= cutoff_time)  \n",
    "\n",
    "\n",
    "X_train_val = X[:cutoff_index]  \n",
    "Y_train_val = Y[:cutoff_index]  \n",
    "X_test = X[cutoff_index:]  \n",
    "Y_test = Y[cutoff_index:]  \n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.25, random_state=42)  \n",
    "\n",
    "\n",
    "# Creating sequences\n",
    "X_train_seq, Y_train_seq = create_sequences(X_train, Y_train, seq_len)\n",
    "X_val_seq, Y_val_seq = create_sequences(X_val, Y_val, seq_len)\n",
    "X_test_seq, Y_test_seq = create_sequences(X_test, Y_test, seq_len)\n",
    "\n",
    "# Converting numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train_seq.astype(np.float32)).unsqueeze(-1)\n",
    "Y_train_tensor = torch.from_numpy(Y_train_seq.astype(np.float32))\n",
    "X_val_tensor = torch.from_numpy(X_val_seq.astype(np.float32)).unsqueeze(-1)\n",
    "Y_val_tensor = torch.from_numpy(Y_val_seq.astype(np.float32))\n",
    "X_test_tensor = torch.from_numpy(X_test_seq.astype(np.float32)).unsqueeze(-1)\n",
    "Y_test_tensor = torch.from_numpy(Y_test_seq.astype(np.float32))\n",
    "\n",
    "# Creating TensorDatasets and DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, Y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, Y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, Y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extracting one batch of training data\n",
    "X_train_batch, Y_train_batch = next(iter(train_loader))\n",
    "X_train_batch_np = X_train_batch.detach().numpy()\n",
    "Y_train_batch_np = Y_train_batch.detach().numpy()\n",
    "\n",
    "X_val_batch, Y_val_batch = next(iter(val_loader))\n",
    "X_val_batch_np = X_val_batch.detach().numpy()\n",
    "Y_val_batch_np = Y_val_batch.detach().numpy()\n",
    "\n",
    "X_test_batch, Y_test_batch = next(iter(test_loader))\n",
    "X_test_batch_np = X_test_batch.detach().numpy()\n",
    "Y_test_batch_np = Y_test_batch.detach().numpy()\n",
    "\n",
    "sample_idx = 0\n",
    "\n",
    "X_train_sample = X_train_batch_np[sample_idx,:,0]\n",
    "Y_train_sample_position = Y_train_batch_np[sample_idx,:,0]\n",
    "Y_train_sample_velocity = Y_train_batch_np[sample_idx,:,1]\n",
    "X_val_sample = X_val_batch_np[sample_idx,:,0]\n",
    "Y_val_sample_position = Y_val_batch_np[sample_idx,:,0]\n",
    "Y_val_sample_velocity = Y_val_batch_np[sample_idx,:,1]\n",
    "#X_test_sample = X_test_batch_np[:,:,0]\n",
    "#Y_test_sample_position = Y_test_batch_np[:,:,0]\n",
    "#Y_test_sample_velocity = Y_test_batch_np[:,:,1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Position vs Time\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(X_train_sample, Y_train_sample_position, '.',label=\"train\")\n",
    "plt.plot(X_val_sample, Y_val_sample_position, '.',label=\"val\")\n",
    "plt.plot(X_test, Y_test[:,0], '.', label=\"test\")  \n",
    "plt.plot(X,Y[:,0],alpha=0.5)\n",
    "plt.axvline(x=30*0.85, color='r', linestyle='--')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.title(\"Sample Position Over Time\")\n",
    "plt.legend()\n",
    "\n",
    "# Velocity vs Time\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(X_train_sample, Y_train_sample_velocity, '.',label=\"train\")\n",
    "plt.plot(X_val_sample, Y_val_sample_velocity, '.',label=\"val\")\n",
    "plt.plot(X_test, Y_test[:,1], '.', label=\"test\")  \n",
    "plt.plot(X,Y[:,1],alpha=0.5)\n",
    "plt.axvline(x=30*0.85, color='r', linestyle='--')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Velocity\")\n",
    "plt.title(\"Sample Velocity Over Time\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "\n",
    "The RNN will be trained on data point with time < 25, discarding validation data this means 595 point of the ODE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25000, Training Loss: 0.183842, lr : 0.010000\n",
      "Epoch: 2/25000, Training Loss: 0.195565, lr : 0.010000\n",
      "Epoch: 3/25000, Training Loss: 0.185431, lr : 0.010000\n",
      "Epoch: 4/25000, Training Loss: 0.183849, lr : 0.010000\n",
      "Epoch: 5/25000, Training Loss: 0.186558, lr : 0.010000\n",
      "Epoch: 6/25000, Training Loss: 0.184635, lr : 0.010000\n",
      "Epoch: 7/25000, Training Loss: 0.178148, lr : 0.010000\n",
      "Epoch: 8/25000, Training Loss: 0.182552, lr : 0.010000\n",
      "Epoch: 9/25000, Training Loss: 0.172879, lr : 0.010000\n",
      "Epoch: 10/25000, Training Loss: 0.185512, lr : 0.010000\n",
      "Epoch: 11/25000, Training Loss: 0.182288, lr : 0.010000\n",
      "Epoch: 12/25000, Training Loss: 0.175373, lr : 0.010000\n",
      "Epoch: 13/25000, Training Loss: 0.185222, lr : 0.010000\n",
      "Epoch: 14/25000, Training Loss: 0.173410, lr : 0.010000\n",
      "Epoch: 15/25000, Training Loss: 0.182905, lr : 0.010000\n",
      "Epoch: 16/25000, Training Loss: 0.173769, lr : 0.010000\n",
      "Epoch: 17/25000, Training Loss: 0.181595, lr : 0.010000\n",
      "Epoch: 18/25000, Training Loss: 0.171743, lr : 0.010000\n",
      "Epoch: 19/25000, Training Loss: 0.173377, lr : 0.010000\n",
      "Epoch: 20/25000, Training Loss: 0.170151, lr : 0.010000\n",
      "Epoch: 21/25000, Training Loss: 0.161195, lr : 0.010000\n",
      "Epoch: 22/25000, Training Loss: 0.156053, lr : 0.010000\n",
      "Epoch: 23/25000, Training Loss: 0.158471, lr : 0.010000\n",
      "Epoch: 24/25000, Training Loss: 0.160117, lr : 0.010000\n",
      "Epoch: 25/25000, Training Loss: 0.153613, lr : 0.010000\n",
      "Epoch: 26/25000, Training Loss: 0.159579, lr : 0.010000\n",
      "Epoch: 27/25000, Training Loss: 0.151358, lr : 0.010000\n",
      "Epoch: 28/25000, Training Loss: 0.152610, lr : 0.010000\n",
      "Epoch: 29/25000, Training Loss: 0.152314, lr : 0.010000\n",
      "Epoch: 30/25000, Training Loss: 0.146509, lr : 0.010000\n",
      "Epoch: 31/25000, Training Loss: 0.146638, lr : 0.010000\n",
      "Epoch: 32/25000, Training Loss: 0.143553, lr : 0.010000\n",
      "Epoch: 33/25000, Training Loss: 0.135304, lr : 0.010000\n",
      "Epoch: 34/25000, Training Loss: 0.138337, lr : 0.010000\n",
      "Epoch: 35/25000, Training Loss: 0.135233, lr : 0.010000\n",
      "Epoch: 36/25000, Training Loss: 0.128160, lr : 0.010000\n",
      "Epoch: 37/25000, Training Loss: 0.123644, lr : 0.010000\n",
      "Epoch: 38/25000, Training Loss: 0.122546, lr : 0.010000\n",
      "Epoch: 39/25000, Training Loss: 0.122120, lr : 0.010000\n",
      "Epoch: 40/25000, Training Loss: 0.119713, lr : 0.010000\n",
      "Epoch: 41/25000, Training Loss: 0.115435, lr : 0.010000\n",
      "Epoch: 42/25000, Training Loss: 0.116907, lr : 0.010000\n",
      "Epoch: 43/25000, Training Loss: 0.112010, lr : 0.010000\n",
      "Epoch: 44/25000, Training Loss: 0.106633, lr : 0.010000\n",
      "Epoch: 45/25000, Training Loss: 0.105781, lr : 0.010000\n",
      "Epoch: 46/25000, Training Loss: 0.104002, lr : 0.010000\n",
      "Epoch: 47/25000, Training Loss: 0.099788, lr : 0.010000\n",
      "Epoch: 48/25000, Training Loss: 0.101524, lr : 0.010000\n",
      "Epoch: 49/25000, Training Loss: 0.100669, lr : 0.010000\n",
      "Epoch: 50/25000, Training Loss: 0.096292, lr : 0.010000\n",
      "Epoch: 51/25000, Training Loss: 0.095288, lr : 0.010000\n",
      "Epoch: 52/25000, Training Loss: 0.091561, lr : 0.010000\n",
      "Epoch: 53/25000, Training Loss: 0.088041, lr : 0.010000\n",
      "Epoch: 54/25000, Training Loss: 0.087047, lr : 0.010000\n",
      "Epoch: 55/25000, Training Loss: 0.085993, lr : 0.010000\n",
      "Epoch: 56/25000, Training Loss: 0.084258, lr : 0.010000\n",
      "Epoch: 57/25000, Training Loss: 0.085385, lr : 0.010000\n",
      "Epoch: 58/25000, Training Loss: 0.081749, lr : 0.010000\n",
      "Epoch: 59/25000, Training Loss: 0.082264, lr : 0.010000\n",
      "Epoch: 60/25000, Training Loss: 0.081978, lr : 0.010000\n",
      "Epoch: 61/25000, Training Loss: 0.082803, lr : 0.010000\n",
      "Epoch: 62/25000, Training Loss: 0.076227, lr : 0.010000\n",
      "Epoch: 63/25000, Training Loss: 0.075428, lr : 0.010000\n",
      "Epoch: 64/25000, Training Loss: 0.077745, lr : 0.010000\n",
      "Epoch: 65/25000, Training Loss: 0.074615, lr : 0.010000\n",
      "Epoch: 66/25000, Training Loss: 0.075755, lr : 0.010000\n",
      "Epoch: 67/25000, Training Loss: 0.075484, lr : 0.010000\n",
      "Epoch: 68/25000, Training Loss: 0.074600, lr : 0.010000\n",
      "Epoch: 69/25000, Training Loss: 0.073434, lr : 0.010000\n",
      "Epoch: 70/25000, Training Loss: 0.072913, lr : 0.010000\n",
      "Epoch: 71/25000, Training Loss: 0.072493, lr : 0.010000\n",
      "Epoch: 72/25000, Training Loss: 0.064584, lr : 0.010000\n",
      "Epoch: 73/25000, Training Loss: 0.070031, lr : 0.010000\n",
      "Epoch: 74/25000, Training Loss: 0.065416, lr : 0.010000\n",
      "Epoch: 75/25000, Training Loss: 0.068027, lr : 0.010000\n",
      "Epoch: 76/25000, Training Loss: 0.065272, lr : 0.010000\n",
      "Epoch: 77/25000, Training Loss: 0.063941, lr : 0.010000\n",
      "Epoch: 78/25000, Training Loss: 0.064163, lr : 0.010000\n",
      "Epoch: 79/25000, Training Loss: 0.063435, lr : 0.010000\n",
      "Epoch: 80/25000, Training Loss: 0.059991, lr : 0.010000\n",
      "Epoch: 81/25000, Training Loss: 0.057137, lr : 0.010000\n",
      "Epoch: 82/25000, Training Loss: 0.058063, lr : 0.010000\n",
      "Epoch: 83/25000, Training Loss: 0.056670, lr : 0.010000\n",
      "Epoch: 84/25000, Training Loss: 0.058361, lr : 0.010000\n",
      "Epoch: 85/25000, Training Loss: 0.054428, lr : 0.010000\n",
      "Epoch: 86/25000, Training Loss: 0.054399, lr : 0.010000\n",
      "Epoch: 87/25000, Training Loss: 0.052658, lr : 0.010000\n",
      "Epoch: 88/25000, Training Loss: 0.054270, lr : 0.010000\n",
      "Epoch: 89/25000, Training Loss: 0.051208, lr : 0.010000\n",
      "Epoch: 90/25000, Training Loss: 0.050089, lr : 0.010000\n",
      "Epoch: 91/25000, Training Loss: 0.048102, lr : 0.010000\n",
      "Epoch: 92/25000, Training Loss: 0.049266, lr : 0.010000\n",
      "Epoch: 93/25000, Training Loss: 0.046926, lr : 0.010000\n",
      "Epoch: 94/25000, Training Loss: 0.047571, lr : 0.010000\n",
      "Epoch: 95/25000, Training Loss: 0.044035, lr : 0.010000\n",
      "Epoch: 96/25000, Training Loss: 0.044720, lr : 0.010000\n",
      "Epoch: 97/25000, Training Loss: 0.047812, lr : 0.010000\n",
      "Epoch: 98/25000, Training Loss: 0.043985, lr : 0.010000\n",
      "Epoch: 99/25000, Training Loss: 0.043444, lr : 0.010000\n",
      "Epoch: 100/25000, Training Loss: 0.042662, lr : 0.010000\n",
      "Epoch: 101/25000, Training Loss: 0.039290, lr : 0.010000\n",
      "Epoch: 102/25000, Training Loss: 0.040639, lr : 0.010000\n",
      "Epoch: 103/25000, Training Loss: 0.041162, lr : 0.010000\n",
      "Epoch: 104/25000, Training Loss: 0.037590, lr : 0.010000\n",
      "Epoch: 105/25000, Training Loss: 0.038526, lr : 0.010000\n",
      "Epoch: 106/25000, Training Loss: 0.040491, lr : 0.010000\n",
      "Epoch: 107/25000, Training Loss: 0.037057, lr : 0.010000\n",
      "Epoch: 108/25000, Training Loss: 0.037289, lr : 0.010000\n",
      "Epoch: 109/25000, Training Loss: 0.037626, lr : 0.010000\n",
      "Epoch: 110/25000, Training Loss: 0.037894, lr : 0.010000\n",
      "Epoch: 111/25000, Training Loss: 0.034528, lr : 0.010000\n",
      "Epoch: 112/25000, Training Loss: 0.035091, lr : 0.010000\n",
      "Epoch: 113/25000, Training Loss: 0.033420, lr : 0.010000\n",
      "Epoch: 114/25000, Training Loss: 0.034270, lr : 0.010000\n",
      "Epoch: 115/25000, Training Loss: 0.032537, lr : 0.010000\n",
      "Epoch: 116/25000, Training Loss: 0.033082, lr : 0.010000\n",
      "Epoch: 117/25000, Training Loss: 0.031628, lr : 0.010000\n",
      "Epoch: 118/25000, Training Loss: 0.030858, lr : 0.010000\n",
      "Epoch: 119/25000, Training Loss: 0.032364, lr : 0.010000\n",
      "Epoch: 120/25000, Training Loss: 0.031977, lr : 0.010000\n",
      "Epoch: 121/25000, Training Loss: 0.026953, lr : 0.010000\n",
      "Epoch: 122/25000, Training Loss: 0.029646, lr : 0.010000\n",
      "Epoch: 123/25000, Training Loss: 0.028959, lr : 0.010000\n",
      "Epoch: 124/25000, Training Loss: 0.029279, lr : 0.010000\n",
      "Epoch: 125/25000, Training Loss: 0.027917, lr : 0.010000\n",
      "Epoch: 126/25000, Training Loss: 0.029044, lr : 0.010000\n",
      "Epoch: 127/25000, Training Loss: 0.026429, lr : 0.010000\n",
      "Epoch: 128/25000, Training Loss: 0.027101, lr : 0.010000\n",
      "Epoch: 129/25000, Training Loss: 0.027010, lr : 0.010000\n",
      "Epoch: 130/25000, Training Loss: 0.027081, lr : 0.010000\n",
      "Epoch: 131/25000, Training Loss: 0.024918, lr : 0.010000\n",
      "Epoch: 132/25000, Training Loss: 0.027246, lr : 0.010000\n",
      "Epoch: 133/25000, Training Loss: 0.023455, lr : 0.010000\n",
      "Epoch: 134/25000, Training Loss: 0.023480, lr : 0.010000\n",
      "Epoch: 135/25000, Training Loss: 0.025309, lr : 0.010000\n",
      "Epoch: 136/25000, Training Loss: 0.025739, lr : 0.010000\n",
      "Epoch: 137/25000, Training Loss: 0.024667, lr : 0.010000\n",
      "Epoch: 138/25000, Training Loss: 0.021411, lr : 0.010000\n",
      "Epoch: 139/25000, Training Loss: 0.021842, lr : 0.010000\n",
      "Epoch: 140/25000, Training Loss: 0.024498, lr : 0.010000\n",
      "Epoch: 141/25000, Training Loss: 0.024072, lr : 0.010000\n",
      "Epoch: 142/25000, Training Loss: 0.023192, lr : 0.010000\n",
      "Epoch: 143/25000, Training Loss: 0.026313, lr : 0.010000\n",
      "Epoch: 144/25000, Training Loss: 0.023086, lr : 0.010000\n",
      "Epoch: 145/25000, Training Loss: 0.022734, lr : 0.010000\n",
      "Epoch: 146/25000, Training Loss: 0.022506, lr : 0.010000\n",
      "Epoch: 147/25000, Training Loss: 0.021771, lr : 0.010000\n",
      "Epoch: 148/25000, Training Loss: 0.022483, lr : 0.010000\n",
      "Epoch: 149/25000, Training Loss: 0.022293, lr : 0.010000\n",
      "Epoch: 150/25000, Training Loss: 0.021585, lr : 0.010000\n",
      "Epoch: 151/25000, Training Loss: 0.021604, lr : 0.010000\n",
      "Epoch: 152/25000, Training Loss: 0.020264, lr : 0.010000\n",
      "Epoch: 153/25000, Training Loss: 0.021850, lr : 0.010000\n",
      "Epoch: 154/25000, Training Loss: 0.020069, lr : 0.010000\n",
      "Epoch: 155/25000, Training Loss: 0.021860, lr : 0.010000\n",
      "Epoch: 156/25000, Training Loss: 0.019932, lr : 0.010000\n",
      "Epoch: 157/25000, Training Loss: 0.020009, lr : 0.010000\n",
      "Epoch: 158/25000, Training Loss: 0.018686, lr : 0.010000\n",
      "Epoch: 159/25000, Training Loss: 0.018395, lr : 0.010000\n",
      "Epoch: 160/25000, Training Loss: 0.018054, lr : 0.010000\n",
      "Epoch: 161/25000, Training Loss: 0.018241, lr : 0.010000\n",
      "Epoch: 162/25000, Training Loss: 0.019893, lr : 0.010000\n",
      "Epoch: 163/25000, Training Loss: 0.019633, lr : 0.010000\n",
      "Epoch: 164/25000, Training Loss: 0.017542, lr : 0.010000\n",
      "Epoch: 165/25000, Training Loss: 0.018975, lr : 0.010000\n",
      "Epoch: 166/25000, Training Loss: 0.017508, lr : 0.010000\n",
      "Epoch: 167/25000, Training Loss: 0.018225, lr : 0.010000\n",
      "Epoch: 168/25000, Training Loss: 0.016560, lr : 0.010000\n",
      "Epoch: 169/25000, Training Loss: 0.016116, lr : 0.010000\n",
      "Epoch: 170/25000, Training Loss: 0.016898, lr : 0.010000\n",
      "Epoch: 171/25000, Training Loss: 0.017611, lr : 0.010000\n",
      "Epoch: 172/25000, Training Loss: 0.015694, lr : 0.010000\n",
      "Epoch: 173/25000, Training Loss: 0.016003, lr : 0.010000\n",
      "Epoch: 174/25000, Training Loss: 0.016415, lr : 0.010000\n",
      "Epoch: 175/25000, Training Loss: 0.016032, lr : 0.010000\n",
      "Epoch: 176/25000, Training Loss: 0.016615, lr : 0.010000\n",
      "Epoch: 177/25000, Training Loss: 0.014812, lr : 0.010000\n",
      "Epoch: 178/25000, Training Loss: 0.015674, lr : 0.010000\n",
      "Epoch: 179/25000, Training Loss: 0.016289, lr : 0.010000\n",
      "Epoch: 180/25000, Training Loss: 0.014696, lr : 0.010000\n",
      "Epoch: 181/25000, Training Loss: 0.017026, lr : 0.010000\n",
      "Epoch: 182/25000, Training Loss: 0.017344, lr : 0.010000\n",
      "Epoch: 183/25000, Training Loss: 0.016011, lr : 0.010000\n",
      "Epoch: 184/25000, Training Loss: 0.013923, lr : 0.010000\n",
      "Epoch: 185/25000, Training Loss: 0.014934, lr : 0.010000\n",
      "Epoch: 186/25000, Training Loss: 0.014019, lr : 0.010000\n",
      "Epoch: 187/25000, Training Loss: 0.015799, lr : 0.010000\n",
      "Epoch: 188/25000, Training Loss: 0.016533, lr : 0.010000\n",
      "Epoch: 189/25000, Training Loss: 0.015534, lr : 0.010000\n",
      "Epoch: 190/25000, Training Loss: 0.013637, lr : 0.010000\n",
      "Epoch: 191/25000, Training Loss: 0.014136, lr : 0.010000\n",
      "Epoch: 192/25000, Training Loss: 0.013127, lr : 0.010000\n",
      "Epoch: 193/25000, Training Loss: 0.013207, lr : 0.010000\n",
      "Epoch: 194/25000, Training Loss: 0.015574, lr : 0.010000\n",
      "Epoch: 195/25000, Training Loss: 0.013491, lr : 0.010000\n",
      "Epoch: 196/25000, Training Loss: 0.013518, lr : 0.010000\n",
      "Epoch: 197/25000, Training Loss: 0.013890, lr : 0.010000\n",
      "Epoch: 198/25000, Training Loss: 0.013193, lr : 0.010000\n",
      "Epoch: 199/25000, Training Loss: 0.013784, lr : 0.010000\n",
      "Epoch: 200/25000, Training Loss: 0.014420, lr : 0.010000\n",
      "Epoch: 201/25000, Training Loss: 0.013988, lr : 0.010000\n",
      "Epoch: 202/25000, Training Loss: 0.013148, lr : 0.010000\n",
      "Epoch: 203/25000, Training Loss: 0.013198, lr : 0.010000\n",
      "Epoch: 204/25000, Training Loss: 0.011488, lr : 0.010000\n",
      "Epoch: 205/25000, Training Loss: 0.012677, lr : 0.010000\n",
      "Epoch: 206/25000, Training Loss: 0.012441, lr : 0.010000\n",
      "Epoch: 207/25000, Training Loss: 0.012920, lr : 0.010000\n",
      "Epoch: 208/25000, Training Loss: 0.012534, lr : 0.010000\n",
      "Epoch: 209/25000, Training Loss: 0.012298, lr : 0.010000\n",
      "Epoch: 210/25000, Training Loss: 0.011927, lr : 0.010000\n",
      "Epoch: 211/25000, Training Loss: 0.013686, lr : 0.010000\n",
      "Epoch: 212/25000, Training Loss: 0.011999, lr : 0.010000\n",
      "Epoch: 213/25000, Training Loss: 0.013170, lr : 0.010000\n",
      "Epoch: 214/25000, Training Loss: 0.013263, lr : 0.010000\n",
      "Epoch: 215/25000, Training Loss: 0.012282, lr : 0.010000\n",
      "Epoch: 216/25000, Training Loss: 0.012440, lr : 0.010000\n",
      "Epoch: 217/25000, Training Loss: 0.011712, lr : 0.010000\n",
      "Epoch: 218/25000, Training Loss: 0.012454, lr : 0.010000\n",
      "Epoch: 219/25000, Training Loss: 0.011773, lr : 0.010000\n",
      "Epoch: 220/25000, Training Loss: 0.011653, lr : 0.010000\n",
      "Epoch: 221/25000, Training Loss: 0.013010, lr : 0.010000\n",
      "Epoch: 222/25000, Training Loss: 0.011651, lr : 0.010000\n",
      "Epoch: 223/25000, Training Loss: 0.010375, lr : 0.010000\n",
      "Epoch: 224/25000, Training Loss: 0.012394, lr : 0.010000\n",
      "Epoch: 225/25000, Training Loss: 0.013118, lr : 0.010000\n",
      "Epoch: 226/25000, Training Loss: 0.011136, lr : 0.010000\n",
      "Epoch: 227/25000, Training Loss: 0.011513, lr : 0.010000\n",
      "Epoch: 228/25000, Training Loss: 0.012155, lr : 0.010000\n",
      "Epoch: 229/25000, Training Loss: 0.011554, lr : 0.010000\n",
      "Epoch: 230/25000, Training Loss: 0.011337, lr : 0.010000\n",
      "Epoch: 231/25000, Training Loss: 0.010796, lr : 0.010000\n",
      "Epoch: 232/25000, Training Loss: 0.010547, lr : 0.010000\n",
      "Epoch: 233/25000, Training Loss: 0.011042, lr : 0.010000\n",
      "Epoch: 234/25000, Training Loss: 0.010014, lr : 0.010000\n",
      "Epoch: 235/25000, Training Loss: 0.011511, lr : 0.010000\n",
      "Epoch: 236/25000, Training Loss: 0.011848, lr : 0.010000\n",
      "Epoch: 237/25000, Training Loss: 0.009516, lr : 0.010000\n",
      "Epoch: 238/25000, Training Loss: 0.011777, lr : 0.010000\n",
      "Epoch: 239/25000, Training Loss: 0.009504, lr : 0.010000\n",
      "Epoch: 240/25000, Training Loss: 0.012519, lr : 0.010000\n",
      "Epoch: 241/25000, Training Loss: 0.010786, lr : 0.010000\n",
      "Epoch: 242/25000, Training Loss: 0.010607, lr : 0.010000\n",
      "Epoch: 243/25000, Training Loss: 0.010439, lr : 0.010000\n",
      "Epoch: 244/25000, Training Loss: 0.009941, lr : 0.010000\n",
      "Epoch: 245/25000, Training Loss: 0.009708, lr : 0.010000\n",
      "Epoch: 246/25000, Training Loss: 0.008947, lr : 0.010000\n",
      "Epoch: 247/25000, Training Loss: 0.010734, lr : 0.010000\n",
      "Epoch: 248/25000, Training Loss: 0.010249, lr : 0.010000\n",
      "Epoch: 249/25000, Training Loss: 0.009444, lr : 0.010000\n",
      "Epoch: 250/25000, Training Loss: 0.010505, lr : 0.010000\n",
      "Epoch: 251/25000, Training Loss: 0.008532, lr : 0.010000\n",
      "Epoch: 252/25000, Training Loss: 0.008555, lr : 0.010000\n",
      "Epoch: 253/25000, Training Loss: 0.008100, lr : 0.010000\n",
      "Epoch: 254/25000, Training Loss: 0.008553, lr : 0.010000\n",
      "Epoch: 255/25000, Training Loss: 0.010329, lr : 0.010000\n",
      "Epoch: 256/25000, Training Loss: 0.009301, lr : 0.010000\n",
      "Epoch: 257/25000, Training Loss: 0.009478, lr : 0.010000\n",
      "Epoch: 258/25000, Training Loss: 0.009748, lr : 0.010000\n",
      "Epoch: 259/25000, Training Loss: 0.009376, lr : 0.010000\n",
      "Epoch: 260/25000, Training Loss: 0.009108, lr : 0.010000\n",
      "Epoch: 261/25000, Training Loss: 0.008805, lr : 0.010000\n",
      "Epoch: 262/25000, Training Loss: 0.009323, lr : 0.010000\n",
      "Epoch: 263/25000, Training Loss: 0.009618, lr : 0.010000\n",
      "Epoch: 264/25000, Training Loss: 0.009998, lr : 0.010000\n",
      "Epoch: 265/25000, Training Loss: 0.009522, lr : 0.010000\n",
      "Epoch: 266/25000, Training Loss: 0.008151, lr : 0.010000\n",
      "Epoch: 267/25000, Training Loss: 0.008571, lr : 0.010000\n",
      "Epoch: 268/25000, Training Loss: 0.008096, lr : 0.010000\n",
      "Epoch: 269/25000, Training Loss: 0.008660, lr : 0.010000\n",
      "Epoch: 270/25000, Training Loss: 0.008904, lr : 0.010000\n",
      "Epoch: 271/25000, Training Loss: 0.008459, lr : 0.010000\n",
      "Epoch: 272/25000, Training Loss: 0.008575, lr : 0.010000\n",
      "Epoch: 273/25000, Training Loss: 0.008371, lr : 0.010000\n",
      "Epoch: 274/25000, Training Loss: 0.008523, lr : 0.010000\n",
      "Epoch: 275/25000, Training Loss: 0.008568, lr : 0.010000\n",
      "Epoch: 276/25000, Training Loss: 0.007101, lr : 0.010000\n",
      "Epoch: 277/25000, Training Loss: 0.008525, lr : 0.010000\n",
      "Epoch: 278/25000, Training Loss: 0.007978, lr : 0.010000\n",
      "Epoch: 279/25000, Training Loss: 0.010241, lr : 0.010000\n",
      "Epoch: 280/25000, Training Loss: 0.007453, lr : 0.010000\n",
      "Epoch: 281/25000, Training Loss: 0.008276, lr : 0.010000\n",
      "Epoch: 282/25000, Training Loss: 0.008439, lr : 0.010000\n",
      "Epoch: 283/25000, Training Loss: 0.008270, lr : 0.010000\n",
      "Epoch: 284/25000, Training Loss: 0.007998, lr : 0.010000\n",
      "Epoch: 285/25000, Training Loss: 0.007834, lr : 0.010000\n",
      "Epoch: 286/25000, Training Loss: 0.007846, lr : 0.010000\n",
      "Epoch: 287/25000, Training Loss: 0.008314, lr : 0.010000\n",
      "Epoch: 288/25000, Training Loss: 0.008959, lr : 0.010000\n",
      "Epoch: 289/25000, Training Loss: 0.008315, lr : 0.010000\n",
      "Epoch: 290/25000, Training Loss: 0.008595, lr : 0.010000\n",
      "Epoch: 291/25000, Training Loss: 0.008026, lr : 0.010000\n",
      "Epoch: 292/25000, Training Loss: 0.007647, lr : 0.010000\n",
      "Epoch: 293/25000, Training Loss: 0.009162, lr : 0.010000\n",
      "Epoch: 294/25000, Training Loss: 0.006920, lr : 0.010000\n",
      "Epoch: 295/25000, Training Loss: 0.008456, lr : 0.010000\n",
      "Epoch: 296/25000, Training Loss: 0.009431, lr : 0.010000\n",
      "Epoch: 297/25000, Training Loss: 0.008332, lr : 0.010000\n",
      "Epoch: 298/25000, Training Loss: 0.009148, lr : 0.010000\n",
      "Epoch: 299/25000, Training Loss: 0.008998, lr : 0.010000\n",
      "Epoch: 300/25000, Training Loss: 0.006825, lr : 0.010000\n",
      "Epoch: 301/25000, Training Loss: 0.008332, lr : 0.010000\n",
      "Epoch: 302/25000, Training Loss: 0.008575, lr : 0.010000\n",
      "Epoch: 303/25000, Training Loss: 0.009145, lr : 0.010000\n",
      "Epoch: 304/25000, Training Loss: 0.009442, lr : 0.010000\n",
      "Epoch: 305/25000, Training Loss: 0.008175, lr : 0.010000\n",
      "Epoch: 306/25000, Training Loss: 0.007545, lr : 0.010000\n",
      "Epoch: 307/25000, Training Loss: 0.007851, lr : 0.010000\n",
      "Epoch: 308/25000, Training Loss: 0.007873, lr : 0.010000\n",
      "Epoch: 309/25000, Training Loss: 0.007851, lr : 0.010000\n",
      "Epoch: 310/25000, Training Loss: 0.007640, lr : 0.010000\n",
      "Epoch: 311/25000, Training Loss: 0.008048, lr : 0.010000\n",
      "Epoch: 312/25000, Training Loss: 0.007562, lr : 0.010000\n",
      "Epoch: 313/25000, Training Loss: 0.007510, lr : 0.010000\n",
      "Epoch: 314/25000, Training Loss: 0.007616, lr : 0.010000\n",
      "Epoch: 315/25000, Training Loss: 0.007959, lr : 0.010000\n",
      "Epoch: 316/25000, Training Loss: 0.008586, lr : 0.010000\n",
      "Epoch: 317/25000, Training Loss: 0.008689, lr : 0.010000\n",
      "Epoch: 318/25000, Training Loss: 0.008240, lr : 0.010000\n",
      "Epoch: 319/25000, Training Loss: 0.008633, lr : 0.010000\n",
      "Epoch: 320/25000, Training Loss: 0.007014, lr : 0.010000\n",
      "Epoch: 321/25000, Training Loss: 0.007614, lr : 0.010000\n",
      "Epoch: 322/25000, Training Loss: 0.008011, lr : 0.010000\n",
      "Epoch: 323/25000, Training Loss: 0.007739, lr : 0.010000\n",
      "Epoch: 324/25000, Training Loss: 0.007265, lr : 0.010000\n",
      "Epoch: 325/25000, Training Loss: 0.007888, lr : 0.010000\n",
      "Epoch: 326/25000, Training Loss: 0.008444, lr : 0.010000\n",
      "Epoch: 327/25000, Training Loss: 0.007167, lr : 0.010000\n",
      "Epoch: 328/25000, Training Loss: 0.006990, lr : 0.010000\n",
      "Epoch: 329/25000, Training Loss: 0.007197, lr : 0.010000\n",
      "Epoch: 330/25000, Training Loss: 0.008314, lr : 0.010000\n",
      "Epoch: 331/25000, Training Loss: 0.007031, lr : 0.010000\n",
      "Epoch: 332/25000, Training Loss: 0.007024, lr : 0.010000\n",
      "Epoch: 333/25000, Training Loss: 0.006644, lr : 0.010000\n",
      "Epoch: 334/25000, Training Loss: 0.006789, lr : 0.010000\n",
      "Epoch: 335/25000, Training Loss: 0.007346, lr : 0.010000\n",
      "Epoch: 336/25000, Training Loss: 0.006980, lr : 0.010000\n",
      "Epoch: 337/25000, Training Loss: 0.007748, lr : 0.010000\n",
      "Epoch: 338/25000, Training Loss: 0.007044, lr : 0.010000\n",
      "Epoch: 339/25000, Training Loss: 0.006965, lr : 0.010000\n",
      "Epoch: 340/25000, Training Loss: 0.007530, lr : 0.010000\n",
      "Epoch: 341/25000, Training Loss: 0.006381, lr : 0.010000\n",
      "Epoch: 342/25000, Training Loss: 0.006579, lr : 0.010000\n",
      "Epoch: 343/25000, Training Loss: 0.007169, lr : 0.010000\n",
      "Epoch: 344/25000, Training Loss: 0.006543, lr : 0.010000\n",
      "Epoch: 345/25000, Training Loss: 0.006401, lr : 0.010000\n",
      "Epoch: 346/25000, Training Loss: 0.006574, lr : 0.010000\n",
      "Epoch: 347/25000, Training Loss: 0.006928, lr : 0.010000\n",
      "Epoch: 348/25000, Training Loss: 0.006589, lr : 0.010000\n",
      "Epoch: 349/25000, Training Loss: 0.007148, lr : 0.010000\n",
      "Epoch: 350/25000, Training Loss: 0.007065, lr : 0.010000\n",
      "Epoch: 351/25000, Training Loss: 0.007511, lr : 0.010000\n",
      "Epoch: 352/25000, Training Loss: 0.008137, lr : 0.010000\n",
      "Epoch: 353/25000, Training Loss: 0.006860, lr : 0.010000\n",
      "Epoch: 354/25000, Training Loss: 0.007962, lr : 0.010000\n",
      "Epoch: 355/25000, Training Loss: 0.007095, lr : 0.010000\n",
      "Epoch: 356/25000, Training Loss: 0.006355, lr : 0.010000\n",
      "Epoch: 357/25000, Training Loss: 0.007018, lr : 0.010000\n",
      "Epoch: 358/25000, Training Loss: 0.006507, lr : 0.010000\n",
      "Epoch: 359/25000, Training Loss: 0.005822, lr : 0.010000\n",
      "Epoch: 360/25000, Training Loss: 0.006827, lr : 0.010000\n",
      "Epoch: 361/25000, Training Loss: 0.006609, lr : 0.010000\n",
      "Epoch: 362/25000, Training Loss: 0.006266, lr : 0.010000\n",
      "Epoch: 363/25000, Training Loss: 0.005477, lr : 0.010000\n",
      "Epoch: 364/25000, Training Loss: 0.006248, lr : 0.010000\n",
      "Epoch: 365/25000, Training Loss: 0.006819, lr : 0.010000\n",
      "Epoch: 366/25000, Training Loss: 0.007724, lr : 0.010000\n",
      "Epoch: 367/25000, Training Loss: 0.006278, lr : 0.010000\n",
      "Epoch: 368/25000, Training Loss: 0.006079, lr : 0.010000\n",
      "Epoch: 369/25000, Training Loss: 0.006890, lr : 0.010000\n",
      "Epoch: 370/25000, Training Loss: 0.006047, lr : 0.010000\n",
      "Epoch: 371/25000, Training Loss: 0.005877, lr : 0.010000\n",
      "Epoch: 372/25000, Training Loss: 0.006228, lr : 0.010000\n",
      "Epoch: 373/25000, Training Loss: 0.006177, lr : 0.010000\n",
      "Epoch: 374/25000, Training Loss: 0.006206, lr : 0.010000\n",
      "Epoch: 375/25000, Training Loss: 0.007056, lr : 0.010000\n",
      "Epoch: 376/25000, Training Loss: 0.006116, lr : 0.010000\n",
      "Epoch: 377/25000, Training Loss: 0.005921, lr : 0.010000\n",
      "Epoch: 378/25000, Training Loss: 0.006968, lr : 0.010000\n",
      "Epoch: 379/25000, Training Loss: 0.006992, lr : 0.010000\n",
      "Epoch: 380/25000, Training Loss: 0.006014, lr : 0.010000\n",
      "Epoch: 381/25000, Training Loss: 0.006433, lr : 0.010000\n",
      "Epoch: 382/25000, Training Loss: 0.006703, lr : 0.010000\n",
      "Epoch: 383/25000, Training Loss: 0.005750, lr : 0.010000\n",
      "Epoch: 384/25000, Training Loss: 0.005490, lr : 0.010000\n",
      "Epoch: 385/25000, Training Loss: 0.005274, lr : 0.010000\n",
      "Epoch: 386/25000, Training Loss: 0.006326, lr : 0.010000\n",
      "Epoch: 387/25000, Training Loss: 0.005846, lr : 0.010000\n",
      "Epoch: 388/25000, Training Loss: 0.007118, lr : 0.010000\n",
      "Epoch: 389/25000, Training Loss: 0.006312, lr : 0.010000\n",
      "Epoch: 390/25000, Training Loss: 0.006542, lr : 0.010000\n",
      "Epoch: 391/25000, Training Loss: 0.005933, lr : 0.010000\n",
      "Epoch: 392/25000, Training Loss: 0.007064, lr : 0.010000\n",
      "Epoch: 393/25000, Training Loss: 0.006581, lr : 0.010000\n",
      "Epoch: 394/25000, Training Loss: 0.006451, lr : 0.010000\n",
      "Epoch: 395/25000, Training Loss: 0.007086, lr : 0.010000\n",
      "Epoch: 396/25000, Training Loss: 0.006863, lr : 0.010000\n",
      "Epoch: 397/25000, Training Loss: 0.006243, lr : 0.010000\n",
      "Epoch: 398/25000, Training Loss: 0.006070, lr : 0.010000\n",
      "Epoch: 399/25000, Training Loss: 0.005975, lr : 0.010000\n",
      "Epoch: 400/25000, Training Loss: 0.007415, lr : 0.010000\n",
      "Epoch: 401/25000, Training Loss: 0.007392, lr : 0.010000\n",
      "Epoch: 402/25000, Training Loss: 0.006116, lr : 0.010000\n",
      "Epoch: 403/25000, Training Loss: 0.005973, lr : 0.010000\n",
      "Epoch: 404/25000, Training Loss: 0.006945, lr : 0.010000\n",
      "Epoch: 405/25000, Training Loss: 0.006132, lr : 0.010000\n",
      "Epoch: 406/25000, Training Loss: 0.006450, lr : 0.010000\n",
      "Epoch: 407/25000, Training Loss: 0.006870, lr : 0.010000\n",
      "Epoch: 408/25000, Training Loss: 0.005563, lr : 0.010000\n",
      "Epoch: 409/25000, Training Loss: 0.006675, lr : 0.010000\n",
      "Epoch: 410/25000, Training Loss: 0.005986, lr : 0.010000\n",
      "Epoch: 411/25000, Training Loss: 0.006478, lr : 0.010000\n",
      "Epoch: 412/25000, Training Loss: 0.006200, lr : 0.010000\n",
      "Epoch: 413/25000, Training Loss: 0.005917, lr : 0.010000\n",
      "Epoch: 414/25000, Training Loss: 0.005825, lr : 0.010000\n",
      "Epoch: 415/25000, Training Loss: 0.005512, lr : 0.010000\n",
      "Epoch: 416/25000, Training Loss: 0.006058, lr : 0.010000\n",
      "Epoch: 417/25000, Training Loss: 0.006040, lr : 0.010000\n",
      "Epoch: 418/25000, Training Loss: 0.006075, lr : 0.010000\n",
      "Epoch: 419/25000, Training Loss: 0.006047, lr : 0.010000\n",
      "Epoch: 420/25000, Training Loss: 0.006895, lr : 0.010000\n",
      "Epoch: 421/25000, Training Loss: 0.006158, lr : 0.010000\n",
      "Epoch: 422/25000, Training Loss: 0.006386, lr : 0.010000\n",
      "Epoch: 423/25000, Training Loss: 0.006565, lr : 0.010000\n",
      "Epoch: 424/25000, Training Loss: 0.005292, lr : 0.010000\n",
      "Epoch: 425/25000, Training Loss: 0.005653, lr : 0.010000\n",
      "Epoch: 426/25000, Training Loss: 0.006487, lr : 0.010000\n",
      "Epoch: 427/25000, Training Loss: 0.005529, lr : 0.010000\n",
      "Epoch: 428/25000, Training Loss: 0.005560, lr : 0.010000\n",
      "Epoch: 429/25000, Training Loss: 0.005508, lr : 0.010000\n",
      "Epoch: 430/25000, Training Loss: 0.005519, lr : 0.010000\n",
      "Epoch: 431/25000, Training Loss: 0.005178, lr : 0.010000\n",
      "Epoch: 432/25000, Training Loss: 0.005699, lr : 0.010000\n",
      "Epoch: 433/25000, Training Loss: 0.006010, lr : 0.010000\n",
      "Epoch: 434/25000, Training Loss: 0.005952, lr : 0.010000\n",
      "Epoch: 435/25000, Training Loss: 0.004794, lr : 0.010000\n",
      "Epoch: 436/25000, Training Loss: 0.005630, lr : 0.010000\n",
      "Epoch: 437/25000, Training Loss: 0.006259, lr : 0.008500\n",
      "Epoch: 438/25000, Training Loss: 0.006166, lr : 0.008500\n",
      "Epoch: 439/25000, Training Loss: 0.006219, lr : 0.008500\n",
      "Epoch: 440/25000, Training Loss: 0.005180, lr : 0.008500\n",
      "Epoch: 441/25000, Training Loss: 0.005452, lr : 0.008500\n",
      "Epoch: 442/25000, Training Loss: 0.005814, lr : 0.008500\n",
      "Epoch: 443/25000, Training Loss: 0.005224, lr : 0.008500\n",
      "Epoch: 444/25000, Training Loss: 0.005310, lr : 0.008500\n",
      "Epoch: 445/25000, Training Loss: 0.005799, lr : 0.008500\n",
      "Epoch: 446/25000, Training Loss: 0.005100, lr : 0.008500\n",
      "Epoch: 447/25000, Training Loss: 0.005210, lr : 0.008500\n",
      "Epoch: 448/25000, Training Loss: 0.006245, lr : 0.008500\n",
      "Epoch: 449/25000, Training Loss: 0.006033, lr : 0.008500\n",
      "Epoch: 450/25000, Training Loss: 0.005562, lr : 0.008500\n",
      "Epoch: 451/25000, Training Loss: 0.006136, lr : 0.008500\n",
      "Epoch: 452/25000, Training Loss: 0.005504, lr : 0.008500\n",
      "Epoch: 453/25000, Training Loss: 0.006498, lr : 0.008500\n",
      "Epoch: 454/25000, Training Loss: 0.005374, lr : 0.008500\n",
      "Epoch: 455/25000, Training Loss: 0.006086, lr : 0.008500\n",
      "Epoch: 456/25000, Training Loss: 0.005067, lr : 0.008500\n",
      "Epoch: 457/25000, Training Loss: 0.004252, lr : 0.008500\n",
      "Epoch: 458/25000, Training Loss: 0.005338, lr : 0.008500\n",
      "Epoch: 459/25000, Training Loss: 0.006118, lr : 0.008500\n",
      "Epoch: 460/25000, Training Loss: 0.005780, lr : 0.008500\n",
      "Epoch: 461/25000, Training Loss: 0.006686, lr : 0.008500\n",
      "Epoch: 462/25000, Training Loss: 0.006278, lr : 0.008500\n",
      "Epoch: 463/25000, Training Loss: 0.005150, lr : 0.008500\n",
      "Epoch: 464/25000, Training Loss: 0.005344, lr : 0.008500\n",
      "Epoch: 465/25000, Training Loss: 0.005868, lr : 0.008500\n",
      "Epoch: 466/25000, Training Loss: 0.005382, lr : 0.008500\n",
      "Epoch: 467/25000, Training Loss: 0.005637, lr : 0.008500\n",
      "Epoch: 468/25000, Training Loss: 0.005432, lr : 0.008500\n",
      "Epoch: 469/25000, Training Loss: 0.006129, lr : 0.008500\n",
      "Epoch: 470/25000, Training Loss: 0.005402, lr : 0.008500\n",
      "Epoch: 471/25000, Training Loss: 0.005313, lr : 0.008500\n",
      "Epoch: 472/25000, Training Loss: 0.005126, lr : 0.008500\n",
      "Epoch: 473/25000, Training Loss: 0.007388, lr : 0.008500\n",
      "Epoch: 474/25000, Training Loss: 0.005341, lr : 0.008500\n",
      "Epoch: 475/25000, Training Loss: 0.005977, lr : 0.008500\n",
      "Epoch: 476/25000, Training Loss: 0.005002, lr : 0.008500\n",
      "Epoch: 477/25000, Training Loss: 0.004844, lr : 0.008500\n",
      "Epoch: 478/25000, Training Loss: 0.005269, lr : 0.008500\n",
      "Epoch: 479/25000, Training Loss: 0.006447, lr : 0.008500\n",
      "Epoch: 480/25000, Training Loss: 0.005588, lr : 0.008500\n",
      "Epoch: 481/25000, Training Loss: 0.005376, lr : 0.008500\n",
      "Epoch: 482/25000, Training Loss: 0.005425, lr : 0.008500\n",
      "Epoch: 483/25000, Training Loss: 0.005973, lr : 0.008500\n",
      "Epoch: 484/25000, Training Loss: 0.005232, lr : 0.008500\n",
      "Epoch: 485/25000, Training Loss: 0.005588, lr : 0.008500\n",
      "Epoch: 486/25000, Training Loss: 0.005627, lr : 0.008500\n",
      "Epoch: 487/25000, Training Loss: 0.006603, lr : 0.008500\n",
      "Epoch: 488/25000, Training Loss: 0.006194, lr : 0.008500\n",
      "Epoch: 489/25000, Training Loss: 0.005704, lr : 0.008500\n",
      "Epoch: 490/25000, Training Loss: 0.005713, lr : 0.008500\n",
      "Epoch: 491/25000, Training Loss: 0.005401, lr : 0.008500\n",
      "Epoch: 492/25000, Training Loss: 0.005613, lr : 0.008500\n",
      "Epoch: 493/25000, Training Loss: 0.005046, lr : 0.008500\n",
      "Epoch: 494/25000, Training Loss: 0.004948, lr : 0.008500\n",
      "Epoch: 495/25000, Training Loss: 0.004828, lr : 0.008500\n",
      "Epoch: 496/25000, Training Loss: 0.005758, lr : 0.008500\n",
      "Epoch: 497/25000, Training Loss: 0.005107, lr : 0.008500\n",
      "Epoch: 498/25000, Training Loss: 0.004459, lr : 0.008500\n",
      "Epoch: 499/25000, Training Loss: 0.005288, lr : 0.008500\n",
      "Epoch: 500/25000, Training Loss: 0.006387, lr : 0.008500\n",
      "Epoch: 501/25000, Training Loss: 0.005212, lr : 0.008500\n",
      "Epoch: 502/25000, Training Loss: 0.005471, lr : 0.008500\n",
      "Epoch: 503/25000, Training Loss: 0.005063, lr : 0.008500\n",
      "Epoch: 504/25000, Training Loss: 0.004762, lr : 0.008500\n",
      "Epoch: 505/25000, Training Loss: 0.006178, lr : 0.008500\n",
      "Epoch: 506/25000, Training Loss: 0.004570, lr : 0.008500\n",
      "Epoch: 507/25000, Training Loss: 0.005361, lr : 0.008500\n",
      "Epoch: 508/25000, Training Loss: 0.005305, lr : 0.008500\n",
      "Epoch: 509/25000, Training Loss: 0.004620, lr : 0.008500\n",
      "Epoch: 510/25000, Training Loss: 0.005454, lr : 0.008500\n",
      "Epoch: 511/25000, Training Loss: 0.005427, lr : 0.008500\n",
      "Epoch: 512/25000, Training Loss: 0.005751, lr : 0.008500\n",
      "Epoch: 513/25000, Training Loss: 0.005564, lr : 0.008500\n",
      "Epoch: 514/25000, Training Loss: 0.005679, lr : 0.008500\n",
      "Epoch: 515/25000, Training Loss: 0.004710, lr : 0.008500\n",
      "Epoch: 516/25000, Training Loss: 0.005218, lr : 0.008500\n",
      "Epoch: 517/25000, Training Loss: 0.004436, lr : 0.008500\n",
      "Epoch: 518/25000, Training Loss: 0.005021, lr : 0.008500\n",
      "Epoch: 519/25000, Training Loss: 0.005326, lr : 0.008500\n",
      "Epoch: 520/25000, Training Loss: 0.004836, lr : 0.008500\n",
      "Epoch: 521/25000, Training Loss: 0.004676, lr : 0.008500\n",
      "Epoch: 522/25000, Training Loss: 0.004937, lr : 0.008500\n",
      "Epoch: 523/25000, Training Loss: 0.004520, lr : 0.008500\n",
      "Epoch: 524/25000, Training Loss: 0.004362, lr : 0.008500\n",
      "Epoch: 525/25000, Training Loss: 0.004622, lr : 0.008500\n",
      "Epoch: 526/25000, Training Loss: 0.005720, lr : 0.008500\n",
      "Epoch: 527/25000, Training Loss: 0.005419, lr : 0.008500\n",
      "Epoch: 528/25000, Training Loss: 0.005493, lr : 0.008500\n",
      "Epoch: 529/25000, Training Loss: 0.004618, lr : 0.008500\n",
      "Epoch: 530/25000, Training Loss: 0.004740, lr : 0.008500\n",
      "Epoch: 531/25000, Training Loss: 0.004375, lr : 0.008500\n",
      "Epoch: 532/25000, Training Loss: 0.004980, lr : 0.008500\n",
      "Epoch: 533/25000, Training Loss: 0.005762, lr : 0.008500\n",
      "Epoch: 534/25000, Training Loss: 0.005701, lr : 0.008500\n",
      "Epoch: 535/25000, Training Loss: 0.004419, lr : 0.008500\n",
      "Epoch: 536/25000, Training Loss: 0.005749, lr : 0.008500\n",
      "Epoch: 537/25000, Training Loss: 0.004516, lr : 0.008500\n",
      "Epoch: 538/25000, Training Loss: 0.004905, lr : 0.008500\n",
      "Epoch: 539/25000, Training Loss: 0.004395, lr : 0.008500\n",
      "Epoch: 540/25000, Training Loss: 0.004045, lr : 0.008500\n",
      "Epoch: 541/25000, Training Loss: 0.004834, lr : 0.008500\n",
      "Epoch: 542/25000, Training Loss: 0.005135, lr : 0.008500\n",
      "Epoch: 543/25000, Training Loss: 0.004532, lr : 0.008500\n",
      "Epoch: 544/25000, Training Loss: 0.005026, lr : 0.008500\n",
      "Epoch: 545/25000, Training Loss: 0.005090, lr : 0.008500\n",
      "Epoch: 546/25000, Training Loss: 0.005439, lr : 0.008500\n",
      "Epoch: 547/25000, Training Loss: 0.005696, lr : 0.008500\n",
      "Epoch: 548/25000, Training Loss: 0.004232, lr : 0.008500\n",
      "Epoch: 549/25000, Training Loss: 0.005260, lr : 0.008500\n",
      "Epoch: 550/25000, Training Loss: 0.005563, lr : 0.008500\n",
      "Epoch: 551/25000, Training Loss: 0.004468, lr : 0.008500\n",
      "Epoch: 552/25000, Training Loss: 0.004856, lr : 0.008500\n",
      "Epoch: 553/25000, Training Loss: 0.004163, lr : 0.008500\n",
      "Epoch: 554/25000, Training Loss: 0.004475, lr : 0.008500\n",
      "Epoch: 555/25000, Training Loss: 0.005464, lr : 0.008500\n",
      "Epoch: 556/25000, Training Loss: 0.005034, lr : 0.008500\n",
      "Epoch: 557/25000, Training Loss: 0.004715, lr : 0.008500\n",
      "Epoch: 558/25000, Training Loss: 0.005033, lr : 0.008500\n",
      "Epoch: 559/25000, Training Loss: 0.005185, lr : 0.008500\n",
      "Epoch: 560/25000, Training Loss: 0.005276, lr : 0.008500\n",
      "Epoch: 561/25000, Training Loss: 0.005104, lr : 0.008500\n",
      "Epoch: 562/25000, Training Loss: 0.005374, lr : 0.008500\n",
      "Epoch: 563/25000, Training Loss: 0.005356, lr : 0.008500\n",
      "Epoch: 564/25000, Training Loss: 0.005248, lr : 0.008500\n",
      "Epoch: 565/25000, Training Loss: 0.004707, lr : 0.008500\n",
      "Epoch: 566/25000, Training Loss: 0.004238, lr : 0.008500\n",
      "Epoch: 567/25000, Training Loss: 0.005011, lr : 0.008500\n",
      "Epoch: 568/25000, Training Loss: 0.004961, lr : 0.008500\n",
      "Epoch: 569/25000, Training Loss: 0.005068, lr : 0.008500\n",
      "Epoch: 570/25000, Training Loss: 0.004768, lr : 0.008500\n",
      "Epoch: 571/25000, Training Loss: 0.004810, lr : 0.008500\n",
      "Epoch: 572/25000, Training Loss: 0.004519, lr : 0.008500\n",
      "Epoch: 573/25000, Training Loss: 0.004943, lr : 0.008500\n",
      "Epoch: 574/25000, Training Loss: 0.004975, lr : 0.008500\n",
      "Epoch: 575/25000, Training Loss: 0.004781, lr : 0.008500\n",
      "Epoch: 576/25000, Training Loss: 0.004200, lr : 0.008500\n",
      "Epoch: 577/25000, Training Loss: 0.005214, lr : 0.008500\n",
      "Epoch: 578/25000, Training Loss: 0.004560, lr : 0.008500\n",
      "Epoch: 579/25000, Training Loss: 0.005292, lr : 0.008500\n",
      "Epoch: 580/25000, Training Loss: 0.005059, lr : 0.008500\n",
      "Epoch: 581/25000, Training Loss: 0.004573, lr : 0.008500\n",
      "Epoch: 582/25000, Training Loss: 0.004226, lr : 0.008500\n",
      "Epoch: 583/25000, Training Loss: 0.004394, lr : 0.008500\n",
      "Epoch: 584/25000, Training Loss: 0.004082, lr : 0.008500\n",
      "Epoch: 585/25000, Training Loss: 0.004985, lr : 0.008500\n",
      "Epoch: 586/25000, Training Loss: 0.005173, lr : 0.008500\n",
      "Epoch: 587/25000, Training Loss: 0.004435, lr : 0.008500\n",
      "Epoch: 588/25000, Training Loss: 0.004767, lr : 0.008500\n",
      "Epoch: 589/25000, Training Loss: 0.004096, lr : 0.008500\n",
      "Epoch: 590/25000, Training Loss: 0.004143, lr : 0.008500\n",
      "Epoch: 591/25000, Training Loss: 0.004749, lr : 0.008500\n",
      "Epoch: 592/25000, Training Loss: 0.005492, lr : 0.008500\n",
      "Epoch: 593/25000, Training Loss: 0.004811, lr : 0.008500\n",
      "Epoch: 594/25000, Training Loss: 0.004393, lr : 0.008500\n",
      "Epoch: 595/25000, Training Loss: 0.004839, lr : 0.008500\n",
      "Epoch: 596/25000, Training Loss: 0.004649, lr : 0.008500\n",
      "Epoch: 597/25000, Training Loss: 0.006125, lr : 0.008500\n",
      "Epoch: 598/25000, Training Loss: 0.004487, lr : 0.008500\n",
      "Epoch: 599/25000, Training Loss: 0.004530, lr : 0.008500\n",
      "Epoch: 600/25000, Training Loss: 0.005304, lr : 0.008500\n",
      "Epoch: 601/25000, Training Loss: 0.005306, lr : 0.008500\n",
      "Epoch: 602/25000, Training Loss: 0.004801, lr : 0.008500\n",
      "Epoch: 603/25000, Training Loss: 0.004685, lr : 0.008500\n",
      "Epoch: 604/25000, Training Loss: 0.004724, lr : 0.008500\n",
      "Epoch: 605/25000, Training Loss: 0.004885, lr : 0.008500\n",
      "Epoch: 606/25000, Training Loss: 0.005279, lr : 0.008500\n",
      "Epoch: 607/25000, Training Loss: 0.005782, lr : 0.008500\n",
      "Epoch: 608/25000, Training Loss: 0.004523, lr : 0.008500\n",
      "Epoch: 609/25000, Training Loss: 0.004380, lr : 0.008500\n",
      "Epoch: 610/25000, Training Loss: 0.004303, lr : 0.008500\n",
      "Epoch: 611/25000, Training Loss: 0.004886, lr : 0.008500\n",
      "Epoch: 612/25000, Training Loss: 0.004085, lr : 0.008500\n",
      "Epoch: 613/25000, Training Loss: 0.004291, lr : 0.008500\n",
      "Epoch: 614/25000, Training Loss: 0.004411, lr : 0.008500\n",
      "Epoch: 615/25000, Training Loss: 0.004829, lr : 0.008500\n",
      "Epoch: 616/25000, Training Loss: 0.004257, lr : 0.008500\n",
      "Epoch: 617/25000, Training Loss: 0.003824, lr : 0.008500\n",
      "Epoch: 618/25000, Training Loss: 0.004484, lr : 0.008500\n",
      "Epoch: 619/25000, Training Loss: 0.004316, lr : 0.008500\n",
      "Epoch: 620/25000, Training Loss: 0.003897, lr : 0.008500\n",
      "Epoch: 621/25000, Training Loss: 0.003897, lr : 0.008500\n",
      "Epoch: 622/25000, Training Loss: 0.004282, lr : 0.008500\n",
      "Epoch: 623/25000, Training Loss: 0.004111, lr : 0.008500\n",
      "Epoch: 624/25000, Training Loss: 0.005577, lr : 0.008500\n",
      "Epoch: 625/25000, Training Loss: 0.003787, lr : 0.008500\n",
      "Epoch: 626/25000, Training Loss: 0.004578, lr : 0.008500\n",
      "Epoch: 627/25000, Training Loss: 0.005131, lr : 0.008500\n",
      "Epoch: 628/25000, Training Loss: 0.004797, lr : 0.008500\n",
      "Epoch: 629/25000, Training Loss: 0.004630, lr : 0.008500\n",
      "Epoch: 630/25000, Training Loss: 0.004899, lr : 0.008500\n",
      "Epoch: 631/25000, Training Loss: 0.004273, lr : 0.008500\n",
      "Epoch: 632/25000, Training Loss: 0.004112, lr : 0.008500\n",
      "Epoch: 633/25000, Training Loss: 0.004707, lr : 0.008500\n",
      "Epoch: 634/25000, Training Loss: 0.004149, lr : 0.008500\n",
      "Epoch: 635/25000, Training Loss: 0.005069, lr : 0.008500\n",
      "Epoch: 636/25000, Training Loss: 0.004134, lr : 0.008500\n",
      "Epoch: 637/25000, Training Loss: 0.004270, lr : 0.008500\n",
      "Epoch: 638/25000, Training Loss: 0.004389, lr : 0.008500\n",
      "Epoch: 639/25000, Training Loss: 0.004216, lr : 0.008500\n",
      "Epoch: 640/25000, Training Loss: 0.004622, lr : 0.008500\n",
      "Epoch: 641/25000, Training Loss: 0.004524, lr : 0.008500\n",
      "Epoch: 642/25000, Training Loss: 0.004309, lr : 0.008500\n",
      "Epoch: 643/25000, Training Loss: 0.004701, lr : 0.008500\n",
      "Epoch: 644/25000, Training Loss: 0.004360, lr : 0.008500\n",
      "Epoch: 645/25000, Training Loss: 0.005403, lr : 0.008500\n",
      "Epoch: 646/25000, Training Loss: 0.004179, lr : 0.008500\n",
      "Epoch: 647/25000, Training Loss: 0.004085, lr : 0.008500\n",
      "Epoch: 648/25000, Training Loss: 0.003874, lr : 0.008500\n",
      "Epoch: 649/25000, Training Loss: 0.004346, lr : 0.008500\n",
      "Epoch: 650/25000, Training Loss: 0.004337, lr : 0.008500\n",
      "Epoch: 651/25000, Training Loss: 0.004693, lr : 0.008500\n",
      "Epoch: 652/25000, Training Loss: 0.005060, lr : 0.008500\n",
      "Epoch: 653/25000, Training Loss: 0.004141, lr : 0.008500\n",
      "Epoch: 654/25000, Training Loss: 0.004949, lr : 0.008500\n",
      "Epoch: 655/25000, Training Loss: 0.004583, lr : 0.008500\n",
      "Epoch: 656/25000, Training Loss: 0.004887, lr : 0.008500\n",
      "Epoch: 657/25000, Training Loss: 0.003909, lr : 0.008500\n",
      "Epoch: 658/25000, Training Loss: 0.004519, lr : 0.008500\n",
      "Epoch: 659/25000, Training Loss: 0.004708, lr : 0.008500\n",
      "Epoch: 660/25000, Training Loss: 0.004682, lr : 0.008500\n",
      "Epoch: 661/25000, Training Loss: 0.004619, lr : 0.008500\n",
      "Epoch: 662/25000, Training Loss: 0.004389, lr : 0.008500\n",
      "Epoch: 663/25000, Training Loss: 0.004848, lr : 0.008500\n",
      "Epoch: 664/25000, Training Loss: 0.004319, lr : 0.008500\n",
      "Epoch: 665/25000, Training Loss: 0.004552, lr : 0.008500\n",
      "Epoch: 666/25000, Training Loss: 0.004149, lr : 0.008500\n",
      "Epoch: 667/25000, Training Loss: 0.005039, lr : 0.008500\n",
      "Epoch: 668/25000, Training Loss: 0.004700, lr : 0.008500\n",
      "Epoch: 669/25000, Training Loss: 0.003588, lr : 0.008500\n",
      "Epoch: 670/25000, Training Loss: 0.004210, lr : 0.008500\n",
      "Epoch: 671/25000, Training Loss: 0.003784, lr : 0.008500\n",
      "Epoch: 672/25000, Training Loss: 0.005040, lr : 0.008500\n",
      "Epoch: 673/25000, Training Loss: 0.003955, lr : 0.008500\n",
      "Epoch: 674/25000, Training Loss: 0.004402, lr : 0.008500\n",
      "Epoch: 675/25000, Training Loss: 0.004593, lr : 0.008500\n",
      "Epoch: 676/25000, Training Loss: 0.004661, lr : 0.008500\n",
      "Epoch: 677/25000, Training Loss: 0.004319, lr : 0.008500\n",
      "Epoch: 678/25000, Training Loss: 0.004903, lr : 0.008500\n",
      "Epoch: 679/25000, Training Loss: 0.005032, lr : 0.008500\n",
      "Epoch: 680/25000, Training Loss: 0.004184, lr : 0.008500\n",
      "Epoch: 681/25000, Training Loss: 0.004848, lr : 0.008500\n",
      "Epoch: 682/25000, Training Loss: 0.004196, lr : 0.008500\n",
      "Epoch: 683/25000, Training Loss: 0.004051, lr : 0.008500\n",
      "Epoch: 684/25000, Training Loss: 0.004747, lr : 0.008500\n",
      "Epoch: 685/25000, Training Loss: 0.004502, lr : 0.008500\n",
      "Epoch: 686/25000, Training Loss: 0.004078, lr : 0.008500\n",
      "Epoch: 687/25000, Training Loss: 0.003729, lr : 0.008500\n",
      "Epoch: 688/25000, Training Loss: 0.004861, lr : 0.008500\n",
      "Epoch: 689/25000, Training Loss: 0.004006, lr : 0.008500\n",
      "Epoch: 690/25000, Training Loss: 0.004405, lr : 0.008500\n",
      "Epoch: 691/25000, Training Loss: 0.004382, lr : 0.008500\n",
      "Epoch: 692/25000, Training Loss: 0.004196, lr : 0.008500\n",
      "Epoch: 693/25000, Training Loss: 0.004029, lr : 0.008500\n",
      "Epoch: 694/25000, Training Loss: 0.004998, lr : 0.008500\n",
      "Epoch: 695/25000, Training Loss: 0.004484, lr : 0.008500\n",
      "Epoch: 696/25000, Training Loss: 0.004755, lr : 0.008500\n",
      "Epoch: 697/25000, Training Loss: 0.004202, lr : 0.008500\n",
      "Epoch: 698/25000, Training Loss: 0.004307, lr : 0.008500\n",
      "Epoch: 699/25000, Training Loss: 0.004359, lr : 0.008500\n",
      "Epoch: 700/25000, Training Loss: 0.003870, lr : 0.008500\n",
      "Epoch: 701/25000, Training Loss: 0.004807, lr : 0.008500\n",
      "Epoch: 702/25000, Training Loss: 0.004357, lr : 0.008500\n",
      "Epoch: 703/25000, Training Loss: 0.004143, lr : 0.008500\n",
      "Epoch: 704/25000, Training Loss: 0.003929, lr : 0.008500\n",
      "Epoch: 705/25000, Training Loss: 0.004007, lr : 0.008500\n",
      "Epoch: 706/25000, Training Loss: 0.004073, lr : 0.008500\n",
      "Epoch: 707/25000, Training Loss: 0.004674, lr : 0.008500\n",
      "Epoch: 708/25000, Training Loss: 0.004162, lr : 0.008500\n",
      "Epoch: 709/25000, Training Loss: 0.004511, lr : 0.008500\n",
      "Epoch: 710/25000, Training Loss: 0.004544, lr : 0.008500\n",
      "Epoch: 711/25000, Training Loss: 0.004161, lr : 0.008500\n",
      "Epoch: 712/25000, Training Loss: 0.005000, lr : 0.008500\n",
      "Epoch: 713/25000, Training Loss: 0.004972, lr : 0.008500\n",
      "Epoch: 714/25000, Training Loss: 0.003827, lr : 0.008500\n",
      "Epoch: 715/25000, Training Loss: 0.004315, lr : 0.008500\n",
      "Epoch: 716/25000, Training Loss: 0.004674, lr : 0.008500\n",
      "Epoch: 717/25000, Training Loss: 0.003820, lr : 0.008500\n",
      "Epoch: 718/25000, Training Loss: 0.005409, lr : 0.008500\n",
      "Epoch: 719/25000, Training Loss: 0.004267, lr : 0.008500\n",
      "Epoch: 720/25000, Training Loss: 0.004097, lr : 0.008500\n",
      "Epoch: 721/25000, Training Loss: 0.003904, lr : 0.008500\n",
      "Epoch: 722/25000, Training Loss: 0.004267, lr : 0.008500\n",
      "Epoch: 723/25000, Training Loss: 0.003949, lr : 0.008500\n",
      "Epoch: 724/25000, Training Loss: 0.004203, lr : 0.008500\n",
      "Epoch: 725/25000, Training Loss: 0.003947, lr : 0.008500\n",
      "Epoch: 726/25000, Training Loss: 0.003983, lr : 0.008500\n",
      "Epoch: 727/25000, Training Loss: 0.003952, lr : 0.008500\n",
      "Epoch: 728/25000, Training Loss: 0.003823, lr : 0.008500\n",
      "Epoch: 729/25000, Training Loss: 0.003531, lr : 0.008500\n",
      "Epoch: 730/25000, Training Loss: 0.004172, lr : 0.008500\n",
      "Epoch: 731/25000, Training Loss: 0.004051, lr : 0.008500\n",
      "Epoch: 732/25000, Training Loss: 0.004344, lr : 0.008500\n",
      "Epoch: 733/25000, Training Loss: 0.004115, lr : 0.008500\n",
      "Epoch: 734/25000, Training Loss: 0.004064, lr : 0.008500\n",
      "Epoch: 735/25000, Training Loss: 0.004786, lr : 0.008500\n",
      "Epoch: 736/25000, Training Loss: 0.004384, lr : 0.008500\n",
      "Epoch: 737/25000, Training Loss: 0.003762, lr : 0.008500\n",
      "Epoch: 738/25000, Training Loss: 0.004082, lr : 0.008500\n",
      "Epoch: 739/25000, Training Loss: 0.004439, lr : 0.008500\n",
      "Epoch: 740/25000, Training Loss: 0.003343, lr : 0.008500\n",
      "Epoch: 741/25000, Training Loss: 0.004574, lr : 0.008500\n",
      "Epoch: 742/25000, Training Loss: 0.004160, lr : 0.008500\n",
      "Epoch: 743/25000, Training Loss: 0.003959, lr : 0.008500\n",
      "Epoch: 744/25000, Training Loss: 0.004107, lr : 0.008500\n",
      "Epoch: 745/25000, Training Loss: 0.004079, lr : 0.008500\n",
      "Epoch: 746/25000, Training Loss: 0.003864, lr : 0.008500\n",
      "Epoch: 747/25000, Training Loss: 0.003993, lr : 0.008500\n",
      "Epoch: 748/25000, Training Loss: 0.004303, lr : 0.008500\n",
      "Epoch: 749/25000, Training Loss: 0.004008, lr : 0.008500\n",
      "Epoch: 750/25000, Training Loss: 0.003883, lr : 0.008500\n",
      "Epoch: 751/25000, Training Loss: 0.004039, lr : 0.008500\n",
      "Epoch: 752/25000, Training Loss: 0.003600, lr : 0.008500\n",
      "Epoch: 753/25000, Training Loss: 0.003875, lr : 0.008500\n",
      "Epoch: 754/25000, Training Loss: 0.003963, lr : 0.008500\n",
      "Epoch: 755/25000, Training Loss: 0.004385, lr : 0.008500\n",
      "Epoch: 756/25000, Training Loss: 0.004168, lr : 0.008500\n",
      "Epoch: 757/25000, Training Loss: 0.003784, lr : 0.008500\n",
      "Epoch: 758/25000, Training Loss: 0.003804, lr : 0.008500\n",
      "Epoch: 759/25000, Training Loss: 0.003843, lr : 0.008500\n",
      "Epoch: 760/25000, Training Loss: 0.003219, lr : 0.008500\n",
      "Epoch: 761/25000, Training Loss: 0.003853, lr : 0.008500\n",
      "Epoch: 762/25000, Training Loss: 0.003415, lr : 0.008500\n",
      "Epoch: 763/25000, Training Loss: 0.004208, lr : 0.008500\n",
      "Epoch: 764/25000, Training Loss: 0.003781, lr : 0.008500\n",
      "Epoch: 765/25000, Training Loss: 0.003660, lr : 0.008500\n",
      "Epoch: 766/25000, Training Loss: 0.003778, lr : 0.008500\n",
      "Epoch: 767/25000, Training Loss: 0.004005, lr : 0.008500\n",
      "Epoch: 768/25000, Training Loss: 0.003895, lr : 0.008500\n",
      "Epoch: 769/25000, Training Loss: 0.003860, lr : 0.008500\n",
      "Epoch: 770/25000, Training Loss: 0.003852, lr : 0.008500\n",
      "Epoch: 771/25000, Training Loss: 0.004221, lr : 0.008500\n",
      "Epoch: 772/25000, Training Loss: 0.003710, lr : 0.008500\n",
      "Epoch: 773/25000, Training Loss: 0.004241, lr : 0.008500\n",
      "Epoch: 774/25000, Training Loss: 0.003054, lr : 0.008500\n",
      "Epoch: 775/25000, Training Loss: 0.004513, lr : 0.008500\n",
      "Epoch: 776/25000, Training Loss: 0.004384, lr : 0.008500\n",
      "Epoch: 777/25000, Training Loss: 0.003535, lr : 0.008500\n",
      "Epoch: 778/25000, Training Loss: 0.003898, lr : 0.008500\n",
      "Epoch: 779/25000, Training Loss: 0.003670, lr : 0.008500\n",
      "Epoch: 780/25000, Training Loss: 0.004450, lr : 0.008500\n",
      "Epoch: 781/25000, Training Loss: 0.004439, lr : 0.008500\n",
      "Epoch: 782/25000, Training Loss: 0.004179, lr : 0.008500\n",
      "Epoch: 783/25000, Training Loss: 0.004112, lr : 0.008500\n",
      "Epoch: 784/25000, Training Loss: 0.004047, lr : 0.008500\n",
      "Epoch: 785/25000, Training Loss: 0.003595, lr : 0.008500\n",
      "Epoch: 786/25000, Training Loss: 0.004265, lr : 0.008500\n",
      "Epoch: 787/25000, Training Loss: 0.004401, lr : 0.008500\n",
      "Epoch: 788/25000, Training Loss: 0.003423, lr : 0.008500\n",
      "Epoch: 789/25000, Training Loss: 0.003784, lr : 0.008500\n",
      "Epoch: 790/25000, Training Loss: 0.004381, lr : 0.008500\n",
      "Epoch: 791/25000, Training Loss: 0.004312, lr : 0.008500\n",
      "Epoch: 792/25000, Training Loss: 0.003942, lr : 0.008500\n",
      "Epoch: 793/25000, Training Loss: 0.003965, lr : 0.008500\n",
      "Epoch: 794/25000, Training Loss: 0.003800, lr : 0.008500\n",
      "Epoch: 795/25000, Training Loss: 0.003911, lr : 0.008500\n",
      "Epoch: 796/25000, Training Loss: 0.003295, lr : 0.008500\n",
      "Epoch: 797/25000, Training Loss: 0.003895, lr : 0.008500\n",
      "Epoch: 798/25000, Training Loss: 0.004088, lr : 0.008500\n",
      "Epoch: 799/25000, Training Loss: 0.003913, lr : 0.008500\n",
      "Epoch: 800/25000, Training Loss: 0.003643, lr : 0.008500\n",
      "Epoch: 801/25000, Training Loss: 0.003975, lr : 0.008500\n",
      "Epoch: 802/25000, Training Loss: 0.003266, lr : 0.008500\n",
      "Epoch: 803/25000, Training Loss: 0.004444, lr : 0.008500\n",
      "Epoch: 804/25000, Training Loss: 0.003967, lr : 0.008500\n",
      "Epoch: 805/25000, Training Loss: 0.003914, lr : 0.008500\n",
      "Epoch: 806/25000, Training Loss: 0.004510, lr : 0.008500\n",
      "Epoch: 807/25000, Training Loss: 0.003349, lr : 0.008500\n",
      "Epoch: 808/25000, Training Loss: 0.003886, lr : 0.008500\n",
      "Epoch: 809/25000, Training Loss: 0.004012, lr : 0.008500\n",
      "Epoch: 810/25000, Training Loss: 0.004441, lr : 0.008500\n",
      "Epoch: 811/25000, Training Loss: 0.004018, lr : 0.008500\n",
      "Epoch: 812/25000, Training Loss: 0.004168, lr : 0.008500\n",
      "Epoch: 813/25000, Training Loss: 0.004216, lr : 0.008500\n",
      "Epoch: 814/25000, Training Loss: 0.004308, lr : 0.008500\n",
      "Epoch: 815/25000, Training Loss: 0.004339, lr : 0.008500\n",
      "Epoch: 816/25000, Training Loss: 0.004139, lr : 0.008500\n",
      "Epoch: 817/25000, Training Loss: 0.003757, lr : 0.008500\n",
      "Epoch: 818/25000, Training Loss: 0.004495, lr : 0.008500\n",
      "Epoch: 819/25000, Training Loss: 0.003706, lr : 0.008500\n",
      "Epoch: 820/25000, Training Loss: 0.003951, lr : 0.008500\n",
      "Epoch: 821/25000, Training Loss: 0.003802, lr : 0.008500\n",
      "Epoch: 822/25000, Training Loss: 0.003928, lr : 0.008500\n",
      "Epoch: 823/25000, Training Loss: 0.003764, lr : 0.008500\n",
      "Epoch: 824/25000, Training Loss: 0.004015, lr : 0.008500\n",
      "Epoch: 825/25000, Training Loss: 0.003484, lr : 0.008500\n",
      "Epoch: 826/25000, Training Loss: 0.003591, lr : 0.008500\n",
      "Epoch: 827/25000, Training Loss: 0.003791, lr : 0.008500\n",
      "Epoch: 828/25000, Training Loss: 0.003929, lr : 0.008500\n",
      "Epoch: 829/25000, Training Loss: 0.003934, lr : 0.008500\n",
      "Epoch: 830/25000, Training Loss: 0.003390, lr : 0.008500\n",
      "Epoch: 831/25000, Training Loss: 0.003687, lr : 0.008500\n",
      "Epoch: 832/25000, Training Loss: 0.003829, lr : 0.008500\n",
      "Epoch: 833/25000, Training Loss: 0.003334, lr : 0.008500\n",
      "Epoch: 834/25000, Training Loss: 0.003649, lr : 0.008500\n",
      "Epoch: 835/25000, Training Loss: 0.003794, lr : 0.008500\n",
      "Epoch: 836/25000, Training Loss: 0.004135, lr : 0.008500\n",
      "Epoch: 837/25000, Training Loss: 0.003686, lr : 0.008500\n",
      "Epoch: 838/25000, Training Loss: 0.003540, lr : 0.008500\n",
      "Epoch: 839/25000, Training Loss: 0.003901, lr : 0.008500\n",
      "Epoch: 840/25000, Training Loss: 0.004074, lr : 0.008500\n",
      "Epoch: 841/25000, Training Loss: 0.003859, lr : 0.008500\n",
      "Epoch: 842/25000, Training Loss: 0.003259, lr : 0.008500\n",
      "Epoch: 843/25000, Training Loss: 0.003987, lr : 0.008500\n",
      "Epoch: 844/25000, Training Loss: 0.003527, lr : 0.008500\n",
      "Epoch: 845/25000, Training Loss: 0.003922, lr : 0.008500\n",
      "Epoch: 846/25000, Training Loss: 0.003231, lr : 0.008500\n",
      "Epoch: 847/25000, Training Loss: 0.003781, lr : 0.008500\n",
      "Epoch: 848/25000, Training Loss: 0.003963, lr : 0.008500\n",
      "Epoch: 849/25000, Training Loss: 0.003338, lr : 0.008500\n",
      "Epoch: 850/25000, Training Loss: 0.003510, lr : 0.008500\n",
      "Epoch: 851/25000, Training Loss: 0.003723, lr : 0.008500\n",
      "Epoch: 852/25000, Training Loss: 0.003599, lr : 0.008500\n",
      "Epoch: 853/25000, Training Loss: 0.003876, lr : 0.008500\n",
      "Epoch: 854/25000, Training Loss: 0.003552, lr : 0.008500\n",
      "Epoch: 855/25000, Training Loss: 0.003499, lr : 0.008500\n",
      "Epoch: 856/25000, Training Loss: 0.002852, lr : 0.008500\n",
      "Epoch: 857/25000, Training Loss: 0.003772, lr : 0.008500\n",
      "Epoch: 858/25000, Training Loss: 0.003269, lr : 0.008500\n",
      "Epoch: 859/25000, Training Loss: 0.003689, lr : 0.008500\n",
      "Epoch: 860/25000, Training Loss: 0.003483, lr : 0.008500\n",
      "Epoch: 861/25000, Training Loss: 0.003993, lr : 0.008500\n",
      "Epoch: 862/25000, Training Loss: 0.003020, lr : 0.008500\n",
      "Epoch: 863/25000, Training Loss: 0.003851, lr : 0.008500\n",
      "Epoch: 864/25000, Training Loss: 0.003565, lr : 0.008500\n",
      "Epoch: 865/25000, Training Loss: 0.003506, lr : 0.008500\n",
      "Epoch: 866/25000, Training Loss: 0.003442, lr : 0.008500\n",
      "Epoch: 867/25000, Training Loss: 0.003204, lr : 0.008500\n",
      "Epoch: 868/25000, Training Loss: 0.004021, lr : 0.008500\n",
      "Epoch: 869/25000, Training Loss: 0.003358, lr : 0.008500\n",
      "Epoch: 870/25000, Training Loss: 0.003652, lr : 0.008500\n",
      "Epoch: 871/25000, Training Loss: 0.003358, lr : 0.008500\n",
      "Epoch: 872/25000, Training Loss: 0.003535, lr : 0.008500\n",
      "Epoch: 873/25000, Training Loss: 0.003382, lr : 0.008500\n",
      "Epoch: 874/25000, Training Loss: 0.003614, lr : 0.008500\n",
      "Epoch: 875/25000, Training Loss: 0.004133, lr : 0.008500\n",
      "Epoch: 876/25000, Training Loss: 0.003456, lr : 0.008500\n",
      "Epoch: 877/25000, Training Loss: 0.003297, lr : 0.008500\n",
      "Epoch: 878/25000, Training Loss: 0.003601, lr : 0.008500\n",
      "Epoch: 879/25000, Training Loss: 0.003840, lr : 0.008500\n",
      "Epoch: 880/25000, Training Loss: 0.004154, lr : 0.008500\n",
      "Epoch: 881/25000, Training Loss: 0.003387, lr : 0.008500\n",
      "Epoch: 882/25000, Training Loss: 0.003222, lr : 0.008500\n",
      "Epoch: 883/25000, Training Loss: 0.003495, lr : 0.008500\n",
      "Epoch: 884/25000, Training Loss: 0.004109, lr : 0.008500\n",
      "Epoch: 885/25000, Training Loss: 0.003655, lr : 0.008500\n",
      "Epoch: 886/25000, Training Loss: 0.003995, lr : 0.008500\n",
      "Epoch: 887/25000, Training Loss: 0.004228, lr : 0.008500\n",
      "Epoch: 888/25000, Training Loss: 0.004061, lr : 0.008500\n",
      "Epoch: 889/25000, Training Loss: 0.003416, lr : 0.008500\n",
      "Epoch: 890/25000, Training Loss: 0.003665, lr : 0.008500\n",
      "Epoch: 891/25000, Training Loss: 0.003741, lr : 0.008500\n",
      "Epoch: 892/25000, Training Loss: 0.003270, lr : 0.008500\n",
      "Epoch: 893/25000, Training Loss: 0.003532, lr : 0.008500\n",
      "Epoch: 894/25000, Training Loss: 0.003579, lr : 0.008500\n",
      "Epoch: 895/25000, Training Loss: 0.004004, lr : 0.008500\n",
      "Epoch: 896/25000, Training Loss: 0.003602, lr : 0.008500\n",
      "Epoch: 897/25000, Training Loss: 0.003800, lr : 0.008500\n",
      "Epoch: 898/25000, Training Loss: 0.003414, lr : 0.008500\n",
      "Epoch: 899/25000, Training Loss: 0.003634, lr : 0.008500\n",
      "Epoch: 900/25000, Training Loss: 0.003113, lr : 0.008500\n",
      "Epoch: 901/25000, Training Loss: 0.003909, lr : 0.008500\n",
      "Epoch: 902/25000, Training Loss: 0.003269, lr : 0.008500\n",
      "Epoch: 903/25000, Training Loss: 0.003599, lr : 0.008500\n",
      "Epoch: 904/25000, Training Loss: 0.003926, lr : 0.008500\n",
      "Epoch: 905/25000, Training Loss: 0.003839, lr : 0.008500\n",
      "Epoch: 906/25000, Training Loss: 0.003430, lr : 0.008500\n",
      "Epoch: 907/25000, Training Loss: 0.003085, lr : 0.008500\n",
      "Epoch: 908/25000, Training Loss: 0.003113, lr : 0.008500\n",
      "Epoch: 909/25000, Training Loss: 0.003987, lr : 0.008500\n",
      "Epoch: 910/25000, Training Loss: 0.003707, lr : 0.008500\n",
      "Epoch: 911/25000, Training Loss: 0.003467, lr : 0.008500\n",
      "Epoch: 912/25000, Training Loss: 0.003898, lr : 0.008500\n",
      "Epoch: 913/25000, Training Loss: 0.003312, lr : 0.008500\n",
      "Epoch: 914/25000, Training Loss: 0.003403, lr : 0.008500\n",
      "Epoch: 915/25000, Training Loss: 0.003645, lr : 0.008500\n",
      "Epoch: 916/25000, Training Loss: 0.003813, lr : 0.008500\n",
      "Epoch: 917/25000, Training Loss: 0.003489, lr : 0.008500\n",
      "Epoch: 918/25000, Training Loss: 0.003578, lr : 0.008500\n",
      "Epoch: 919/25000, Training Loss: 0.003770, lr : 0.008500\n",
      "Epoch: 920/25000, Training Loss: 0.003654, lr : 0.008500\n",
      "Epoch: 921/25000, Training Loss: 0.003476, lr : 0.008500\n",
      "Epoch: 922/25000, Training Loss: 0.003719, lr : 0.008500\n",
      "Epoch: 923/25000, Training Loss: 0.003567, lr : 0.008500\n",
      "Epoch: 924/25000, Training Loss: 0.003202, lr : 0.008500\n",
      "Epoch: 925/25000, Training Loss: 0.002917, lr : 0.008500\n",
      "Epoch: 926/25000, Training Loss: 0.004041, lr : 0.008500\n",
      "Epoch: 927/25000, Training Loss: 0.003548, lr : 0.008500\n",
      "Epoch: 928/25000, Training Loss: 0.004071, lr : 0.008500\n",
      "Epoch: 929/25000, Training Loss: 0.003324, lr : 0.008500\n",
      "Epoch: 930/25000, Training Loss: 0.003155, lr : 0.008500\n",
      "Epoch: 931/25000, Training Loss: 0.003641, lr : 0.008500\n",
      "Epoch: 932/25000, Training Loss: 0.003913, lr : 0.008500\n",
      "Epoch: 933/25000, Training Loss: 0.003662, lr : 0.008500\n",
      "Epoch: 934/25000, Training Loss: 0.003416, lr : 0.008500\n",
      "Epoch: 935/25000, Training Loss: 0.003439, lr : 0.008500\n",
      "Epoch: 936/25000, Training Loss: 0.003152, lr : 0.008500\n",
      "Epoch: 937/25000, Training Loss: 0.004307, lr : 0.008500\n",
      "Epoch: 938/25000, Training Loss: 0.003471, lr : 0.008500\n",
      "Epoch: 939/25000, Training Loss: 0.003479, lr : 0.008500\n",
      "Epoch: 940/25000, Training Loss: 0.003488, lr : 0.008500\n",
      "Epoch: 941/25000, Training Loss: 0.003324, lr : 0.008500\n",
      "Epoch: 942/25000, Training Loss: 0.002893, lr : 0.008500\n",
      "Epoch: 943/25000, Training Loss: 0.003724, lr : 0.008500\n",
      "Epoch: 944/25000, Training Loss: 0.003539, lr : 0.008500\n",
      "Epoch: 945/25000, Training Loss: 0.003459, lr : 0.008500\n",
      "Epoch: 946/25000, Training Loss: 0.003816, lr : 0.008500\n",
      "Epoch: 947/25000, Training Loss: 0.003228, lr : 0.008500\n",
      "Epoch: 948/25000, Training Loss: 0.003019, lr : 0.008500\n",
      "Epoch: 949/25000, Training Loss: 0.003466, lr : 0.008500\n",
      "Epoch: 950/25000, Training Loss: 0.002893, lr : 0.008500\n",
      "Epoch: 951/25000, Training Loss: 0.002907, lr : 0.008500\n",
      "Epoch: 952/25000, Training Loss: 0.003285, lr : 0.008500\n",
      "Epoch: 953/25000, Training Loss: 0.003685, lr : 0.008500\n",
      "Epoch: 954/25000, Training Loss: 0.004352, lr : 0.008500\n",
      "Epoch: 955/25000, Training Loss: 0.003638, lr : 0.008500\n",
      "Epoch: 956/25000, Training Loss: 0.003757, lr : 0.008500\n",
      "Epoch: 957/25000, Training Loss: 0.003295, lr : 0.008500\n",
      "Epoch: 958/25000, Training Loss: 0.002967, lr : 0.008500\n",
      "Epoch: 959/25000, Training Loss: 0.003704, lr : 0.008500\n",
      "Epoch: 960/25000, Training Loss: 0.003049, lr : 0.008500\n",
      "Epoch: 961/25000, Training Loss: 0.003251, lr : 0.008500\n",
      "Epoch: 962/25000, Training Loss: 0.003819, lr : 0.008500\n",
      "Epoch: 963/25000, Training Loss: 0.003597, lr : 0.008500\n",
      "Epoch: 964/25000, Training Loss: 0.003805, lr : 0.008500\n",
      "Epoch: 965/25000, Training Loss: 0.003875, lr : 0.008500\n",
      "Epoch: 966/25000, Training Loss: 0.003322, lr : 0.008500\n",
      "Epoch: 967/25000, Training Loss: 0.003247, lr : 0.008500\n",
      "Epoch: 968/25000, Training Loss: 0.002799, lr : 0.008500\n",
      "Epoch: 969/25000, Training Loss: 0.002986, lr : 0.008500\n",
      "Epoch: 970/25000, Training Loss: 0.003228, lr : 0.008500\n",
      "Epoch: 971/25000, Training Loss: 0.003455, lr : 0.008500\n",
      "Epoch: 972/25000, Training Loss: 0.002867, lr : 0.008500\n",
      "Epoch: 973/25000, Training Loss: 0.003305, lr : 0.008500\n",
      "Epoch: 974/25000, Training Loss: 0.003317, lr : 0.008500\n",
      "Epoch: 975/25000, Training Loss: 0.003114, lr : 0.008500\n",
      "Epoch: 976/25000, Training Loss: 0.002828, lr : 0.008500\n",
      "Epoch: 977/25000, Training Loss: 0.003209, lr : 0.008500\n",
      "Epoch: 978/25000, Training Loss: 0.003033, lr : 0.008500\n",
      "Epoch: 979/25000, Training Loss: 0.003006, lr : 0.008500\n",
      "Epoch: 980/25000, Training Loss: 0.002645, lr : 0.008500\n",
      "Epoch: 981/25000, Training Loss: 0.002841, lr : 0.008500\n",
      "Epoch: 982/25000, Training Loss: 0.003434, lr : 0.008500\n",
      "Epoch: 983/25000, Training Loss: 0.002621, lr : 0.008500\n",
      "Epoch: 984/25000, Training Loss: 0.003289, lr : 0.008500\n",
      "Epoch: 985/25000, Training Loss: 0.003329, lr : 0.008500\n",
      "Epoch: 986/25000, Training Loss: 0.003067, lr : 0.008500\n",
      "Epoch: 987/25000, Training Loss: 0.003583, lr : 0.008500\n",
      "Epoch: 988/25000, Training Loss: 0.002675, lr : 0.008500\n",
      "Epoch: 989/25000, Training Loss: 0.003329, lr : 0.008500\n",
      "Epoch: 990/25000, Training Loss: 0.002799, lr : 0.008500\n",
      "Epoch: 991/25000, Training Loss: 0.003491, lr : 0.008500\n",
      "Epoch: 992/25000, Training Loss: 0.003131, lr : 0.008500\n",
      "Epoch: 993/25000, Training Loss: 0.003502, lr : 0.008500\n",
      "Epoch: 994/25000, Training Loss: 0.002473, lr : 0.008500\n",
      "Epoch: 995/25000, Training Loss: 0.003336, lr : 0.008500\n",
      "Epoch: 996/25000, Training Loss: 0.003307, lr : 0.008500\n",
      "Epoch: 997/25000, Training Loss: 0.002638, lr : 0.008500\n",
      "Epoch: 998/25000, Training Loss: 0.003528, lr : 0.008500\n",
      "Epoch: 999/25000, Training Loss: 0.003166, lr : 0.008500\n",
      "Epoch: 1000/25000, Training Loss: 0.002638, lr : 0.008500\n",
      "Epoch: 1001/25000, Training Loss: 0.002809, lr : 0.008500\n",
      "Epoch: 1002/25000, Training Loss: 0.002782, lr : 0.008500\n",
      "Epoch: 1003/25000, Training Loss: 0.003133, lr : 0.008500\n",
      "Epoch: 1004/25000, Training Loss: 0.002911, lr : 0.008500\n",
      "Epoch: 1005/25000, Training Loss: 0.003279, lr : 0.008500\n",
      "Epoch: 1006/25000, Training Loss: 0.003491, lr : 0.008500\n",
      "Epoch: 1007/25000, Training Loss: 0.003007, lr : 0.008500\n",
      "Epoch: 1008/25000, Training Loss: 0.003580, lr : 0.008500\n",
      "Epoch: 1009/25000, Training Loss: 0.002912, lr : 0.008500\n",
      "Epoch: 1010/25000, Training Loss: 0.002858, lr : 0.008500\n",
      "Epoch: 1011/25000, Training Loss: 0.003517, lr : 0.008500\n",
      "Epoch: 1012/25000, Training Loss: 0.002750, lr : 0.008500\n",
      "Epoch: 1013/25000, Training Loss: 0.003122, lr : 0.008500\n",
      "Epoch: 1014/25000, Training Loss: 0.002431, lr : 0.008500\n",
      "Epoch: 1015/25000, Training Loss: 0.002689, lr : 0.008500\n",
      "Epoch: 1016/25000, Training Loss: 0.003127, lr : 0.008500\n",
      "Epoch: 1017/25000, Training Loss: 0.003541, lr : 0.008500\n",
      "Epoch: 1018/25000, Training Loss: 0.002448, lr : 0.008500\n",
      "Epoch: 1019/25000, Training Loss: 0.002814, lr : 0.008500\n",
      "Epoch: 1020/25000, Training Loss: 0.003546, lr : 0.008500\n",
      "Epoch: 1021/25000, Training Loss: 0.002976, lr : 0.008500\n",
      "Epoch: 1022/25000, Training Loss: 0.003044, lr : 0.008500\n",
      "Epoch: 1023/25000, Training Loss: 0.003053, lr : 0.008500\n",
      "Epoch: 1024/25000, Training Loss: 0.003352, lr : 0.008500\n",
      "Epoch: 1025/25000, Training Loss: 0.002647, lr : 0.008500\n",
      "Epoch: 1026/25000, Training Loss: 0.003131, lr : 0.008500\n",
      "Epoch: 1027/25000, Training Loss: 0.002861, lr : 0.008500\n",
      "Epoch: 1028/25000, Training Loss: 0.003182, lr : 0.008500\n",
      "Epoch: 1029/25000, Training Loss: 0.003219, lr : 0.008500\n",
      "Epoch: 1030/25000, Training Loss: 0.003272, lr : 0.008500\n",
      "Epoch: 1031/25000, Training Loss: 0.002862, lr : 0.008500\n",
      "Epoch: 1032/25000, Training Loss: 0.003069, lr : 0.008500\n",
      "Epoch: 1033/25000, Training Loss: 0.002600, lr : 0.008500\n",
      "Epoch: 1034/25000, Training Loss: 0.002807, lr : 0.008500\n",
      "Epoch: 1035/25000, Training Loss: 0.003153, lr : 0.008500\n",
      "Epoch: 1036/25000, Training Loss: 0.002873, lr : 0.008500\n",
      "Epoch: 1037/25000, Training Loss: 0.002520, lr : 0.008500\n",
      "Epoch: 1038/25000, Training Loss: 0.002963, lr : 0.008500\n",
      "Epoch: 1039/25000, Training Loss: 0.002703, lr : 0.008500\n",
      "Epoch: 1040/25000, Training Loss: 0.003107, lr : 0.008500\n",
      "Epoch: 1041/25000, Training Loss: 0.003514, lr : 0.008500\n",
      "Epoch: 1042/25000, Training Loss: 0.003307, lr : 0.008500\n",
      "Epoch: 1043/25000, Training Loss: 0.002674, lr : 0.008500\n",
      "Epoch: 1044/25000, Training Loss: 0.003198, lr : 0.008500\n",
      "Epoch: 1045/25000, Training Loss: 0.003531, lr : 0.008500\n",
      "Epoch: 1046/25000, Training Loss: 0.003490, lr : 0.008500\n",
      "Epoch: 1047/25000, Training Loss: 0.002997, lr : 0.008500\n",
      "Epoch: 1048/25000, Training Loss: 0.003166, lr : 0.008500\n",
      "Epoch: 1049/25000, Training Loss: 0.002813, lr : 0.008500\n",
      "Epoch: 1050/25000, Training Loss: 0.003261, lr : 0.008500\n",
      "Epoch: 1051/25000, Training Loss: 0.003190, lr : 0.008500\n",
      "Epoch: 1052/25000, Training Loss: 0.002797, lr : 0.008500\n",
      "Epoch: 1053/25000, Training Loss: 0.003480, lr : 0.008500\n",
      "Epoch: 1054/25000, Training Loss: 0.002548, lr : 0.008500\n",
      "Epoch: 1055/25000, Training Loss: 0.002840, lr : 0.008500\n",
      "Epoch: 1056/25000, Training Loss: 0.002771, lr : 0.008500\n",
      "Epoch: 1057/25000, Training Loss: 0.003186, lr : 0.008500\n",
      "Epoch: 1058/25000, Training Loss: 0.003024, lr : 0.008500\n",
      "Epoch: 1059/25000, Training Loss: 0.002475, lr : 0.008500\n",
      "Epoch: 1060/25000, Training Loss: 0.003082, lr : 0.008500\n",
      "Epoch: 1061/25000, Training Loss: 0.003429, lr : 0.008500\n",
      "Epoch: 1062/25000, Training Loss: 0.002737, lr : 0.008500\n",
      "Epoch: 1063/25000, Training Loss: 0.002789, lr : 0.008500\n",
      "Epoch: 1064/25000, Training Loss: 0.002864, lr : 0.008500\n",
      "Epoch: 1065/25000, Training Loss: 0.002575, lr : 0.008500\n",
      "Epoch: 1066/25000, Training Loss: 0.002828, lr : 0.008500\n",
      "Epoch: 1067/25000, Training Loss: 0.002494, lr : 0.008500\n",
      "Epoch: 1068/25000, Training Loss: 0.002948, lr : 0.008500\n",
      "Epoch: 1069/25000, Training Loss: 0.002790, lr : 0.008500\n",
      "Epoch: 1070/25000, Training Loss: 0.002647, lr : 0.008500\n",
      "Epoch: 1071/25000, Training Loss: 0.003158, lr : 0.008500\n",
      "Epoch: 1072/25000, Training Loss: 0.002756, lr : 0.008500\n",
      "Epoch: 1073/25000, Training Loss: 0.002986, lr : 0.008500\n",
      "Epoch: 1074/25000, Training Loss: 0.002685, lr : 0.008500\n",
      "Epoch: 1075/25000, Training Loss: 0.002981, lr : 0.008500\n",
      "Epoch: 1076/25000, Training Loss: 0.003333, lr : 0.008500\n",
      "Epoch: 1077/25000, Training Loss: 0.003160, lr : 0.008500\n",
      "Epoch: 1078/25000, Training Loss: 0.002982, lr : 0.008500\n",
      "Epoch: 1079/25000, Training Loss: 0.003115, lr : 0.008500\n",
      "Epoch: 1080/25000, Training Loss: 0.002509, lr : 0.008500\n",
      "Epoch: 1081/25000, Training Loss: 0.002794, lr : 0.008500\n",
      "Epoch: 1082/25000, Training Loss: 0.002883, lr : 0.008500\n",
      "Epoch: 1083/25000, Training Loss: 0.002818, lr : 0.008500\n",
      "Epoch: 1084/25000, Training Loss: 0.002822, lr : 0.008500\n",
      "Epoch: 1085/25000, Training Loss: 0.002907, lr : 0.008500\n",
      "Epoch: 1086/25000, Training Loss: 0.002893, lr : 0.008500\n",
      "Epoch: 1087/25000, Training Loss: 0.003263, lr : 0.008500\n",
      "Epoch: 1088/25000, Training Loss: 0.003089, lr : 0.008500\n",
      "Epoch: 1089/25000, Training Loss: 0.002839, lr : 0.008500\n",
      "Epoch: 1090/25000, Training Loss: 0.002382, lr : 0.008500\n",
      "Epoch: 1091/25000, Training Loss: 0.002742, lr : 0.008500\n",
      "Epoch: 1092/25000, Training Loss: 0.002448, lr : 0.008500\n",
      "Epoch: 1093/25000, Training Loss: 0.002902, lr : 0.008500\n",
      "Epoch: 1094/25000, Training Loss: 0.002774, lr : 0.008500\n",
      "Epoch: 1095/25000, Training Loss: 0.002948, lr : 0.008500\n",
      "Epoch: 1096/25000, Training Loss: 0.002368, lr : 0.008500\n",
      "Epoch: 1097/25000, Training Loss: 0.002658, lr : 0.008500\n",
      "Epoch: 1098/25000, Training Loss: 0.002720, lr : 0.008500\n",
      "Epoch: 1099/25000, Training Loss: 0.002934, lr : 0.008500\n",
      "Epoch: 1100/25000, Training Loss: 0.002913, lr : 0.008500\n",
      "Epoch: 1101/25000, Training Loss: 0.003076, lr : 0.008500\n",
      "Epoch: 1102/25000, Training Loss: 0.002957, lr : 0.008500\n",
      "Epoch: 1103/25000, Training Loss: 0.002383, lr : 0.008500\n",
      "Epoch: 1104/25000, Training Loss: 0.002458, lr : 0.008500\n",
      "Epoch: 1105/25000, Training Loss: 0.002629, lr : 0.008500\n",
      "Epoch: 1106/25000, Training Loss: 0.002633, lr : 0.008500\n",
      "Epoch: 1107/25000, Training Loss: 0.002486, lr : 0.008500\n",
      "Epoch: 1108/25000, Training Loss: 0.002706, lr : 0.008500\n",
      "Epoch: 1109/25000, Training Loss: 0.002799, lr : 0.008500\n",
      "Epoch: 1110/25000, Training Loss: 0.002756, lr : 0.008500\n",
      "Epoch: 1111/25000, Training Loss: 0.002576, lr : 0.008500\n",
      "Epoch: 1112/25000, Training Loss: 0.002990, lr : 0.008500\n",
      "Epoch: 1113/25000, Training Loss: 0.002588, lr : 0.008500\n",
      "Epoch: 1114/25000, Training Loss: 0.002425, lr : 0.008500\n",
      "Epoch: 1115/25000, Training Loss: 0.003019, lr : 0.008500\n",
      "Epoch: 1116/25000, Training Loss: 0.002698, lr : 0.008500\n",
      "Epoch: 1117/25000, Training Loss: 0.002857, lr : 0.008500\n",
      "Epoch: 1118/25000, Training Loss: 0.002931, lr : 0.008500\n",
      "Epoch: 1119/25000, Training Loss: 0.002438, lr : 0.008500\n",
      "Epoch: 1120/25000, Training Loss: 0.002603, lr : 0.008500\n",
      "Epoch: 1121/25000, Training Loss: 0.002651, lr : 0.008500\n",
      "Epoch: 1122/25000, Training Loss: 0.002753, lr : 0.008500\n",
      "Epoch: 1123/25000, Training Loss: 0.002701, lr : 0.008500\n",
      "Epoch: 1124/25000, Training Loss: 0.002848, lr : 0.008500\n",
      "Epoch: 1125/25000, Training Loss: 0.002321, lr : 0.008500\n",
      "Epoch: 1126/25000, Training Loss: 0.002906, lr : 0.008500\n",
      "Epoch: 1127/25000, Training Loss: 0.002780, lr : 0.008500\n",
      "Epoch: 1128/25000, Training Loss: 0.002901, lr : 0.008500\n",
      "Epoch: 1129/25000, Training Loss: 0.002520, lr : 0.008500\n",
      "Epoch: 1130/25000, Training Loss: 0.002451, lr : 0.008500\n",
      "Epoch: 1131/25000, Training Loss: 0.003132, lr : 0.008500\n",
      "Epoch: 1132/25000, Training Loss: 0.002899, lr : 0.008500\n",
      "Epoch: 1133/25000, Training Loss: 0.002369, lr : 0.008500\n",
      "Epoch: 1134/25000, Training Loss: 0.003110, lr : 0.008500\n",
      "Epoch: 1135/25000, Training Loss: 0.003152, lr : 0.008500\n",
      "Epoch: 1136/25000, Training Loss: 0.002779, lr : 0.008500\n",
      "Epoch: 1137/25000, Training Loss: 0.003232, lr : 0.008500\n",
      "Epoch: 1138/25000, Training Loss: 0.003065, lr : 0.008500\n",
      "Epoch: 1139/25000, Training Loss: 0.002760, lr : 0.008500\n",
      "Epoch: 1140/25000, Training Loss: 0.002918, lr : 0.008500\n",
      "Epoch: 1141/25000, Training Loss: 0.003108, lr : 0.008500\n",
      "Epoch: 1142/25000, Training Loss: 0.003250, lr : 0.008500\n",
      "Epoch: 1143/25000, Training Loss: 0.002676, lr : 0.008500\n",
      "Epoch: 1144/25000, Training Loss: 0.003237, lr : 0.008500\n",
      "Epoch: 1145/25000, Training Loss: 0.002467, lr : 0.008500\n",
      "Epoch: 1146/25000, Training Loss: 0.002485, lr : 0.008500\n",
      "Epoch: 1147/25000, Training Loss: 0.002435, lr : 0.008500\n",
      "Epoch: 1148/25000, Training Loss: 0.003038, lr : 0.008500\n",
      "Epoch: 1149/25000, Training Loss: 0.003011, lr : 0.008500\n",
      "Epoch: 1150/25000, Training Loss: 0.002367, lr : 0.008500\n",
      "Epoch: 1151/25000, Training Loss: 0.002606, lr : 0.008500\n",
      "Epoch: 1152/25000, Training Loss: 0.002960, lr : 0.008500\n",
      "Epoch: 1153/25000, Training Loss: 0.002864, lr : 0.008500\n",
      "Epoch: 1154/25000, Training Loss: 0.002437, lr : 0.008500\n",
      "Epoch: 1155/25000, Training Loss: 0.002382, lr : 0.008500\n",
      "Epoch: 1156/25000, Training Loss: 0.002847, lr : 0.008500\n",
      "Epoch: 1157/25000, Training Loss: 0.002453, lr : 0.008500\n",
      "Epoch: 1158/25000, Training Loss: 0.002772, lr : 0.008500\n",
      "Epoch: 1159/25000, Training Loss: 0.002639, lr : 0.008500\n",
      "Epoch: 1160/25000, Training Loss: 0.002762, lr : 0.008500\n",
      "Epoch: 1161/25000, Training Loss: 0.002283, lr : 0.008500\n",
      "Epoch: 1162/25000, Training Loss: 0.002456, lr : 0.008500\n",
      "Epoch: 1163/25000, Training Loss: 0.002980, lr : 0.008500\n",
      "Epoch: 1164/25000, Training Loss: 0.002774, lr : 0.008500\n",
      "Epoch: 1165/25000, Training Loss: 0.002530, lr : 0.008500\n",
      "Epoch: 1166/25000, Training Loss: 0.002529, lr : 0.008500\n",
      "Epoch: 1167/25000, Training Loss: 0.002973, lr : 0.008500\n",
      "Epoch: 1168/25000, Training Loss: 0.002403, lr : 0.008500\n",
      "Epoch: 1169/25000, Training Loss: 0.002500, lr : 0.008500\n",
      "Epoch: 1170/25000, Training Loss: 0.002207, lr : 0.008500\n",
      "Epoch: 1171/25000, Training Loss: 0.002283, lr : 0.008500\n",
      "Epoch: 1172/25000, Training Loss: 0.002700, lr : 0.008500\n",
      "Epoch: 1173/25000, Training Loss: 0.002578, lr : 0.008500\n",
      "Epoch: 1174/25000, Training Loss: 0.002312, lr : 0.008500\n",
      "Epoch: 1175/25000, Training Loss: 0.002480, lr : 0.008500\n",
      "Epoch: 1176/25000, Training Loss: 0.002389, lr : 0.008500\n",
      "Epoch: 1177/25000, Training Loss: 0.002483, lr : 0.008500\n",
      "Epoch: 1178/25000, Training Loss: 0.002966, lr : 0.008500\n",
      "Epoch: 1179/25000, Training Loss: 0.002804, lr : 0.008500\n",
      "Epoch: 1180/25000, Training Loss: 0.002368, lr : 0.008500\n",
      "Epoch: 1181/25000, Training Loss: 0.002448, lr : 0.008500\n",
      "Epoch: 1182/25000, Training Loss: 0.002516, lr : 0.008500\n",
      "Epoch: 1183/25000, Training Loss: 0.002660, lr : 0.008500\n",
      "Epoch: 1184/25000, Training Loss: 0.002096, lr : 0.008500\n",
      "Epoch: 1185/25000, Training Loss: 0.002643, lr : 0.008500\n",
      "Epoch: 1186/25000, Training Loss: 0.002983, lr : 0.008500\n",
      "Epoch: 1187/25000, Training Loss: 0.002637, lr : 0.008500\n",
      "Epoch: 1188/25000, Training Loss: 0.003097, lr : 0.008500\n",
      "Epoch: 1189/25000, Training Loss: 0.002156, lr : 0.008500\n",
      "Epoch: 1190/25000, Training Loss: 0.002700, lr : 0.008500\n",
      "Epoch: 1191/25000, Training Loss: 0.002497, lr : 0.008500\n",
      "Epoch: 1192/25000, Training Loss: 0.002748, lr : 0.008500\n",
      "Epoch: 1193/25000, Training Loss: 0.002204, lr : 0.008500\n",
      "Epoch: 1194/25000, Training Loss: 0.002874, lr : 0.008500\n",
      "Epoch: 1195/25000, Training Loss: 0.002504, lr : 0.008500\n",
      "Epoch: 1196/25000, Training Loss: 0.002144, lr : 0.008500\n",
      "Epoch: 1197/25000, Training Loss: 0.002620, lr : 0.008500\n",
      "Epoch: 1198/25000, Training Loss: 0.002344, lr : 0.008500\n",
      "Epoch: 1199/25000, Training Loss: 0.002611, lr : 0.008500\n",
      "Epoch: 1200/25000, Training Loss: 0.002857, lr : 0.008500\n",
      "Epoch: 1201/25000, Training Loss: 0.002472, lr : 0.008500\n",
      "Epoch: 1202/25000, Training Loss: 0.002962, lr : 0.008500\n",
      "Epoch: 1203/25000, Training Loss: 0.002358, lr : 0.008500\n",
      "Epoch: 1204/25000, Training Loss: 0.002674, lr : 0.008500\n",
      "Epoch: 1205/25000, Training Loss: 0.002595, lr : 0.008500\n",
      "Epoch: 1206/25000, Training Loss: 0.002627, lr : 0.008500\n",
      "Epoch: 1207/25000, Training Loss: 0.002084, lr : 0.008500\n",
      "Epoch: 1208/25000, Training Loss: 0.002558, lr : 0.008500\n",
      "Epoch: 1209/25000, Training Loss: 0.002617, lr : 0.008500\n",
      "Epoch: 1210/25000, Training Loss: 0.002443, lr : 0.008500\n",
      "Epoch: 1211/25000, Training Loss: 0.002519, lr : 0.008500\n",
      "Epoch: 1212/25000, Training Loss: 0.002687, lr : 0.008500\n",
      "Epoch: 1213/25000, Training Loss: 0.002433, lr : 0.008500\n",
      "Epoch: 1214/25000, Training Loss: 0.003050, lr : 0.008500\n",
      "Epoch: 1215/25000, Training Loss: 0.002205, lr : 0.008500\n",
      "Epoch: 1216/25000, Training Loss: 0.002679, lr : 0.008500\n",
      "Epoch: 1217/25000, Training Loss: 0.002950, lr : 0.008500\n",
      "Epoch: 1218/25000, Training Loss: 0.002603, lr : 0.008500\n",
      "Epoch: 1219/25000, Training Loss: 0.002528, lr : 0.008500\n",
      "Epoch: 1220/25000, Training Loss: 0.002501, lr : 0.008500\n",
      "Epoch: 1221/25000, Training Loss: 0.002842, lr : 0.008500\n",
      "Epoch: 1222/25000, Training Loss: 0.002424, lr : 0.008500\n",
      "Epoch: 1223/25000, Training Loss: 0.001974, lr : 0.008500\n",
      "Epoch: 1224/25000, Training Loss: 0.002425, lr : 0.008500\n",
      "Epoch: 1225/25000, Training Loss: 0.002508, lr : 0.008500\n",
      "Epoch: 1226/25000, Training Loss: 0.002753, lr : 0.008500\n",
      "Epoch: 1227/25000, Training Loss: 0.002045, lr : 0.008500\n",
      "Epoch: 1228/25000, Training Loss: 0.003030, lr : 0.008500\n",
      "Epoch: 1229/25000, Training Loss: 0.002792, lr : 0.008500\n",
      "Epoch: 1230/25000, Training Loss: 0.002453, lr : 0.008500\n",
      "Epoch: 1231/25000, Training Loss: 0.002899, lr : 0.008500\n",
      "Epoch: 1232/25000, Training Loss: 0.002411, lr : 0.008500\n",
      "Epoch: 1233/25000, Training Loss: 0.002727, lr : 0.008500\n",
      "Epoch: 1234/25000, Training Loss: 0.002554, lr : 0.008500\n",
      "Epoch: 1235/25000, Training Loss: 0.002607, lr : 0.008500\n",
      "Epoch: 1236/25000, Training Loss: 0.002144, lr : 0.008500\n",
      "Epoch: 1237/25000, Training Loss: 0.002047, lr : 0.008500\n",
      "Epoch: 1238/25000, Training Loss: 0.002777, lr : 0.008500\n",
      "Epoch: 1239/25000, Training Loss: 0.002486, lr : 0.008500\n",
      "Epoch: 1240/25000, Training Loss: 0.002274, lr : 0.008500\n",
      "Epoch: 1241/25000, Training Loss: 0.002903, lr : 0.008500\n",
      "Epoch: 1242/25000, Training Loss: 0.002517, lr : 0.008500\n",
      "Epoch: 1243/25000, Training Loss: 0.002757, lr : 0.008500\n",
      "Epoch: 1244/25000, Training Loss: 0.002468, lr : 0.008500\n",
      "Epoch: 1245/25000, Training Loss: 0.002101, lr : 0.008500\n",
      "Epoch: 1246/25000, Training Loss: 0.002005, lr : 0.008500\n",
      "Epoch: 1247/25000, Training Loss: 0.002640, lr : 0.008500\n",
      "Epoch: 1248/25000, Training Loss: 0.002588, lr : 0.008500\n",
      "Epoch: 1249/25000, Training Loss: 0.002713, lr : 0.008500\n",
      "Epoch: 1250/25000, Training Loss: 0.002051, lr : 0.008500\n",
      "Epoch: 1251/25000, Training Loss: 0.002051, lr : 0.008500\n",
      "Epoch: 1252/25000, Training Loss: 0.002385, lr : 0.008500\n",
      "Epoch: 1253/25000, Training Loss: 0.001914, lr : 0.008500\n",
      "Epoch: 1254/25000, Training Loss: 0.002136, lr : 0.008500\n",
      "Epoch: 1255/25000, Training Loss: 0.002590, lr : 0.008500\n",
      "Epoch: 1256/25000, Training Loss: 0.002455, lr : 0.008500\n",
      "Epoch: 1257/25000, Training Loss: 0.002235, lr : 0.008500\n",
      "Epoch: 1258/25000, Training Loss: 0.002572, lr : 0.008500\n",
      "Epoch: 1259/25000, Training Loss: 0.002764, lr : 0.008500\n",
      "Epoch: 1260/25000, Training Loss: 0.002830, lr : 0.008500\n",
      "Epoch: 1261/25000, Training Loss: 0.002621, lr : 0.008500\n",
      "Epoch: 1262/25000, Training Loss: 0.002599, lr : 0.008500\n",
      "Epoch: 1263/25000, Training Loss: 0.002191, lr : 0.008500\n",
      "Epoch: 1264/25000, Training Loss: 0.002684, lr : 0.008500\n",
      "Epoch: 1265/25000, Training Loss: 0.002923, lr : 0.008500\n",
      "Epoch: 1266/25000, Training Loss: 0.002754, lr : 0.008500\n",
      "Epoch: 1267/25000, Training Loss: 0.002497, lr : 0.008500\n",
      "Epoch: 1268/25000, Training Loss: 0.002273, lr : 0.008500\n",
      "Epoch: 1269/25000, Training Loss: 0.002321, lr : 0.008500\n",
      "Epoch: 1270/25000, Training Loss: 0.002313, lr : 0.008500\n",
      "Epoch: 1271/25000, Training Loss: 0.002425, lr : 0.008500\n",
      "Epoch: 1272/25000, Training Loss: 0.002314, lr : 0.008500\n",
      "Epoch: 1273/25000, Training Loss: 0.002272, lr : 0.008500\n",
      "Epoch: 1274/25000, Training Loss: 0.002266, lr : 0.008500\n",
      "Epoch: 1275/25000, Training Loss: 0.002395, lr : 0.008500\n",
      "Epoch: 1276/25000, Training Loss: 0.002340, lr : 0.008500\n",
      "Epoch: 1277/25000, Training Loss: 0.002414, lr : 0.008500\n",
      "Epoch: 1278/25000, Training Loss: 0.002390, lr : 0.008500\n",
      "Epoch: 1279/25000, Training Loss: 0.001884, lr : 0.008500\n",
      "Epoch: 1280/25000, Training Loss: 0.001993, lr : 0.008500\n",
      "Epoch: 1281/25000, Training Loss: 0.002361, lr : 0.008500\n",
      "Epoch: 1282/25000, Training Loss: 0.002735, lr : 0.008500\n",
      "Epoch: 1283/25000, Training Loss: 0.002486, lr : 0.008500\n",
      "Epoch: 1284/25000, Training Loss: 0.001987, lr : 0.008500\n",
      "Epoch: 1285/25000, Training Loss: 0.002277, lr : 0.008500\n",
      "Epoch: 1286/25000, Training Loss: 0.002443, lr : 0.008500\n",
      "Epoch: 1287/25000, Training Loss: 0.002366, lr : 0.008500\n",
      "Epoch: 1288/25000, Training Loss: 0.002284, lr : 0.008500\n",
      "Epoch: 1289/25000, Training Loss: 0.002422, lr : 0.008500\n",
      "Epoch: 1290/25000, Training Loss: 0.002005, lr : 0.008500\n",
      "Epoch: 1291/25000, Training Loss: 0.002720, lr : 0.008500\n",
      "Epoch: 1292/25000, Training Loss: 0.002707, lr : 0.008500\n",
      "Epoch: 1293/25000, Training Loss: 0.002621, lr : 0.008500\n",
      "Epoch: 1294/25000, Training Loss: 0.002673, lr : 0.008500\n",
      "Epoch: 1295/25000, Training Loss: 0.003022, lr : 0.008500\n",
      "Epoch: 1296/25000, Training Loss: 0.002960, lr : 0.008500\n",
      "Epoch: 1297/25000, Training Loss: 0.002472, lr : 0.008500\n",
      "Epoch: 1298/25000, Training Loss: 0.002330, lr : 0.008500\n",
      "Epoch: 1299/25000, Training Loss: 0.002159, lr : 0.008500\n",
      "Epoch: 1300/25000, Training Loss: 0.002505, lr : 0.008500\n",
      "Epoch: 1301/25000, Training Loss: 0.002537, lr : 0.008500\n",
      "Epoch: 1302/25000, Training Loss: 0.002338, lr : 0.008500\n",
      "Epoch: 1303/25000, Training Loss: 0.001985, lr : 0.008500\n",
      "Epoch: 1304/25000, Training Loss: 0.002454, lr : 0.008500\n",
      "Epoch: 1305/25000, Training Loss: 0.002500, lr : 0.008500\n",
      "Epoch: 1306/25000, Training Loss: 0.002312, lr : 0.008500\n",
      "Epoch: 1307/25000, Training Loss: 0.002475, lr : 0.008500\n",
      "Epoch: 1308/25000, Training Loss: 0.002517, lr : 0.008500\n",
      "Epoch: 1309/25000, Training Loss: 0.001923, lr : 0.008500\n",
      "Epoch: 1310/25000, Training Loss: 0.002180, lr : 0.008500\n",
      "Epoch: 1311/25000, Training Loss: 0.002186, lr : 0.008500\n",
      "Epoch: 1312/25000, Training Loss: 0.001849, lr : 0.008500\n",
      "Epoch: 1313/25000, Training Loss: 0.002191, lr : 0.008500\n",
      "Epoch: 1314/25000, Training Loss: 0.002108, lr : 0.008500\n",
      "Epoch: 1315/25000, Training Loss: 0.002129, lr : 0.008500\n",
      "Epoch: 1316/25000, Training Loss: 0.002523, lr : 0.008500\n",
      "Epoch: 1317/25000, Training Loss: 0.002357, lr : 0.008500\n",
      "Epoch: 1318/25000, Training Loss: 0.002049, lr : 0.008500\n",
      "Epoch: 1319/25000, Training Loss: 0.002036, lr : 0.008500\n",
      "Epoch: 1320/25000, Training Loss: 0.002383, lr : 0.008500\n",
      "Epoch: 1321/25000, Training Loss: 0.002018, lr : 0.008500\n",
      "Epoch: 1322/25000, Training Loss: 0.002102, lr : 0.008500\n",
      "Epoch: 1323/25000, Training Loss: 0.002387, lr : 0.008500\n",
      "Epoch: 1324/25000, Training Loss: 0.002077, lr : 0.008500\n",
      "Epoch: 1325/25000, Training Loss: 0.002320, lr : 0.008500\n",
      "Epoch: 1326/25000, Training Loss: 0.002061, lr : 0.008500\n",
      "Epoch: 1327/25000, Training Loss: 0.001958, lr : 0.008500\n",
      "Epoch: 1328/25000, Training Loss: 0.002144, lr : 0.008500\n",
      "Epoch: 1329/25000, Training Loss: 0.001939, lr : 0.008500\n",
      "Epoch: 1330/25000, Training Loss: 0.002747, lr : 0.008500\n",
      "Epoch: 1331/25000, Training Loss: 0.001908, lr : 0.008500\n",
      "Epoch: 1332/25000, Training Loss: 0.002354, lr : 0.008500\n",
      "Epoch: 1333/25000, Training Loss: 0.002541, lr : 0.008500\n",
      "Epoch: 1334/25000, Training Loss: 0.002473, lr : 0.008500\n",
      "Epoch: 1335/25000, Training Loss: 0.002451, lr : 0.008500\n",
      "Epoch: 1336/25000, Training Loss: 0.002286, lr : 0.008500\n",
      "Epoch: 1337/25000, Training Loss: 0.002058, lr : 0.008500\n",
      "Epoch: 1338/25000, Training Loss: 0.002803, lr : 0.008500\n",
      "Epoch: 1339/25000, Training Loss: 0.002300, lr : 0.008500\n",
      "Epoch: 1340/25000, Training Loss: 0.002238, lr : 0.008500\n",
      "Epoch: 1341/25000, Training Loss: 0.002016, lr : 0.008500\n",
      "Epoch: 1342/25000, Training Loss: 0.001900, lr : 0.008500\n",
      "Epoch: 1343/25000, Training Loss: 0.002014, lr : 0.008500\n",
      "Epoch: 1344/25000, Training Loss: 0.002110, lr : 0.008500\n",
      "Epoch: 1345/25000, Training Loss: 0.002141, lr : 0.008500\n",
      "Epoch: 1346/25000, Training Loss: 0.002083, lr : 0.008500\n",
      "Epoch: 1347/25000, Training Loss: 0.002405, lr : 0.008500\n",
      "Epoch: 1348/25000, Training Loss: 0.002627, lr : 0.008500\n",
      "Epoch: 1349/25000, Training Loss: 0.002225, lr : 0.008500\n",
      "Epoch: 1350/25000, Training Loss: 0.002063, lr : 0.008500\n",
      "Epoch: 1351/25000, Training Loss: 0.002179, lr : 0.008500\n",
      "Epoch: 1352/25000, Training Loss: 0.002094, lr : 0.008500\n",
      "Epoch: 1353/25000, Training Loss: 0.002062, lr : 0.008500\n",
      "Epoch: 1354/25000, Training Loss: 0.002226, lr : 0.008500\n",
      "Epoch: 1355/25000, Training Loss: 0.001845, lr : 0.008500\n",
      "Epoch: 1356/25000, Training Loss: 0.002106, lr : 0.008500\n",
      "Epoch: 1357/25000, Training Loss: 0.002104, lr : 0.008500\n",
      "Epoch: 1358/25000, Training Loss: 0.002418, lr : 0.008500\n",
      "Epoch: 1359/25000, Training Loss: 0.002222, lr : 0.008500\n",
      "Epoch: 1360/25000, Training Loss: 0.002307, lr : 0.008500\n",
      "Epoch: 1361/25000, Training Loss: 0.002071, lr : 0.008500\n",
      "Epoch: 1362/25000, Training Loss: 0.001969, lr : 0.008500\n",
      "Epoch: 1363/25000, Training Loss: 0.002218, lr : 0.008500\n",
      "Epoch: 1364/25000, Training Loss: 0.002059, lr : 0.008500\n",
      "Epoch: 1365/25000, Training Loss: 0.002009, lr : 0.008500\n",
      "Epoch: 1366/25000, Training Loss: 0.002166, lr : 0.008500\n",
      "Epoch: 1367/25000, Training Loss: 0.001722, lr : 0.008500\n",
      "Epoch: 1368/25000, Training Loss: 0.001801, lr : 0.008500\n",
      "Epoch: 1369/25000, Training Loss: 0.002190, lr : 0.008500\n",
      "Epoch: 1370/25000, Training Loss: 0.002119, lr : 0.008500\n",
      "Epoch: 1371/25000, Training Loss: 0.002115, lr : 0.008500\n",
      "Epoch: 1372/25000, Training Loss: 0.001863, lr : 0.008500\n",
      "Epoch: 1373/25000, Training Loss: 0.001946, lr : 0.008500\n",
      "Epoch: 1374/25000, Training Loss: 0.002041, lr : 0.008500\n",
      "Epoch: 1375/25000, Training Loss: 0.002164, lr : 0.008500\n",
      "Epoch: 1376/25000, Training Loss: 0.001876, lr : 0.008500\n",
      "Epoch: 1377/25000, Training Loss: 0.002147, lr : 0.008500\n",
      "Epoch: 1378/25000, Training Loss: 0.002040, lr : 0.008500\n",
      "Epoch: 1379/25000, Training Loss: 0.001992, lr : 0.008500\n",
      "Epoch: 1380/25000, Training Loss: 0.002384, lr : 0.008500\n",
      "Epoch: 1381/25000, Training Loss: 0.002216, lr : 0.008500\n",
      "Epoch: 1382/25000, Training Loss: 0.002141, lr : 0.008500\n",
      "Epoch: 1383/25000, Training Loss: 0.002144, lr : 0.008500\n",
      "Epoch: 1384/25000, Training Loss: 0.001856, lr : 0.008500\n",
      "Epoch: 1385/25000, Training Loss: 0.001939, lr : 0.008500\n",
      "Epoch: 1386/25000, Training Loss: 0.001829, lr : 0.008500\n",
      "Epoch: 1387/25000, Training Loss: 0.001906, lr : 0.008500\n",
      "Epoch: 1388/25000, Training Loss: 0.002070, lr : 0.008500\n",
      "Epoch: 1389/25000, Training Loss: 0.001978, lr : 0.008500\n",
      "Epoch: 1390/25000, Training Loss: 0.001878, lr : 0.008500\n",
      "Epoch: 1391/25000, Training Loss: 0.002269, lr : 0.008500\n",
      "Epoch: 1392/25000, Training Loss: 0.002067, lr : 0.008500\n",
      "Epoch: 1393/25000, Training Loss: 0.002133, lr : 0.008500\n",
      "Epoch: 1394/25000, Training Loss: 0.002173, lr : 0.008500\n",
      "Epoch: 1395/25000, Training Loss: 0.002148, lr : 0.008500\n",
      "Epoch: 1396/25000, Training Loss: 0.002051, lr : 0.008500\n",
      "Epoch: 1397/25000, Training Loss: 0.002415, lr : 0.008500\n",
      "Epoch: 1398/25000, Training Loss: 0.002192, lr : 0.008500\n",
      "Epoch: 1399/25000, Training Loss: 0.002207, lr : 0.008500\n",
      "Epoch: 1400/25000, Training Loss: 0.002403, lr : 0.008500\n",
      "Epoch: 1401/25000, Training Loss: 0.001976, lr : 0.008500\n",
      "Epoch: 1402/25000, Training Loss: 0.002131, lr : 0.008500\n",
      "Epoch: 1403/25000, Training Loss: 0.001787, lr : 0.008500\n",
      "Epoch: 1404/25000, Training Loss: 0.001894, lr : 0.008500\n",
      "Epoch: 1405/25000, Training Loss: 0.002148, lr : 0.008500\n",
      "Epoch: 1406/25000, Training Loss: 0.001951, lr : 0.008500\n",
      "Epoch: 1407/25000, Training Loss: 0.002682, lr : 0.008500\n",
      "Epoch: 1408/25000, Training Loss: 0.001771, lr : 0.008500\n",
      "Epoch: 1409/25000, Training Loss: 0.002032, lr : 0.008500\n",
      "Epoch: 1410/25000, Training Loss: 0.002158, lr : 0.008500\n",
      "Epoch: 1411/25000, Training Loss: 0.002119, lr : 0.008500\n",
      "Epoch: 1412/25000, Training Loss: 0.002101, lr : 0.008500\n",
      "Epoch: 1413/25000, Training Loss: 0.001971, lr : 0.008500\n",
      "Epoch: 1414/25000, Training Loss: 0.002050, lr : 0.008500\n",
      "Epoch: 1415/25000, Training Loss: 0.001785, lr : 0.008500\n",
      "Epoch: 1416/25000, Training Loss: 0.002432, lr : 0.008500\n",
      "Epoch: 1417/25000, Training Loss: 0.002220, lr : 0.008500\n",
      "Epoch: 1418/25000, Training Loss: 0.002148, lr : 0.008500\n",
      "Epoch: 1419/25000, Training Loss: 0.002164, lr : 0.008500\n",
      "Epoch: 1420/25000, Training Loss: 0.002261, lr : 0.008500\n",
      "Epoch: 1421/25000, Training Loss: 0.002258, lr : 0.008500\n",
      "Epoch: 1422/25000, Training Loss: 0.002157, lr : 0.008500\n",
      "Epoch: 1423/25000, Training Loss: 0.002070, lr : 0.008500\n",
      "Epoch: 1424/25000, Training Loss: 0.002128, lr : 0.008500\n",
      "Epoch: 1425/25000, Training Loss: 0.002129, lr : 0.008500\n",
      "Epoch: 1426/25000, Training Loss: 0.001820, lr : 0.008500\n",
      "Epoch: 1427/25000, Training Loss: 0.001814, lr : 0.008500\n",
      "Epoch: 1428/25000, Training Loss: 0.002004, lr : 0.008500\n",
      "Epoch: 1429/25000, Training Loss: 0.001959, lr : 0.008500\n",
      "Epoch: 1430/25000, Training Loss: 0.001903, lr : 0.008500\n",
      "Epoch: 1431/25000, Training Loss: 0.001938, lr : 0.008500\n",
      "Epoch: 1432/25000, Training Loss: 0.001746, lr : 0.008500\n",
      "Epoch: 1433/25000, Training Loss: 0.001808, lr : 0.008500\n",
      "Epoch: 1434/25000, Training Loss: 0.001524, lr : 0.008500\n",
      "Epoch: 1435/25000, Training Loss: 0.001724, lr : 0.008500\n",
      "Epoch: 1436/25000, Training Loss: 0.002291, lr : 0.008500\n",
      "Epoch: 1437/25000, Training Loss: 0.001827, lr : 0.008500\n",
      "Epoch: 1438/25000, Training Loss: 0.001782, lr : 0.008500\n",
      "Epoch: 1439/25000, Training Loss: 0.002036, lr : 0.008500\n",
      "Epoch: 1440/25000, Training Loss: 0.002056, lr : 0.008500\n",
      "Epoch: 1441/25000, Training Loss: 0.002247, lr : 0.008500\n",
      "Epoch: 1442/25000, Training Loss: 0.001755, lr : 0.008500\n",
      "Epoch: 1443/25000, Training Loss: 0.001895, lr : 0.008500\n",
      "Epoch: 1444/25000, Training Loss: 0.001947, lr : 0.008500\n",
      "Epoch: 1445/25000, Training Loss: 0.001962, lr : 0.008500\n",
      "Epoch: 1446/25000, Training Loss: 0.001791, lr : 0.008500\n",
      "Epoch: 1447/25000, Training Loss: 0.001908, lr : 0.008500\n",
      "Epoch: 1448/25000, Training Loss: 0.001906, lr : 0.008500\n",
      "Epoch: 1449/25000, Training Loss: 0.001965, lr : 0.008500\n",
      "Epoch: 1450/25000, Training Loss: 0.001834, lr : 0.008500\n",
      "Epoch: 1451/25000, Training Loss: 0.001720, lr : 0.008500\n",
      "Epoch: 1452/25000, Training Loss: 0.002407, lr : 0.008500\n",
      "Epoch: 1453/25000, Training Loss: 0.001769, lr : 0.008500\n",
      "Epoch: 1454/25000, Training Loss: 0.002107, lr : 0.008500\n",
      "Epoch: 1455/25000, Training Loss: 0.002076, lr : 0.008500\n",
      "Epoch: 1456/25000, Training Loss: 0.001842, lr : 0.008500\n",
      "Epoch: 1457/25000, Training Loss: 0.002101, lr : 0.008500\n",
      "Epoch: 1458/25000, Training Loss: 0.001808, lr : 0.008500\n",
      "Epoch: 1459/25000, Training Loss: 0.001950, lr : 0.008500\n",
      "Epoch: 1460/25000, Training Loss: 0.002211, lr : 0.008500\n",
      "Epoch: 1461/25000, Training Loss: 0.001950, lr : 0.008500\n",
      "Epoch: 1462/25000, Training Loss: 0.002084, lr : 0.008500\n",
      "Epoch: 1463/25000, Training Loss: 0.002210, lr : 0.008500\n",
      "Epoch: 1464/25000, Training Loss: 0.001617, lr : 0.008500\n",
      "Epoch: 1465/25000, Training Loss: 0.002022, lr : 0.008500\n",
      "Epoch: 1466/25000, Training Loss: 0.001901, lr : 0.008500\n",
      "Epoch: 1467/25000, Training Loss: 0.002174, lr : 0.008500\n",
      "Epoch: 1468/25000, Training Loss: 0.001869, lr : 0.008500\n",
      "Epoch: 1469/25000, Training Loss: 0.002086, lr : 0.008500\n",
      "Epoch: 1470/25000, Training Loss: 0.001811, lr : 0.008500\n",
      "Epoch: 1471/25000, Training Loss: 0.001746, lr : 0.008500\n",
      "Epoch: 1472/25000, Training Loss: 0.001834, lr : 0.008500\n",
      "Epoch: 1473/25000, Training Loss: 0.001788, lr : 0.008500\n",
      "Epoch: 1474/25000, Training Loss: 0.001812, lr : 0.008500\n",
      "Epoch: 1475/25000, Training Loss: 0.001711, lr : 0.008500\n",
      "Epoch: 1476/25000, Training Loss: 0.001949, lr : 0.008500\n",
      "Epoch: 1477/25000, Training Loss: 0.001493, lr : 0.008500\n",
      "Epoch: 1478/25000, Training Loss: 0.001753, lr : 0.008500\n",
      "Epoch: 1479/25000, Training Loss: 0.001476, lr : 0.008500\n",
      "Epoch: 1480/25000, Training Loss: 0.001774, lr : 0.008500\n",
      "Epoch: 1481/25000, Training Loss: 0.002130, lr : 0.008500\n",
      "Epoch: 1482/25000, Training Loss: 0.001935, lr : 0.008500\n",
      "Epoch: 1483/25000, Training Loss: 0.001881, lr : 0.008500\n",
      "Epoch: 1484/25000, Training Loss: 0.002022, lr : 0.008500\n",
      "Epoch: 1485/25000, Training Loss: 0.001726, lr : 0.008500\n",
      "Epoch: 1486/25000, Training Loss: 0.002026, lr : 0.008500\n",
      "Epoch: 1487/25000, Training Loss: 0.001625, lr : 0.008500\n",
      "Epoch: 1488/25000, Training Loss: 0.001818, lr : 0.008500\n",
      "Epoch: 1489/25000, Training Loss: 0.001566, lr : 0.008500\n",
      "Epoch: 1490/25000, Training Loss: 0.001875, lr : 0.008500\n",
      "Epoch: 1491/25000, Training Loss: 0.002075, lr : 0.008500\n",
      "Epoch: 1492/25000, Training Loss: 0.001925, lr : 0.008500\n",
      "Epoch: 1493/25000, Training Loss: 0.002148, lr : 0.008500\n",
      "Epoch: 1494/25000, Training Loss: 0.001829, lr : 0.008500\n",
      "Epoch: 1495/25000, Training Loss: 0.002150, lr : 0.008500\n",
      "Epoch: 1496/25000, Training Loss: 0.001837, lr : 0.008500\n",
      "Epoch: 1497/25000, Training Loss: 0.002424, lr : 0.008500\n",
      "Epoch: 1498/25000, Training Loss: 0.002149, lr : 0.008500\n",
      "Epoch: 1499/25000, Training Loss: 0.001759, lr : 0.008500\n",
      "Epoch: 1500/25000, Training Loss: 0.002312, lr : 0.008500\n",
      "Epoch: 1501/25000, Training Loss: 0.001973, lr : 0.008500\n",
      "Epoch: 1502/25000, Training Loss: 0.002173, lr : 0.008500\n",
      "Epoch: 1503/25000, Training Loss: 0.002253, lr : 0.008500\n",
      "Epoch: 1504/25000, Training Loss: 0.001887, lr : 0.008500\n",
      "Epoch: 1505/25000, Training Loss: 0.002010, lr : 0.008500\n",
      "Epoch: 1506/25000, Training Loss: 0.001892, lr : 0.008500\n",
      "Epoch: 1507/25000, Training Loss: 0.002050, lr : 0.008500\n",
      "Epoch: 1508/25000, Training Loss: 0.001969, lr : 0.008500\n",
      "Epoch: 1509/25000, Training Loss: 0.001931, lr : 0.008500\n",
      "Epoch: 1510/25000, Training Loss: 0.002228, lr : 0.008500\n",
      "Epoch: 1511/25000, Training Loss: 0.001944, lr : 0.008500\n",
      "Epoch: 1512/25000, Training Loss: 0.001424, lr : 0.008500\n",
      "Epoch: 1513/25000, Training Loss: 0.002292, lr : 0.008500\n",
      "Epoch: 1514/25000, Training Loss: 0.001896, lr : 0.008500\n",
      "Epoch: 1515/25000, Training Loss: 0.001852, lr : 0.008500\n",
      "Epoch: 1516/25000, Training Loss: 0.001682, lr : 0.008500\n",
      "Epoch: 1517/25000, Training Loss: 0.002203, lr : 0.008500\n",
      "Epoch: 1518/25000, Training Loss: 0.001697, lr : 0.008500\n",
      "Epoch: 1519/25000, Training Loss: 0.001640, lr : 0.008500\n",
      "Epoch: 1520/25000, Training Loss: 0.001965, lr : 0.008500\n",
      "Epoch: 1521/25000, Training Loss: 0.001966, lr : 0.008500\n",
      "Epoch: 1522/25000, Training Loss: 0.001519, lr : 0.008500\n",
      "Epoch: 1523/25000, Training Loss: 0.002090, lr : 0.008500\n",
      "Epoch: 1524/25000, Training Loss: 0.001680, lr : 0.008500\n",
      "Epoch: 1525/25000, Training Loss: 0.001729, lr : 0.008500\n",
      "Epoch: 1526/25000, Training Loss: 0.001898, lr : 0.008500\n",
      "Epoch: 1527/25000, Training Loss: 0.002094, lr : 0.008500\n",
      "Epoch: 1528/25000, Training Loss: 0.001688, lr : 0.008500\n",
      "Epoch: 1529/25000, Training Loss: 0.001709, lr : 0.008500\n",
      "Epoch: 1530/25000, Training Loss: 0.002017, lr : 0.008500\n",
      "Epoch: 1531/25000, Training Loss: 0.002087, lr : 0.008500\n",
      "Epoch: 1532/25000, Training Loss: 0.002043, lr : 0.008500\n",
      "Epoch: 1533/25000, Training Loss: 0.001656, lr : 0.008500\n",
      "Epoch: 1534/25000, Training Loss: 0.002252, lr : 0.008500\n",
      "Epoch: 1535/25000, Training Loss: 0.001889, lr : 0.008500\n",
      "Epoch: 1536/25000, Training Loss: 0.001614, lr : 0.008500\n",
      "Epoch: 1537/25000, Training Loss: 0.002019, lr : 0.008500\n",
      "Epoch: 1538/25000, Training Loss: 0.002059, lr : 0.008500\n",
      "Epoch: 1539/25000, Training Loss: 0.001801, lr : 0.008500\n",
      "Epoch: 1540/25000, Training Loss: 0.002001, lr : 0.008500\n",
      "Epoch: 1541/25000, Training Loss: 0.001778, lr : 0.008500\n",
      "Epoch: 1542/25000, Training Loss: 0.001881, lr : 0.008500\n",
      "Epoch: 1543/25000, Training Loss: 0.001631, lr : 0.008500\n",
      "Epoch: 1544/25000, Training Loss: 0.002159, lr : 0.008500\n",
      "Epoch: 1545/25000, Training Loss: 0.001709, lr : 0.008500\n",
      "Epoch: 1546/25000, Training Loss: 0.001604, lr : 0.008500\n",
      "Epoch: 1547/25000, Training Loss: 0.002095, lr : 0.008500\n",
      "Epoch: 1548/25000, Training Loss: 0.002089, lr : 0.008500\n",
      "Epoch: 1549/25000, Training Loss: 0.001740, lr : 0.008500\n",
      "Epoch: 1550/25000, Training Loss: 0.001485, lr : 0.008500\n",
      "Epoch: 1551/25000, Training Loss: 0.002024, lr : 0.008500\n",
      "Epoch: 1552/25000, Training Loss: 0.001628, lr : 0.008500\n",
      "Epoch: 1553/25000, Training Loss: 0.001710, lr : 0.008500\n",
      "Epoch: 1554/25000, Training Loss: 0.001792, lr : 0.008500\n",
      "Epoch: 1555/25000, Training Loss: 0.001503, lr : 0.008500\n",
      "Epoch: 1556/25000, Training Loss: 0.001941, lr : 0.008500\n",
      "Epoch: 1557/25000, Training Loss: 0.001757, lr : 0.008500\n",
      "Epoch: 1558/25000, Training Loss: 0.001873, lr : 0.008500\n",
      "Epoch: 1559/25000, Training Loss: 0.001803, lr : 0.008500\n",
      "Epoch: 1560/25000, Training Loss: 0.001960, lr : 0.008500\n",
      "Epoch: 1561/25000, Training Loss: 0.001569, lr : 0.008500\n",
      "Epoch: 1562/25000, Training Loss: 0.001877, lr : 0.008500\n",
      "Epoch: 1563/25000, Training Loss: 0.001880, lr : 0.008500\n",
      "Epoch: 1564/25000, Training Loss: 0.002048, lr : 0.008500\n",
      "Epoch: 1565/25000, Training Loss: 0.002052, lr : 0.008500\n",
      "Epoch: 1566/25000, Training Loss: 0.001765, lr : 0.008500\n",
      "Epoch: 1567/25000, Training Loss: 0.001712, lr : 0.008500\n",
      "Epoch: 1568/25000, Training Loss: 0.001471, lr : 0.008500\n",
      "Epoch: 1569/25000, Training Loss: 0.002025, lr : 0.008500\n",
      "Epoch: 1570/25000, Training Loss: 0.001878, lr : 0.008500\n",
      "Epoch: 1571/25000, Training Loss: 0.002018, lr : 0.008500\n",
      "Epoch: 1572/25000, Training Loss: 0.001882, lr : 0.008500\n",
      "Epoch: 1573/25000, Training Loss: 0.001748, lr : 0.008500\n",
      "Epoch: 1574/25000, Training Loss: 0.001936, lr : 0.008500\n",
      "Epoch: 1575/25000, Training Loss: 0.002287, lr : 0.008500\n",
      "Epoch: 1576/25000, Training Loss: 0.001796, lr : 0.008500\n",
      "Epoch: 1577/25000, Training Loss: 0.001993, lr : 0.008500\n",
      "Epoch: 1578/25000, Training Loss: 0.001912, lr : 0.008500\n",
      "Epoch: 1579/25000, Training Loss: 0.001933, lr : 0.008500\n",
      "Epoch: 1580/25000, Training Loss: 0.002069, lr : 0.008500\n",
      "Epoch: 1581/25000, Training Loss: 0.001984, lr : 0.008500\n",
      "Epoch: 1582/25000, Training Loss: 0.001788, lr : 0.008500\n",
      "Epoch: 1583/25000, Training Loss: 0.001662, lr : 0.008500\n",
      "Epoch: 1584/25000, Training Loss: 0.002649, lr : 0.008500\n",
      "Epoch: 1585/25000, Training Loss: 0.002000, lr : 0.008500\n",
      "Epoch: 1586/25000, Training Loss: 0.002110, lr : 0.008500\n",
      "Epoch: 1587/25000, Training Loss: 0.001885, lr : 0.008500\n",
      "Epoch: 1588/25000, Training Loss: 0.001610, lr : 0.008500\n",
      "Epoch: 1589/25000, Training Loss: 0.002281, lr : 0.008500\n",
      "Epoch: 1590/25000, Training Loss: 0.002040, lr : 0.008500\n",
      "Epoch: 1591/25000, Training Loss: 0.001741, lr : 0.008500\n",
      "Epoch: 1592/25000, Training Loss: 0.002220, lr : 0.008500\n",
      "Epoch: 1593/25000, Training Loss: 0.001824, lr : 0.008500\n",
      "Epoch: 1594/25000, Training Loss: 0.002442, lr : 0.008500\n",
      "Epoch: 1595/25000, Training Loss: 0.001757, lr : 0.008500\n",
      "Epoch: 1596/25000, Training Loss: 0.001897, lr : 0.008500\n",
      "Epoch: 1597/25000, Training Loss: 0.002149, lr : 0.008500\n",
      "Epoch: 1598/25000, Training Loss: 0.001772, lr : 0.008500\n",
      "Epoch: 1599/25000, Training Loss: 0.001678, lr : 0.008500\n",
      "Epoch: 1600/25000, Training Loss: 0.001905, lr : 0.008500\n",
      "Epoch: 1601/25000, Training Loss: 0.001712, lr : 0.008500\n",
      "Epoch: 1602/25000, Training Loss: 0.001738, lr : 0.008500\n",
      "Epoch: 1603/25000, Training Loss: 0.001938, lr : 0.008500\n",
      "Epoch: 1604/25000, Training Loss: 0.001557, lr : 0.008500\n",
      "Epoch: 1605/25000, Training Loss: 0.001658, lr : 0.008500\n",
      "Epoch: 1606/25000, Training Loss: 0.002145, lr : 0.008500\n",
      "Epoch: 1607/25000, Training Loss: 0.001344, lr : 0.008500\n",
      "Epoch: 1608/25000, Training Loss: 0.001718, lr : 0.008500\n",
      "Epoch: 1609/25000, Training Loss: 0.001868, lr : 0.008500\n",
      "Epoch: 1610/25000, Training Loss: 0.001916, lr : 0.008500\n",
      "Epoch: 1611/25000, Training Loss: 0.001735, lr : 0.008500\n",
      "Epoch: 1612/25000, Training Loss: 0.001597, lr : 0.008500\n",
      "Epoch: 1613/25000, Training Loss: 0.001552, lr : 0.008500\n",
      "Epoch: 1614/25000, Training Loss: 0.001729, lr : 0.008500\n",
      "Epoch: 1615/25000, Training Loss: 0.002183, lr : 0.008500\n",
      "Epoch: 1616/25000, Training Loss: 0.002003, lr : 0.008500\n",
      "Epoch: 1617/25000, Training Loss: 0.002184, lr : 0.008500\n",
      "Epoch: 1618/25000, Training Loss: 0.001849, lr : 0.008500\n",
      "Epoch: 1619/25000, Training Loss: 0.002113, lr : 0.008500\n",
      "Epoch: 1620/25000, Training Loss: 0.001692, lr : 0.008500\n",
      "Epoch: 1621/25000, Training Loss: 0.001651, lr : 0.008500\n",
      "Epoch: 1622/25000, Training Loss: 0.002029, lr : 0.008500\n",
      "Epoch: 1623/25000, Training Loss: 0.001925, lr : 0.008500\n",
      "Epoch: 1624/25000, Training Loss: 0.002047, lr : 0.008500\n",
      "Epoch: 1625/25000, Training Loss: 0.002245, lr : 0.008500\n",
      "Epoch: 1626/25000, Training Loss: 0.002191, lr : 0.008500\n",
      "Epoch: 1627/25000, Training Loss: 0.001795, lr : 0.008500\n",
      "Epoch: 1628/25000, Training Loss: 0.001994, lr : 0.008500\n",
      "Epoch: 1629/25000, Training Loss: 0.001782, lr : 0.008500\n",
      "Epoch: 1630/25000, Training Loss: 0.001926, lr : 0.008500\n",
      "Epoch: 1631/25000, Training Loss: 0.001767, lr : 0.008500\n",
      "Epoch: 1632/25000, Training Loss: 0.001990, lr : 0.008500\n",
      "Epoch: 1633/25000, Training Loss: 0.002078, lr : 0.008500\n",
      "Epoch: 1634/25000, Training Loss: 0.001747, lr : 0.008500\n",
      "Epoch: 1635/25000, Training Loss: 0.001990, lr : 0.008500\n",
      "Epoch: 1636/25000, Training Loss: 0.001783, lr : 0.008500\n",
      "Epoch: 1637/25000, Training Loss: 0.001392, lr : 0.008500\n",
      "Epoch: 1638/25000, Training Loss: 0.001678, lr : 0.008500\n",
      "Epoch: 1639/25000, Training Loss: 0.001724, lr : 0.008500\n",
      "Epoch: 1640/25000, Training Loss: 0.001454, lr : 0.008500\n",
      "Epoch: 1641/25000, Training Loss: 0.002061, lr : 0.008500\n",
      "Epoch: 1642/25000, Training Loss: 0.002032, lr : 0.008500\n",
      "Epoch: 1643/25000, Training Loss: 0.001959, lr : 0.008500\n",
      "Epoch: 1644/25000, Training Loss: 0.001630, lr : 0.008500\n",
      "Epoch: 1645/25000, Training Loss: 0.001749, lr : 0.008500\n",
      "Epoch: 1646/25000, Training Loss: 0.001811, lr : 0.008500\n",
      "Epoch: 1647/25000, Training Loss: 0.001505, lr : 0.008500\n",
      "Epoch: 1648/25000, Training Loss: 0.001616, lr : 0.008500\n",
      "Epoch: 1649/25000, Training Loss: 0.001511, lr : 0.008500\n",
      "Epoch: 1650/25000, Training Loss: 0.002151, lr : 0.008500\n",
      "Epoch: 1651/25000, Training Loss: 0.002048, lr : 0.008500\n",
      "Epoch: 1652/25000, Training Loss: 0.001810, lr : 0.008500\n",
      "Epoch: 1653/25000, Training Loss: 0.001812, lr : 0.008500\n",
      "Epoch: 1654/25000, Training Loss: 0.001795, lr : 0.008500\n",
      "Epoch: 1655/25000, Training Loss: 0.002157, lr : 0.008500\n",
      "Epoch: 1656/25000, Training Loss: 0.001772, lr : 0.008500\n",
      "Epoch: 1657/25000, Training Loss: 0.001766, lr : 0.008500\n",
      "Epoch: 1658/25000, Training Loss: 0.001459, lr : 0.008500\n",
      "Epoch: 1659/25000, Training Loss: 0.001980, lr : 0.008500\n",
      "Epoch: 1660/25000, Training Loss: 0.001807, lr : 0.008500\n",
      "Epoch: 1661/25000, Training Loss: 0.001836, lr : 0.008500\n",
      "Epoch: 1662/25000, Training Loss: 0.001356, lr : 0.008500\n",
      "Epoch: 1663/25000, Training Loss: 0.001696, lr : 0.008500\n",
      "Epoch: 1664/25000, Training Loss: 0.002207, lr : 0.008500\n",
      "Epoch: 1665/25000, Training Loss: 0.001786, lr : 0.008500\n",
      "Epoch: 1666/25000, Training Loss: 0.001487, lr : 0.008500\n",
      "Epoch: 1667/25000, Training Loss: 0.001897, lr : 0.008500\n",
      "Epoch: 1668/25000, Training Loss: 0.001553, lr : 0.008500\n",
      "Epoch: 1669/25000, Training Loss: 0.001810, lr : 0.008500\n",
      "Epoch: 1670/25000, Training Loss: 0.001822, lr : 0.008500\n",
      "Epoch: 1671/25000, Training Loss: 0.001275, lr : 0.008500\n",
      "Epoch: 1672/25000, Training Loss: 0.001559, lr : 0.008500\n",
      "Epoch: 1673/25000, Training Loss: 0.001472, lr : 0.008500\n",
      "Epoch: 1674/25000, Training Loss: 0.001683, lr : 0.008500\n",
      "Epoch: 1675/25000, Training Loss: 0.001495, lr : 0.008500\n",
      "Epoch: 1676/25000, Training Loss: 0.001972, lr : 0.008500\n",
      "Epoch: 1677/25000, Training Loss: 0.001746, lr : 0.008500\n",
      "Epoch: 1678/25000, Training Loss: 0.002020, lr : 0.008500\n",
      "Epoch: 1679/25000, Training Loss: 0.001994, lr : 0.008500\n",
      "Epoch: 1680/25000, Training Loss: 0.001669, lr : 0.008500\n",
      "Epoch: 1681/25000, Training Loss: 0.001969, lr : 0.008500\n",
      "Epoch: 1682/25000, Training Loss: 0.001515, lr : 0.008500\n",
      "Epoch: 1683/25000, Training Loss: 0.001797, lr : 0.008500\n",
      "Epoch: 1684/25000, Training Loss: 0.001617, lr : 0.008500\n",
      "Epoch: 1685/25000, Training Loss: 0.001168, lr : 0.008500\n",
      "Epoch: 1686/25000, Training Loss: 0.001841, lr : 0.008500\n",
      "Epoch: 1687/25000, Training Loss: 0.001634, lr : 0.008500\n",
      "Epoch: 1688/25000, Training Loss: 0.001710, lr : 0.008500\n",
      "Epoch: 1689/25000, Training Loss: 0.001716, lr : 0.008500\n",
      "Epoch: 1690/25000, Training Loss: 0.002009, lr : 0.008500\n",
      "Epoch: 1691/25000, Training Loss: 0.001697, lr : 0.008500\n",
      "Epoch: 1692/25000, Training Loss: 0.001901, lr : 0.008500\n",
      "Epoch: 1693/25000, Training Loss: 0.001823, lr : 0.008500\n",
      "Epoch: 1694/25000, Training Loss: 0.001722, lr : 0.008500\n",
      "Epoch: 1695/25000, Training Loss: 0.001836, lr : 0.008500\n",
      "Epoch: 1696/25000, Training Loss: 0.001696, lr : 0.008500\n",
      "Epoch: 1697/25000, Training Loss: 0.001742, lr : 0.008500\n",
      "Epoch: 1698/25000, Training Loss: 0.001647, lr : 0.008500\n",
      "Epoch: 1699/25000, Training Loss: 0.001606, lr : 0.008500\n",
      "Epoch: 1700/25000, Training Loss: 0.002078, lr : 0.008500\n",
      "Epoch: 1701/25000, Training Loss: 0.001803, lr : 0.008500\n",
      "Epoch: 1702/25000, Training Loss: 0.002166, lr : 0.008500\n",
      "Epoch: 1703/25000, Training Loss: 0.001839, lr : 0.008500\n",
      "Epoch: 1704/25000, Training Loss: 0.001492, lr : 0.008500\n",
      "Epoch: 1705/25000, Training Loss: 0.001875, lr : 0.008500\n",
      "Epoch: 1706/25000, Training Loss: 0.001797, lr : 0.008500\n",
      "Epoch: 1707/25000, Training Loss: 0.001587, lr : 0.008500\n",
      "Epoch: 1708/25000, Training Loss: 0.002058, lr : 0.008500\n",
      "Epoch: 1709/25000, Training Loss: 0.001770, lr : 0.008500\n",
      "Epoch: 1710/25000, Training Loss: 0.002028, lr : 0.008500\n",
      "Epoch: 1711/25000, Training Loss: 0.002130, lr : 0.008500\n",
      "Epoch: 1712/25000, Training Loss: 0.001680, lr : 0.008500\n",
      "Epoch: 1713/25000, Training Loss: 0.001693, lr : 0.008500\n",
      "Epoch: 1714/25000, Training Loss: 0.002130, lr : 0.008500\n",
      "Epoch: 1715/25000, Training Loss: 0.001638, lr : 0.008500\n",
      "Epoch: 1716/25000, Training Loss: 0.001593, lr : 0.008500\n",
      "Epoch: 1717/25000, Training Loss: 0.001877, lr : 0.008500\n",
      "Epoch: 1718/25000, Training Loss: 0.002072, lr : 0.008500\n",
      "Epoch: 1719/25000, Training Loss: 0.001697, lr : 0.008500\n",
      "Epoch: 1720/25000, Training Loss: 0.002229, lr : 0.008500\n",
      "Epoch: 1721/25000, Training Loss: 0.001775, lr : 0.008500\n",
      "Epoch: 1722/25000, Training Loss: 0.001833, lr : 0.008500\n",
      "Epoch: 1723/25000, Training Loss: 0.001716, lr : 0.008500\n",
      "Epoch: 1724/25000, Training Loss: 0.001666, lr : 0.008500\n",
      "Epoch: 1725/25000, Training Loss: 0.001885, lr : 0.008500\n",
      "Epoch: 1726/25000, Training Loss: 0.001511, lr : 0.008500\n",
      "Epoch: 1727/25000, Training Loss: 0.001346, lr : 0.008500\n",
      "Epoch: 1728/25000, Training Loss: 0.001540, lr : 0.008500\n",
      "Epoch: 1729/25000, Training Loss: 0.001645, lr : 0.008500\n",
      "Epoch: 1730/25000, Training Loss: 0.001561, lr : 0.008500\n",
      "Epoch: 1731/25000, Training Loss: 0.001273, lr : 0.008500\n",
      "Epoch: 1732/25000, Training Loss: 0.001591, lr : 0.008500\n",
      "Epoch: 1733/25000, Training Loss: 0.001516, lr : 0.008500\n",
      "Epoch: 1734/25000, Training Loss: 0.001589, lr : 0.008500\n",
      "Epoch: 1735/25000, Training Loss: 0.001416, lr : 0.008500\n",
      "Epoch: 1736/25000, Training Loss: 0.001470, lr : 0.008500\n",
      "Epoch: 1737/25000, Training Loss: 0.001589, lr : 0.008500\n",
      "Epoch: 1738/25000, Training Loss: 0.001894, lr : 0.008500\n",
      "Epoch: 1739/25000, Training Loss: 0.001617, lr : 0.008500\n",
      "Epoch: 1740/25000, Training Loss: 0.001569, lr : 0.008500\n",
      "Epoch: 1741/25000, Training Loss: 0.001822, lr : 0.008500\n",
      "Epoch: 1742/25000, Training Loss: 0.001538, lr : 0.008500\n",
      "Epoch: 1743/25000, Training Loss: 0.001464, lr : 0.008500\n",
      "Epoch: 1744/25000, Training Loss: 0.001449, lr : 0.008500\n",
      "Epoch: 1745/25000, Training Loss: 0.001451, lr : 0.008500\n",
      "Epoch: 1746/25000, Training Loss: 0.001336, lr : 0.008500\n",
      "Epoch: 1747/25000, Training Loss: 0.001476, lr : 0.008500\n",
      "Epoch: 1748/25000, Training Loss: 0.001599, lr : 0.008500\n",
      "Epoch: 1749/25000, Training Loss: 0.001216, lr : 0.008500\n",
      "Epoch: 1750/25000, Training Loss: 0.001664, lr : 0.008500\n",
      "Epoch: 1751/25000, Training Loss: 0.001564, lr : 0.008500\n",
      "Epoch: 1752/25000, Training Loss: 0.001813, lr : 0.008500\n",
      "Epoch: 1753/25000, Training Loss: 0.001724, lr : 0.008500\n",
      "Epoch: 1754/25000, Training Loss: 0.001587, lr : 0.008500\n",
      "Epoch: 1755/25000, Training Loss: 0.001290, lr : 0.008500\n",
      "Epoch: 1756/25000, Training Loss: 0.001529, lr : 0.008500\n",
      "Epoch: 1757/25000, Training Loss: 0.001820, lr : 0.008500\n",
      "Epoch: 1758/25000, Training Loss: 0.001848, lr : 0.008500\n",
      "Epoch: 1759/25000, Training Loss: 0.001609, lr : 0.008500\n",
      "Epoch: 1760/25000, Training Loss: 0.001524, lr : 0.008500\n",
      "Epoch: 1761/25000, Training Loss: 0.001907, lr : 0.008500\n",
      "Epoch: 1762/25000, Training Loss: 0.001552, lr : 0.008500\n",
      "Epoch: 1763/25000, Training Loss: 0.001542, lr : 0.008500\n",
      "Epoch: 1764/25000, Training Loss: 0.001531, lr : 0.008500\n",
      "Epoch: 1765/25000, Training Loss: 0.001414, lr : 0.008500\n",
      "Epoch: 1766/25000, Training Loss: 0.001808, lr : 0.008500\n",
      "Epoch: 1767/25000, Training Loss: 0.001909, lr : 0.008500\n",
      "Epoch: 1768/25000, Training Loss: 0.001369, lr : 0.008500\n",
      "Epoch: 1769/25000, Training Loss: 0.001444, lr : 0.008500\n",
      "Epoch: 1770/25000, Training Loss: 0.001437, lr : 0.008500\n",
      "Epoch: 1771/25000, Training Loss: 0.001767, lr : 0.008500\n",
      "Epoch: 1772/25000, Training Loss: 0.001687, lr : 0.008500\n",
      "Epoch: 1773/25000, Training Loss: 0.001347, lr : 0.008500\n",
      "Epoch: 1774/25000, Training Loss: 0.001349, lr : 0.008500\n",
      "Epoch: 1775/25000, Training Loss: 0.001757, lr : 0.008500\n",
      "Epoch: 1776/25000, Training Loss: 0.001610, lr : 0.008500\n",
      "Epoch: 1777/25000, Training Loss: 0.001721, lr : 0.008500\n",
      "Epoch: 1778/25000, Training Loss: 0.001959, lr : 0.008500\n",
      "Epoch: 1779/25000, Training Loss: 0.001332, lr : 0.008500\n",
      "Epoch: 1780/25000, Training Loss: 0.001530, lr : 0.008500\n",
      "Epoch: 1781/25000, Training Loss: 0.001986, lr : 0.008500\n",
      "Epoch: 1782/25000, Training Loss: 0.001512, lr : 0.008500\n",
      "Epoch: 1783/25000, Training Loss: 0.001301, lr : 0.008500\n",
      "Epoch: 1784/25000, Training Loss: 0.001242, lr : 0.008500\n",
      "Epoch: 1785/25000, Training Loss: 0.001701, lr : 0.008500\n",
      "Epoch: 1786/25000, Training Loss: 0.001486, lr : 0.008500\n",
      "Epoch: 1787/25000, Training Loss: 0.001812, lr : 0.008500\n",
      "Epoch: 1788/25000, Training Loss: 0.001834, lr : 0.008500\n",
      "Epoch: 1789/25000, Training Loss: 0.001645, lr : 0.008500\n",
      "Epoch: 1790/25000, Training Loss: 0.002030, lr : 0.008500\n",
      "Epoch: 1791/25000, Training Loss: 0.001408, lr : 0.008500\n",
      "Epoch: 1792/25000, Training Loss: 0.001490, lr : 0.008500\n",
      "Epoch: 1793/25000, Training Loss: 0.001323, lr : 0.008500\n",
      "Epoch: 1794/25000, Training Loss: 0.001731, lr : 0.008500\n",
      "Epoch: 1795/25000, Training Loss: 0.001248, lr : 0.008500\n",
      "Epoch: 1796/25000, Training Loss: 0.001607, lr : 0.008500\n",
      "Epoch: 1797/25000, Training Loss: 0.001552, lr : 0.008500\n",
      "Epoch: 1798/25000, Training Loss: 0.001595, lr : 0.008500\n",
      "Epoch: 1799/25000, Training Loss: 0.001801, lr : 0.008500\n",
      "Epoch: 1800/25000, Training Loss: 0.001229, lr : 0.008500\n",
      "Epoch: 1801/25000, Training Loss: 0.001501, lr : 0.008500\n",
      "Epoch: 1802/25000, Training Loss: 0.001276, lr : 0.008500\n",
      "Epoch: 1803/25000, Training Loss: 0.001315, lr : 0.008500\n",
      "Epoch: 1804/25000, Training Loss: 0.001235, lr : 0.008500\n",
      "Epoch: 1805/25000, Training Loss: 0.001832, lr : 0.008500\n",
      "Epoch: 1806/25000, Training Loss: 0.001577, lr : 0.008500\n",
      "Epoch: 1807/25000, Training Loss: 0.001342, lr : 0.008500\n",
      "Epoch: 1808/25000, Training Loss: 0.001805, lr : 0.008500\n",
      "Epoch: 1809/25000, Training Loss: 0.001431, lr : 0.008500\n",
      "Epoch: 1810/25000, Training Loss: 0.001561, lr : 0.008500\n",
      "Epoch: 1811/25000, Training Loss: 0.001316, lr : 0.008500\n",
      "Epoch: 1812/25000, Training Loss: 0.001624, lr : 0.008500\n",
      "Epoch: 1813/25000, Training Loss: 0.002068, lr : 0.008500\n",
      "Epoch: 1814/25000, Training Loss: 0.001588, lr : 0.008500\n",
      "Epoch: 1815/25000, Training Loss: 0.001137, lr : 0.008500\n",
      "Epoch: 1816/25000, Training Loss: 0.001503, lr : 0.008500\n",
      "Epoch: 1817/25000, Training Loss: 0.001339, lr : 0.008500\n",
      "Epoch: 1818/25000, Training Loss: 0.001349, lr : 0.008500\n",
      "Epoch: 1819/25000, Training Loss: 0.001357, lr : 0.008500\n",
      "Epoch: 1820/25000, Training Loss: 0.001127, lr : 0.008500\n",
      "Epoch: 1821/25000, Training Loss: 0.001462, lr : 0.008500\n",
      "Epoch: 1822/25000, Training Loss: 0.001568, lr : 0.008500\n",
      "Epoch: 1823/25000, Training Loss: 0.001804, lr : 0.008500\n",
      "Epoch: 1824/25000, Training Loss: 0.001619, lr : 0.008500\n",
      "Epoch: 1825/25000, Training Loss: 0.001402, lr : 0.008500\n",
      "Epoch: 1826/25000, Training Loss: 0.001433, lr : 0.008500\n",
      "Epoch: 1827/25000, Training Loss: 0.001607, lr : 0.008500\n",
      "Epoch: 1828/25000, Training Loss: 0.001546, lr : 0.008500\n",
      "Epoch: 1829/25000, Training Loss: 0.001829, lr : 0.008500\n",
      "Epoch: 1830/25000, Training Loss: 0.001235, lr : 0.008500\n",
      "Epoch: 1831/25000, Training Loss: 0.001381, lr : 0.008500\n",
      "Epoch: 1832/25000, Training Loss: 0.001994, lr : 0.008500\n",
      "Epoch: 1833/25000, Training Loss: 0.001839, lr : 0.008500\n",
      "Epoch: 1834/25000, Training Loss: 0.002354, lr : 0.008500\n",
      "Epoch: 1835/25000, Training Loss: 0.001638, lr : 0.008500\n",
      "Epoch: 1836/25000, Training Loss: 0.001710, lr : 0.008500\n",
      "Epoch: 1837/25000, Training Loss: 0.001837, lr : 0.008500\n",
      "Epoch: 1838/25000, Training Loss: 0.001414, lr : 0.008500\n",
      "Epoch: 1839/25000, Training Loss: 0.001566, lr : 0.008500\n",
      "Epoch: 1840/25000, Training Loss: 0.001407, lr : 0.008500\n",
      "Epoch: 1841/25000, Training Loss: 0.001674, lr : 0.008500\n",
      "Epoch: 1842/25000, Training Loss: 0.001637, lr : 0.008500\n",
      "Epoch: 1843/25000, Training Loss: 0.001636, lr : 0.008500\n",
      "Epoch: 1844/25000, Training Loss: 0.001868, lr : 0.008500\n",
      "Epoch: 1845/25000, Training Loss: 0.001578, lr : 0.008500\n",
      "Epoch: 1846/25000, Training Loss: 0.001416, lr : 0.008500\n",
      "Epoch: 1847/25000, Training Loss: 0.001806, lr : 0.008500\n",
      "Epoch: 1848/25000, Training Loss: 0.001580, lr : 0.008500\n",
      "Epoch: 1849/25000, Training Loss: 0.001488, lr : 0.008500\n",
      "Epoch: 1850/25000, Training Loss: 0.001542, lr : 0.008500\n",
      "Epoch: 1851/25000, Training Loss: 0.001422, lr : 0.008500\n",
      "Epoch: 1852/25000, Training Loss: 0.001217, lr : 0.008500\n",
      "Epoch: 1853/25000, Training Loss: 0.001852, lr : 0.008500\n",
      "Epoch: 1854/25000, Training Loss: 0.001295, lr : 0.008500\n",
      "Epoch: 1855/25000, Training Loss: 0.001440, lr : 0.008500\n",
      "Epoch: 1856/25000, Training Loss: 0.001800, lr : 0.008500\n",
      "Epoch: 1857/25000, Training Loss: 0.001817, lr : 0.008500\n",
      "Epoch: 1858/25000, Training Loss: 0.001588, lr : 0.008500\n",
      "Epoch: 1859/25000, Training Loss: 0.001593, lr : 0.008500\n",
      "Epoch: 1860/25000, Training Loss: 0.001697, lr : 0.008500\n",
      "Epoch: 1861/25000, Training Loss: 0.001278, lr : 0.008500\n",
      "Epoch: 1862/25000, Training Loss: 0.001544, lr : 0.008500\n",
      "Epoch: 1863/25000, Training Loss: 0.001360, lr : 0.008500\n",
      "Epoch: 1864/25000, Training Loss: 0.001168, lr : 0.008500\n",
      "Epoch: 1865/25000, Training Loss: 0.001490, lr : 0.008500\n",
      "Epoch: 1866/25000, Training Loss: 0.001595, lr : 0.008500\n",
      "Epoch: 1867/25000, Training Loss: 0.001465, lr : 0.008500\n",
      "Epoch: 1868/25000, Training Loss: 0.001715, lr : 0.008500\n",
      "Epoch: 1869/25000, Training Loss: 0.001672, lr : 0.008500\n",
      "Epoch: 1870/25000, Training Loss: 0.001581, lr : 0.008500\n",
      "Epoch: 1871/25000, Training Loss: 0.001584, lr : 0.008500\n",
      "Epoch: 1872/25000, Training Loss: 0.001653, lr : 0.008500\n",
      "Epoch: 1873/25000, Training Loss: 0.001540, lr : 0.008500\n",
      "Epoch: 1874/25000, Training Loss: 0.001866, lr : 0.008500\n",
      "Epoch: 1875/25000, Training Loss: 0.001862, lr : 0.008500\n",
      "Epoch: 1876/25000, Training Loss: 0.001619, lr : 0.008500\n",
      "Epoch: 1877/25000, Training Loss: 0.001389, lr : 0.008500\n",
      "Epoch: 1878/25000, Training Loss: 0.001627, lr : 0.008500\n",
      "Epoch: 1879/25000, Training Loss: 0.001635, lr : 0.008500\n",
      "Epoch: 1880/25000, Training Loss: 0.001733, lr : 0.008500\n",
      "Epoch: 1881/25000, Training Loss: 0.001905, lr : 0.008500\n",
      "Epoch: 1882/25000, Training Loss: 0.001583, lr : 0.008500\n",
      "Epoch: 1883/25000, Training Loss: 0.001724, lr : 0.008500\n",
      "Epoch: 1884/25000, Training Loss: 0.001753, lr : 0.008500\n",
      "Epoch: 1885/25000, Training Loss: 0.001437, lr : 0.008500\n",
      "Epoch: 1886/25000, Training Loss: 0.001723, lr : 0.008500\n",
      "Epoch: 1887/25000, Training Loss: 0.001855, lr : 0.008500\n",
      "Epoch: 1888/25000, Training Loss: 0.001528, lr : 0.008500\n",
      "Epoch: 1889/25000, Training Loss: 0.001794, lr : 0.008500\n",
      "Epoch: 1890/25000, Training Loss: 0.001650, lr : 0.008500\n",
      "Epoch: 1891/25000, Training Loss: 0.001427, lr : 0.008500\n",
      "Epoch: 1892/25000, Training Loss: 0.001317, lr : 0.008500\n",
      "Epoch: 1893/25000, Training Loss: 0.001512, lr : 0.008500\n",
      "Epoch: 1894/25000, Training Loss: 0.001682, lr : 0.008500\n",
      "Epoch: 1895/25000, Training Loss: 0.001039, lr : 0.008500\n",
      "Epoch: 1896/25000, Training Loss: 0.001193, lr : 0.008500\n",
      "Epoch: 1897/25000, Training Loss: 0.001418, lr : 0.008500\n",
      "Epoch: 1898/25000, Training Loss: 0.001480, lr : 0.008500\n",
      "Epoch: 1899/25000, Training Loss: 0.001551, lr : 0.008500\n",
      "Epoch: 1900/25000, Training Loss: 0.001675, lr : 0.008500\n",
      "Epoch: 1901/25000, Training Loss: 0.001249, lr : 0.008500\n",
      "Epoch: 1902/25000, Training Loss: 0.001336, lr : 0.008500\n",
      "Epoch: 1903/25000, Training Loss: 0.001121, lr : 0.008500\n",
      "Epoch: 1904/25000, Training Loss: 0.001583, lr : 0.008500\n",
      "Epoch: 1905/25000, Training Loss: 0.001577, lr : 0.008500\n",
      "Epoch: 1906/25000, Training Loss: 0.001189, lr : 0.008500\n",
      "Epoch: 1907/25000, Training Loss: 0.001376, lr : 0.008500\n",
      "Epoch: 1908/25000, Training Loss: 0.001359, lr : 0.008500\n",
      "Epoch: 1909/25000, Training Loss: 0.001243, lr : 0.008500\n",
      "Epoch: 1910/25000, Training Loss: 0.001982, lr : 0.008500\n",
      "Epoch: 1911/25000, Training Loss: 0.001276, lr : 0.008500\n",
      "Epoch: 1912/25000, Training Loss: 0.001458, lr : 0.008500\n",
      "Epoch: 1913/25000, Training Loss: 0.001449, lr : 0.008500\n",
      "Epoch: 1914/25000, Training Loss: 0.001953, lr : 0.008500\n",
      "Epoch: 1915/25000, Training Loss: 0.001233, lr : 0.008500\n",
      "Epoch: 1916/25000, Training Loss: 0.001687, lr : 0.008500\n",
      "Epoch: 1917/25000, Training Loss: 0.001483, lr : 0.008500\n",
      "Epoch: 1918/25000, Training Loss: 0.001456, lr : 0.008500\n",
      "Epoch: 1919/25000, Training Loss: 0.001647, lr : 0.008500\n",
      "Epoch: 1920/25000, Training Loss: 0.001288, lr : 0.008500\n",
      "Epoch: 1921/25000, Training Loss: 0.001764, lr : 0.008500\n",
      "Epoch: 1922/25000, Training Loss: 0.001401, lr : 0.008500\n",
      "Epoch: 1923/25000, Training Loss: 0.001311, lr : 0.008500\n",
      "Epoch: 1924/25000, Training Loss: 0.001072, lr : 0.008500\n",
      "Epoch: 1925/25000, Training Loss: 0.001370, lr : 0.008500\n",
      "Epoch: 1926/25000, Training Loss: 0.001065, lr : 0.008500\n",
      "Epoch: 1927/25000, Training Loss: 0.001320, lr : 0.008500\n",
      "Epoch: 1928/25000, Training Loss: 0.001690, lr : 0.008500\n",
      "Epoch: 1929/25000, Training Loss: 0.001571, lr : 0.008500\n",
      "Epoch: 1930/25000, Training Loss: 0.001633, lr : 0.008500\n",
      "Epoch: 1931/25000, Training Loss: 0.001713, lr : 0.008500\n",
      "Epoch: 1932/25000, Training Loss: 0.001355, lr : 0.008500\n",
      "Epoch: 1933/25000, Training Loss: 0.001666, lr : 0.008500\n",
      "Epoch: 1934/25000, Training Loss: 0.001398, lr : 0.008500\n",
      "Epoch: 1935/25000, Training Loss: 0.001352, lr : 0.008500\n",
      "Epoch: 1936/25000, Training Loss: 0.001187, lr : 0.008500\n",
      "Epoch: 1937/25000, Training Loss: 0.001319, lr : 0.008500\n",
      "Epoch: 1938/25000, Training Loss: 0.001163, lr : 0.008500\n",
      "Epoch: 1939/25000, Training Loss: 0.001391, lr : 0.008500\n",
      "Epoch: 1940/25000, Training Loss: 0.001422, lr : 0.008500\n",
      "Epoch: 1941/25000, Training Loss: 0.001008, lr : 0.008500\n",
      "Epoch: 1942/25000, Training Loss: 0.001464, lr : 0.008500\n",
      "Epoch: 1943/25000, Training Loss: 0.001814, lr : 0.008500\n",
      "Epoch: 1944/25000, Training Loss: 0.001190, lr : 0.008500\n",
      "Epoch: 1945/25000, Training Loss: 0.001430, lr : 0.008500\n",
      "Epoch: 1946/25000, Training Loss: 0.001443, lr : 0.008500\n",
      "Epoch: 1947/25000, Training Loss: 0.001522, lr : 0.008500\n",
      "Epoch: 1948/25000, Training Loss: 0.001574, lr : 0.008500\n",
      "Epoch: 1949/25000, Training Loss: 0.001248, lr : 0.008500\n",
      "Epoch: 1950/25000, Training Loss: 0.001199, lr : 0.008500\n",
      "Epoch: 1951/25000, Training Loss: 0.001430, lr : 0.008500\n",
      "Epoch: 1952/25000, Training Loss: 0.001712, lr : 0.008500\n",
      "Epoch: 1953/25000, Training Loss: 0.001753, lr : 0.008500\n",
      "Epoch: 1954/25000, Training Loss: 0.001127, lr : 0.008500\n",
      "Epoch: 1955/25000, Training Loss: 0.001355, lr : 0.008500\n",
      "Epoch: 1956/25000, Training Loss: 0.001346, lr : 0.008500\n",
      "Epoch: 1957/25000, Training Loss: 0.001127, lr : 0.008500\n",
      "Epoch: 1958/25000, Training Loss: 0.001428, lr : 0.008500\n",
      "Epoch: 1959/25000, Training Loss: 0.001665, lr : 0.008500\n",
      "Epoch: 1960/25000, Training Loss: 0.001267, lr : 0.008500\n",
      "Epoch: 1961/25000, Training Loss: 0.001562, lr : 0.008500\n",
      "Epoch: 1962/25000, Training Loss: 0.001674, lr : 0.008500\n",
      "Epoch: 1963/25000, Training Loss: 0.001392, lr : 0.008500\n",
      "Epoch: 1964/25000, Training Loss: 0.001867, lr : 0.008500\n",
      "Epoch: 1965/25000, Training Loss: 0.001641, lr : 0.008500\n",
      "Epoch: 1966/25000, Training Loss: 0.002098, lr : 0.008500\n",
      "Epoch: 1967/25000, Training Loss: 0.001333, lr : 0.008500\n",
      "Epoch: 1968/25000, Training Loss: 0.001516, lr : 0.008500\n",
      "Epoch: 1969/25000, Training Loss: 0.001433, lr : 0.008500\n",
      "Epoch: 1970/25000, Training Loss: 0.001477, lr : 0.008500\n",
      "Epoch: 1971/25000, Training Loss: 0.001798, lr : 0.008500\n",
      "Epoch: 1972/25000, Training Loss: 0.001750, lr : 0.008500\n",
      "Epoch: 1973/25000, Training Loss: 0.001609, lr : 0.008500\n",
      "Epoch: 1974/25000, Training Loss: 0.001339, lr : 0.008500\n",
      "Epoch: 1975/25000, Training Loss: 0.001551, lr : 0.008500\n",
      "Epoch: 1976/25000, Training Loss: 0.001366, lr : 0.008500\n",
      "Epoch: 1977/25000, Training Loss: 0.001655, lr : 0.008500\n",
      "Epoch: 1978/25000, Training Loss: 0.001444, lr : 0.008500\n",
      "Epoch: 1979/25000, Training Loss: 0.001472, lr : 0.008500\n",
      "Epoch: 1980/25000, Training Loss: 0.001437, lr : 0.008500\n",
      "Epoch: 1981/25000, Training Loss: 0.001602, lr : 0.008500\n",
      "Epoch: 1982/25000, Training Loss: 0.001383, lr : 0.008500\n",
      "Epoch: 1983/25000, Training Loss: 0.001425, lr : 0.008500\n",
      "Epoch: 1984/25000, Training Loss: 0.001334, lr : 0.008500\n",
      "Epoch: 1985/25000, Training Loss: 0.001373, lr : 0.008500\n",
      "Epoch: 1986/25000, Training Loss: 0.001399, lr : 0.008500\n",
      "Epoch: 1987/25000, Training Loss: 0.001293, lr : 0.008500\n",
      "Epoch: 1988/25000, Training Loss: 0.001377, lr : 0.008500\n",
      "Epoch: 1989/25000, Training Loss: 0.001561, lr : 0.008500\n",
      "Epoch: 1990/25000, Training Loss: 0.001332, lr : 0.008500\n",
      "Epoch: 1991/25000, Training Loss: 0.001636, lr : 0.008500\n",
      "Epoch: 1992/25000, Training Loss: 0.001473, lr : 0.008500\n",
      "Epoch: 1993/25000, Training Loss: 0.001414, lr : 0.008500\n",
      "Epoch: 1994/25000, Training Loss: 0.001131, lr : 0.008500\n",
      "Epoch: 1995/25000, Training Loss: 0.001600, lr : 0.008500\n",
      "Epoch: 1996/25000, Training Loss: 0.001516, lr : 0.008500\n",
      "Epoch: 1997/25000, Training Loss: 0.001752, lr : 0.008500\n",
      "Epoch: 1998/25000, Training Loss: 0.001368, lr : 0.008500\n",
      "Epoch: 1999/25000, Training Loss: 0.001781, lr : 0.008500\n",
      "Epoch: 2000/25000, Training Loss: 0.001476, lr : 0.008500\n",
      "Epoch: 2001/25000, Training Loss: 0.001406, lr : 0.008500\n",
      "Epoch: 2002/25000, Training Loss: 0.001712, lr : 0.008500\n",
      "Epoch: 2003/25000, Training Loss: 0.001366, lr : 0.008500\n",
      "Epoch: 2004/25000, Training Loss: 0.001585, lr : 0.008500\n",
      "Epoch: 2005/25000, Training Loss: 0.001676, lr : 0.008500\n",
      "Epoch: 2006/25000, Training Loss: 0.001381, lr : 0.008500\n",
      "Epoch: 2007/25000, Training Loss: 0.001175, lr : 0.008500\n",
      "Epoch: 2008/25000, Training Loss: 0.001549, lr : 0.008500\n",
      "Epoch: 2009/25000, Training Loss: 0.001553, lr : 0.008500\n",
      "Epoch: 2010/25000, Training Loss: 0.001503, lr : 0.008500\n",
      "Epoch: 2011/25000, Training Loss: 0.001584, lr : 0.008500\n",
      "Epoch: 2012/25000, Training Loss: 0.001521, lr : 0.008500\n",
      "Epoch: 2013/25000, Training Loss: 0.001422, lr : 0.008500\n",
      "Epoch: 2014/25000, Training Loss: 0.001468, lr : 0.008500\n",
      "Epoch: 2015/25000, Training Loss: 0.001409, lr : 0.008500\n",
      "Epoch: 2016/25000, Training Loss: 0.001342, lr : 0.008500\n",
      "Epoch: 2017/25000, Training Loss: 0.001440, lr : 0.008500\n",
      "Epoch: 2018/25000, Training Loss: 0.001569, lr : 0.008500\n",
      "Epoch: 2019/25000, Training Loss: 0.001383, lr : 0.008500\n",
      "Epoch: 2020/25000, Training Loss: 0.001356, lr : 0.008500\n",
      "Epoch: 2021/25000, Training Loss: 0.001337, lr : 0.008500\n",
      "Epoch: 2022/25000, Training Loss: 0.001125, lr : 0.008500\n",
      "Epoch: 2023/25000, Training Loss: 0.001189, lr : 0.008500\n",
      "Epoch: 2024/25000, Training Loss: 0.001316, lr : 0.008500\n",
      "Epoch: 2025/25000, Training Loss: 0.001483, lr : 0.008500\n",
      "Epoch: 2026/25000, Training Loss: 0.001349, lr : 0.008500\n",
      "Epoch: 2027/25000, Training Loss: 0.000967, lr : 0.008500\n",
      "Epoch: 2028/25000, Training Loss: 0.001059, lr : 0.008500\n",
      "Epoch: 2029/25000, Training Loss: 0.001286, lr : 0.008500\n",
      "Epoch: 2030/25000, Training Loss: 0.001501, lr : 0.008500\n",
      "Epoch: 2031/25000, Training Loss: 0.001313, lr : 0.008500\n",
      "Epoch: 2032/25000, Training Loss: 0.001438, lr : 0.008500\n",
      "Epoch: 2033/25000, Training Loss: 0.001720, lr : 0.008500\n",
      "Epoch: 2034/25000, Training Loss: 0.001686, lr : 0.008500\n",
      "Epoch: 2035/25000, Training Loss: 0.001360, lr : 0.008500\n",
      "Epoch: 2036/25000, Training Loss: 0.001208, lr : 0.008500\n",
      "Epoch: 2037/25000, Training Loss: 0.001481, lr : 0.008500\n",
      "Epoch: 2038/25000, Training Loss: 0.001251, lr : 0.008500\n",
      "Epoch: 2039/25000, Training Loss: 0.001378, lr : 0.008500\n",
      "Epoch: 2040/25000, Training Loss: 0.001576, lr : 0.008500\n",
      "Epoch: 2041/25000, Training Loss: 0.001348, lr : 0.008500\n",
      "Epoch: 2042/25000, Training Loss: 0.001679, lr : 0.008500\n",
      "Epoch: 2043/25000, Training Loss: 0.001158, lr : 0.008500\n",
      "Epoch: 2044/25000, Training Loss: 0.001672, lr : 0.008500\n",
      "Epoch: 2045/25000, Training Loss: 0.001518, lr : 0.008500\n",
      "Epoch: 2046/25000, Training Loss: 0.001508, lr : 0.008500\n",
      "Epoch: 2047/25000, Training Loss: 0.001459, lr : 0.008500\n",
      "Epoch: 2048/25000, Training Loss: 0.001343, lr : 0.008500\n",
      "Epoch: 2049/25000, Training Loss: 0.001201, lr : 0.008500\n",
      "Epoch: 2050/25000, Training Loss: 0.001218, lr : 0.008500\n",
      "Epoch: 2051/25000, Training Loss: 0.001227, lr : 0.008500\n",
      "Epoch: 2052/25000, Training Loss: 0.001692, lr : 0.008500\n",
      "Epoch: 2053/25000, Training Loss: 0.001509, lr : 0.008500\n",
      "Epoch: 2054/25000, Training Loss: 0.001297, lr : 0.008500\n",
      "Epoch: 2055/25000, Training Loss: 0.001203, lr : 0.008500\n",
      "Epoch: 2056/25000, Training Loss: 0.001201, lr : 0.008500\n",
      "Epoch: 2057/25000, Training Loss: 0.001287, lr : 0.008500\n",
      "Epoch: 2058/25000, Training Loss: 0.001467, lr : 0.008500\n",
      "Epoch: 2059/25000, Training Loss: 0.001533, lr : 0.008500\n",
      "Epoch: 2060/25000, Training Loss: 0.001378, lr : 0.008500\n",
      "Epoch: 2061/25000, Training Loss: 0.001428, lr : 0.008500\n",
      "Epoch: 2062/25000, Training Loss: 0.001657, lr : 0.008500\n",
      "Epoch: 2063/25000, Training Loss: 0.001252, lr : 0.008500\n",
      "Epoch: 2064/25000, Training Loss: 0.001414, lr : 0.008500\n",
      "Epoch: 2065/25000, Training Loss: 0.001244, lr : 0.008500\n",
      "Epoch: 2066/25000, Training Loss: 0.001102, lr : 0.008500\n",
      "Epoch: 2067/25000, Training Loss: 0.001590, lr : 0.008500\n",
      "Epoch: 2068/25000, Training Loss: 0.001322, lr : 0.008500\n",
      "Epoch: 2069/25000, Training Loss: 0.001349, lr : 0.008500\n",
      "Epoch: 2070/25000, Training Loss: 0.001079, lr : 0.008500\n",
      "Epoch: 2071/25000, Training Loss: 0.001453, lr : 0.008500\n",
      "Epoch: 2072/25000, Training Loss: 0.001134, lr : 0.008500\n",
      "Epoch: 2073/25000, Training Loss: 0.001270, lr : 0.008500\n",
      "Epoch: 2074/25000, Training Loss: 0.001811, lr : 0.008500\n",
      "Epoch: 2075/25000, Training Loss: 0.001260, lr : 0.008500\n",
      "Epoch: 2076/25000, Training Loss: 0.001115, lr : 0.008500\n",
      "Epoch: 2077/25000, Training Loss: 0.001551, lr : 0.008500\n",
      "Epoch: 2078/25000, Training Loss: 0.001447, lr : 0.008500\n",
      "Epoch: 2079/25000, Training Loss: 0.001034, lr : 0.008500\n",
      "Epoch: 2080/25000, Training Loss: 0.001446, lr : 0.008500\n",
      "Epoch: 2081/25000, Training Loss: 0.001176, lr : 0.008500\n",
      "Epoch: 2082/25000, Training Loss: 0.001479, lr : 0.008500\n",
      "Epoch: 2083/25000, Training Loss: 0.001172, lr : 0.008500\n",
      "Epoch: 2084/25000, Training Loss: 0.001581, lr : 0.008500\n",
      "Epoch: 2085/25000, Training Loss: 0.001493, lr : 0.008500\n",
      "Epoch: 2086/25000, Training Loss: 0.001134, lr : 0.008500\n",
      "Epoch: 2087/25000, Training Loss: 0.001739, lr : 0.008500\n",
      "Epoch: 2088/25000, Training Loss: 0.001339, lr : 0.008500\n",
      "Epoch: 2089/25000, Training Loss: 0.001253, lr : 0.008500\n",
      "Epoch: 2090/25000, Training Loss: 0.001188, lr : 0.008500\n",
      "Epoch: 2091/25000, Training Loss: 0.001329, lr : 0.008500\n",
      "Epoch: 2092/25000, Training Loss: 0.000969, lr : 0.008500\n",
      "Epoch: 2093/25000, Training Loss: 0.001512, lr : 0.008500\n",
      "Epoch: 2094/25000, Training Loss: 0.001406, lr : 0.008500\n",
      "Epoch: 2095/25000, Training Loss: 0.001370, lr : 0.008500\n",
      "Epoch: 2096/25000, Training Loss: 0.001349, lr : 0.008500\n",
      "Epoch: 2097/25000, Training Loss: 0.001121, lr : 0.008500\n",
      "Epoch: 2098/25000, Training Loss: 0.001297, lr : 0.008500\n",
      "Epoch: 2099/25000, Training Loss: 0.001174, lr : 0.008500\n",
      "Epoch: 2100/25000, Training Loss: 0.001117, lr : 0.008500\n",
      "Epoch: 2101/25000, Training Loss: 0.001096, lr : 0.008500\n",
      "Epoch: 2102/25000, Training Loss: 0.001301, lr : 0.008500\n",
      "Epoch: 2103/25000, Training Loss: 0.001175, lr : 0.008500\n",
      "Epoch: 2104/25000, Training Loss: 0.001432, lr : 0.008500\n",
      "Epoch: 2105/25000, Training Loss: 0.001355, lr : 0.008500\n",
      "Epoch: 2106/25000, Training Loss: 0.001052, lr : 0.008500\n",
      "Epoch: 2107/25000, Training Loss: 0.001712, lr : 0.008500\n",
      "Epoch: 2108/25000, Training Loss: 0.001288, lr : 0.008500\n",
      "Epoch: 2109/25000, Training Loss: 0.001226, lr : 0.008500\n",
      "Epoch: 2110/25000, Training Loss: 0.001027, lr : 0.008500\n",
      "Epoch: 2111/25000, Training Loss: 0.001191, lr : 0.008500\n",
      "Epoch: 2112/25000, Training Loss: 0.001088, lr : 0.008500\n",
      "Epoch: 2113/25000, Training Loss: 0.001572, lr : 0.008500\n",
      "Epoch: 2114/25000, Training Loss: 0.001154, lr : 0.008500\n",
      "Epoch: 2115/25000, Training Loss: 0.001305, lr : 0.008500\n",
      "Epoch: 2116/25000, Training Loss: 0.001127, lr : 0.008500\n",
      "Epoch: 2117/25000, Training Loss: 0.001186, lr : 0.008500\n",
      "Epoch: 2118/25000, Training Loss: 0.001343, lr : 0.008500\n",
      "Epoch: 2119/25000, Training Loss: 0.001798, lr : 0.008500\n",
      "Epoch: 2120/25000, Training Loss: 0.001056, lr : 0.008500\n",
      "Epoch: 2121/25000, Training Loss: 0.001319, lr : 0.008500\n",
      "Epoch: 2122/25000, Training Loss: 0.001637, lr : 0.008500\n",
      "Epoch: 2123/25000, Training Loss: 0.001070, lr : 0.008500\n",
      "Epoch: 2124/25000, Training Loss: 0.001507, lr : 0.008500\n",
      "Epoch: 2125/25000, Training Loss: 0.001137, lr : 0.008500\n",
      "Epoch: 2126/25000, Training Loss: 0.001172, lr : 0.008500\n",
      "Epoch: 2127/25000, Training Loss: 0.001282, lr : 0.008500\n",
      "Epoch: 2128/25000, Training Loss: 0.001484, lr : 0.008500\n",
      "Epoch: 2129/25000, Training Loss: 0.001279, lr : 0.008500\n",
      "Epoch: 2130/25000, Training Loss: 0.001266, lr : 0.008500\n",
      "Epoch: 2131/25000, Training Loss: 0.001472, lr : 0.008500\n",
      "Epoch: 2132/25000, Training Loss: 0.001248, lr : 0.008500\n",
      "Epoch: 2133/25000, Training Loss: 0.001185, lr : 0.008500\n",
      "Epoch: 2134/25000, Training Loss: 0.001367, lr : 0.008500\n",
      "Epoch: 2135/25000, Training Loss: 0.001297, lr : 0.008500\n",
      "Epoch: 2136/25000, Training Loss: 0.001420, lr : 0.008500\n",
      "Epoch: 2137/25000, Training Loss: 0.001415, lr : 0.008500\n",
      "Epoch: 2138/25000, Training Loss: 0.001356, lr : 0.008500\n",
      "Epoch: 2139/25000, Training Loss: 0.001082, lr : 0.008500\n",
      "Epoch: 2140/25000, Training Loss: 0.000987, lr : 0.008500\n",
      "Epoch: 2141/25000, Training Loss: 0.001059, lr : 0.008500\n",
      "Epoch: 2142/25000, Training Loss: 0.001375, lr : 0.008500\n",
      "Epoch: 2143/25000, Training Loss: 0.001069, lr : 0.008500\n",
      "Epoch: 2144/25000, Training Loss: 0.001337, lr : 0.008500\n",
      "Epoch: 2145/25000, Training Loss: 0.001112, lr : 0.008500\n",
      "Epoch: 2146/25000, Training Loss: 0.001361, lr : 0.008500\n",
      "Epoch: 2147/25000, Training Loss: 0.001156, lr : 0.008500\n",
      "Epoch: 2148/25000, Training Loss: 0.001805, lr : 0.008500\n",
      "Epoch: 2149/25000, Training Loss: 0.001567, lr : 0.008500\n",
      "Epoch: 2150/25000, Training Loss: 0.001527, lr : 0.008500\n",
      "Epoch: 2151/25000, Training Loss: 0.001232, lr : 0.008500\n",
      "Epoch: 2152/25000, Training Loss: 0.001275, lr : 0.008500\n",
      "Epoch: 2153/25000, Training Loss: 0.001298, lr : 0.008500\n",
      "Epoch: 2154/25000, Training Loss: 0.001437, lr : 0.008500\n",
      "Epoch: 2155/25000, Training Loss: 0.001099, lr : 0.008500\n",
      "Epoch: 2156/25000, Training Loss: 0.001486, lr : 0.008500\n",
      "Epoch: 2157/25000, Training Loss: 0.001455, lr : 0.008500\n",
      "Epoch: 2158/25000, Training Loss: 0.001440, lr : 0.008500\n",
      "Epoch: 2159/25000, Training Loss: 0.001106, lr : 0.008500\n",
      "Epoch: 2160/25000, Training Loss: 0.001357, lr : 0.008500\n",
      "Epoch: 2161/25000, Training Loss: 0.001272, lr : 0.008500\n",
      "Epoch: 2162/25000, Training Loss: 0.001724, lr : 0.008500\n",
      "Epoch: 2163/25000, Training Loss: 0.001132, lr : 0.008500\n",
      "Epoch: 2164/25000, Training Loss: 0.000997, lr : 0.008500\n",
      "Epoch: 2165/25000, Training Loss: 0.001126, lr : 0.008500\n",
      "Epoch: 2166/25000, Training Loss: 0.001192, lr : 0.008500\n",
      "Epoch: 2167/25000, Training Loss: 0.001364, lr : 0.008500\n",
      "Epoch: 2168/25000, Training Loss: 0.001094, lr : 0.008500\n",
      "Epoch: 2169/25000, Training Loss: 0.001382, lr : 0.008500\n",
      "Epoch: 2170/25000, Training Loss: 0.001229, lr : 0.008500\n",
      "Epoch: 2171/25000, Training Loss: 0.001014, lr : 0.008500\n",
      "Epoch: 2172/25000, Training Loss: 0.001506, lr : 0.008500\n",
      "Epoch: 2173/25000, Training Loss: 0.001472, lr : 0.008500\n",
      "Epoch: 2174/25000, Training Loss: 0.001012, lr : 0.008500\n",
      "Epoch: 2175/25000, Training Loss: 0.001199, lr : 0.008500\n",
      "Epoch: 2176/25000, Training Loss: 0.001237, lr : 0.008500\n",
      "Epoch: 2177/25000, Training Loss: 0.001371, lr : 0.008500\n",
      "Epoch: 2178/25000, Training Loss: 0.001151, lr : 0.008500\n",
      "Epoch: 2179/25000, Training Loss: 0.001073, lr : 0.008500\n",
      "Epoch: 2180/25000, Training Loss: 0.001039, lr : 0.008500\n",
      "Epoch: 2181/25000, Training Loss: 0.001132, lr : 0.008500\n",
      "Epoch: 2182/25000, Training Loss: 0.000931, lr : 0.008500\n",
      "Epoch: 2183/25000, Training Loss: 0.000831, lr : 0.008500\n",
      "Epoch: 2184/25000, Training Loss: 0.001456, lr : 0.008500\n",
      "Epoch: 2185/25000, Training Loss: 0.001166, lr : 0.008500\n",
      "Epoch: 2186/25000, Training Loss: 0.001230, lr : 0.008500\n",
      "Epoch: 2187/25000, Training Loss: 0.001284, lr : 0.008500\n",
      "Epoch: 2188/25000, Training Loss: 0.001328, lr : 0.008500\n",
      "Epoch: 2189/25000, Training Loss: 0.001091, lr : 0.008500\n",
      "Epoch: 2190/25000, Training Loss: 0.001208, lr : 0.008500\n",
      "Epoch: 2191/25000, Training Loss: 0.000973, lr : 0.008500\n",
      "Epoch: 2192/25000, Training Loss: 0.001059, lr : 0.008500\n",
      "Epoch: 2193/25000, Training Loss: 0.001090, lr : 0.008500\n",
      "Epoch: 2194/25000, Training Loss: 0.001125, lr : 0.008500\n",
      "Epoch: 2195/25000, Training Loss: 0.001074, lr : 0.008500\n",
      "Epoch: 2196/25000, Training Loss: 0.001082, lr : 0.008500\n",
      "Epoch: 2197/25000, Training Loss: 0.001185, lr : 0.008500\n",
      "Epoch: 2198/25000, Training Loss: 0.001340, lr : 0.008500\n",
      "Epoch: 2199/25000, Training Loss: 0.000917, lr : 0.008500\n",
      "Epoch: 2200/25000, Training Loss: 0.000989, lr : 0.008500\n",
      "Epoch: 2201/25000, Training Loss: 0.000988, lr : 0.008500\n",
      "Epoch: 2202/25000, Training Loss: 0.001154, lr : 0.008500\n",
      "Epoch: 2203/25000, Training Loss: 0.001345, lr : 0.008500\n",
      "Epoch: 2204/25000, Training Loss: 0.001066, lr : 0.008500\n",
      "Epoch: 2205/25000, Training Loss: 0.001062, lr : 0.008500\n",
      "Epoch: 2206/25000, Training Loss: 0.001018, lr : 0.008500\n",
      "Epoch: 2207/25000, Training Loss: 0.001475, lr : 0.008500\n",
      "Epoch: 2208/25000, Training Loss: 0.000962, lr : 0.008500\n",
      "Epoch: 2209/25000, Training Loss: 0.001313, lr : 0.008500\n",
      "Epoch: 2210/25000, Training Loss: 0.001187, lr : 0.008500\n",
      "Epoch: 2211/25000, Training Loss: 0.001421, lr : 0.008500\n",
      "Epoch: 2212/25000, Training Loss: 0.001325, lr : 0.008500\n",
      "Epoch: 2213/25000, Training Loss: 0.001228, lr : 0.008500\n",
      "Epoch: 2214/25000, Training Loss: 0.000956, lr : 0.008500\n",
      "Epoch: 2215/25000, Training Loss: 0.001171, lr : 0.008500\n",
      "Epoch: 2216/25000, Training Loss: 0.001014, lr : 0.008500\n",
      "Epoch: 2217/25000, Training Loss: 0.001550, lr : 0.008500\n",
      "Epoch: 2218/25000, Training Loss: 0.001272, lr : 0.008500\n",
      "Epoch: 2219/25000, Training Loss: 0.001260, lr : 0.008500\n",
      "Epoch: 2220/25000, Training Loss: 0.001139, lr : 0.008500\n",
      "Epoch: 2221/25000, Training Loss: 0.001127, lr : 0.008500\n",
      "Epoch: 2222/25000, Training Loss: 0.001352, lr : 0.008500\n",
      "Epoch: 2223/25000, Training Loss: 0.001366, lr : 0.008500\n",
      "Epoch: 2224/25000, Training Loss: 0.001465, lr : 0.008500\n",
      "Epoch: 2225/25000, Training Loss: 0.001189, lr : 0.008500\n",
      "Epoch: 2226/25000, Training Loss: 0.001105, lr : 0.008500\n",
      "Epoch: 2227/25000, Training Loss: 0.001018, lr : 0.008500\n",
      "Epoch: 2228/25000, Training Loss: 0.001552, lr : 0.008500\n",
      "Epoch: 2229/25000, Training Loss: 0.001095, lr : 0.008500\n",
      "Epoch: 2230/25000, Training Loss: 0.001221, lr : 0.008500\n",
      "Epoch: 2231/25000, Training Loss: 0.001539, lr : 0.008500\n",
      "Epoch: 2232/25000, Training Loss: 0.001195, lr : 0.008500\n",
      "Epoch: 2233/25000, Training Loss: 0.001301, lr : 0.008500\n",
      "Epoch: 2234/25000, Training Loss: 0.001448, lr : 0.008500\n",
      "Epoch: 2235/25000, Training Loss: 0.001123, lr : 0.008500\n",
      "Epoch: 2236/25000, Training Loss: 0.000996, lr : 0.008500\n",
      "Epoch: 2237/25000, Training Loss: 0.001109, lr : 0.008500\n",
      "Epoch: 2238/25000, Training Loss: 0.001721, lr : 0.008500\n",
      "Epoch: 2239/25000, Training Loss: 0.001430, lr : 0.008500\n",
      "Epoch: 2240/25000, Training Loss: 0.000976, lr : 0.008500\n",
      "Epoch: 2241/25000, Training Loss: 0.001225, lr : 0.008500\n",
      "Epoch: 2242/25000, Training Loss: 0.001292, lr : 0.008500\n",
      "Epoch: 2243/25000, Training Loss: 0.001071, lr : 0.008500\n",
      "Epoch: 2244/25000, Training Loss: 0.001126, lr : 0.008500\n",
      "Epoch: 2245/25000, Training Loss: 0.001263, lr : 0.008500\n",
      "Epoch: 2246/25000, Training Loss: 0.000990, lr : 0.008500\n",
      "Epoch: 2247/25000, Training Loss: 0.001109, lr : 0.008500\n",
      "Epoch: 2248/25000, Training Loss: 0.001155, lr : 0.008500\n",
      "Epoch: 2249/25000, Training Loss: 0.001409, lr : 0.008500\n",
      "Epoch: 2250/25000, Training Loss: 0.001257, lr : 0.008500\n",
      "Epoch: 2251/25000, Training Loss: 0.001165, lr : 0.008500\n",
      "Epoch: 2252/25000, Training Loss: 0.001453, lr : 0.008500\n",
      "Epoch: 2253/25000, Training Loss: 0.001275, lr : 0.008500\n",
      "Epoch: 2254/25000, Training Loss: 0.001360, lr : 0.008500\n",
      "Epoch: 2255/25000, Training Loss: 0.000980, lr : 0.008500\n",
      "Epoch: 2256/25000, Training Loss: 0.001143, lr : 0.008500\n",
      "Epoch: 2257/25000, Training Loss: 0.001081, lr : 0.008500\n",
      "Epoch: 2258/25000, Training Loss: 0.001215, lr : 0.008500\n",
      "Epoch: 2259/25000, Training Loss: 0.001039, lr : 0.008500\n",
      "Epoch: 2260/25000, Training Loss: 0.001414, lr : 0.008500\n",
      "Epoch: 2261/25000, Training Loss: 0.001173, lr : 0.008500\n",
      "Epoch: 2262/25000, Training Loss: 0.001262, lr : 0.008500\n",
      "Epoch: 2263/25000, Training Loss: 0.001452, lr : 0.008500\n",
      "Epoch: 2264/25000, Training Loss: 0.001195, lr : 0.008500\n",
      "Epoch: 2265/25000, Training Loss: 0.001255, lr : 0.008500\n",
      "Epoch: 2266/25000, Training Loss: 0.001154, lr : 0.008500\n",
      "Epoch: 2267/25000, Training Loss: 0.001076, lr : 0.008500\n",
      "Epoch: 2268/25000, Training Loss: 0.001341, lr : 0.008500\n",
      "Epoch: 2269/25000, Training Loss: 0.001390, lr : 0.008500\n",
      "Epoch: 2270/25000, Training Loss: 0.001274, lr : 0.008500\n",
      "Epoch: 2271/25000, Training Loss: 0.001142, lr : 0.008500\n",
      "Epoch: 2272/25000, Training Loss: 0.001253, lr : 0.008500\n",
      "Epoch: 2273/25000, Training Loss: 0.001136, lr : 0.008500\n",
      "Epoch: 2274/25000, Training Loss: 0.000907, lr : 0.008500\n",
      "Epoch: 2275/25000, Training Loss: 0.001396, lr : 0.008500\n",
      "Epoch: 2276/25000, Training Loss: 0.001102, lr : 0.008500\n",
      "Epoch: 2277/25000, Training Loss: 0.001298, lr : 0.008500\n",
      "Epoch: 2278/25000, Training Loss: 0.001241, lr : 0.008500\n",
      "Epoch: 2279/25000, Training Loss: 0.001081, lr : 0.008500\n",
      "Epoch: 2280/25000, Training Loss: 0.001002, lr : 0.008500\n",
      "Epoch: 2281/25000, Training Loss: 0.000901, lr : 0.008500\n",
      "Epoch: 2282/25000, Training Loss: 0.001083, lr : 0.008500\n",
      "Epoch: 2283/25000, Training Loss: 0.001394, lr : 0.008500\n",
      "Epoch: 2284/25000, Training Loss: 0.001527, lr : 0.008500\n",
      "Epoch: 2285/25000, Training Loss: 0.001394, lr : 0.008500\n",
      "Epoch: 2286/25000, Training Loss: 0.000927, lr : 0.008500\n",
      "Epoch: 2287/25000, Training Loss: 0.001008, lr : 0.008500\n",
      "Epoch: 2288/25000, Training Loss: 0.000982, lr : 0.008500\n",
      "Epoch: 2289/25000, Training Loss: 0.001180, lr : 0.008500\n",
      "Epoch: 2290/25000, Training Loss: 0.001276, lr : 0.008500\n",
      "Epoch: 2291/25000, Training Loss: 0.001473, lr : 0.008500\n",
      "Epoch: 2292/25000, Training Loss: 0.001210, lr : 0.008500\n",
      "Epoch: 2293/25000, Training Loss: 0.001342, lr : 0.008500\n",
      "Epoch: 2294/25000, Training Loss: 0.001311, lr : 0.008500\n",
      "Epoch: 2295/25000, Training Loss: 0.001298, lr : 0.008500\n",
      "Epoch: 2296/25000, Training Loss: 0.001464, lr : 0.008500\n",
      "Epoch: 2297/25000, Training Loss: 0.001450, lr : 0.008500\n",
      "Epoch: 2298/25000, Training Loss: 0.001245, lr : 0.008500\n",
      "Epoch: 2299/25000, Training Loss: 0.001254, lr : 0.008500\n",
      "Epoch: 2300/25000, Training Loss: 0.001317, lr : 0.008500\n",
      "Epoch: 2301/25000, Training Loss: 0.000984, lr : 0.008500\n",
      "Epoch: 2302/25000, Training Loss: 0.001214, lr : 0.008500\n",
      "Epoch: 2303/25000, Training Loss: 0.000826, lr : 0.008500\n",
      "Epoch: 2304/25000, Training Loss: 0.001390, lr : 0.008500\n",
      "Epoch: 2305/25000, Training Loss: 0.001045, lr : 0.008500\n",
      "Epoch: 2306/25000, Training Loss: 0.001371, lr : 0.008500\n",
      "Epoch: 2307/25000, Training Loss: 0.000863, lr : 0.008500\n",
      "Epoch: 2308/25000, Training Loss: 0.001135, lr : 0.008500\n",
      "Epoch: 2309/25000, Training Loss: 0.001131, lr : 0.008500\n",
      "Epoch: 2310/25000, Training Loss: 0.001386, lr : 0.008500\n",
      "Epoch: 2311/25000, Training Loss: 0.001144, lr : 0.008500\n",
      "Epoch: 2312/25000, Training Loss: 0.001104, lr : 0.008500\n",
      "Epoch: 2313/25000, Training Loss: 0.001049, lr : 0.008500\n",
      "Epoch: 2314/25000, Training Loss: 0.001385, lr : 0.008500\n",
      "Epoch: 2315/25000, Training Loss: 0.001251, lr : 0.008500\n",
      "Epoch: 2316/25000, Training Loss: 0.001073, lr : 0.008500\n",
      "Epoch: 2317/25000, Training Loss: 0.000822, lr : 0.008500\n",
      "Epoch: 2318/25000, Training Loss: 0.001017, lr : 0.008500\n",
      "Epoch: 2319/25000, Training Loss: 0.001195, lr : 0.008500\n",
      "Epoch: 2320/25000, Training Loss: 0.001015, lr : 0.008500\n",
      "Epoch: 2321/25000, Training Loss: 0.000849, lr : 0.008500\n",
      "Epoch: 2322/25000, Training Loss: 0.001105, lr : 0.008500\n",
      "Epoch: 2323/25000, Training Loss: 0.001346, lr : 0.008500\n",
      "Epoch: 2324/25000, Training Loss: 0.001032, lr : 0.008500\n",
      "Epoch: 2325/25000, Training Loss: 0.001568, lr : 0.008500\n",
      "Epoch: 2326/25000, Training Loss: 0.000913, lr : 0.008500\n",
      "Epoch: 2327/25000, Training Loss: 0.001303, lr : 0.008500\n",
      "Epoch: 2328/25000, Training Loss: 0.001131, lr : 0.008500\n",
      "Epoch: 2329/25000, Training Loss: 0.001174, lr : 0.008500\n",
      "Epoch: 2330/25000, Training Loss: 0.001147, lr : 0.008500\n",
      "Epoch: 2331/25000, Training Loss: 0.001288, lr : 0.008500\n",
      "Epoch: 2332/25000, Training Loss: 0.001428, lr : 0.008500\n",
      "Epoch: 2333/25000, Training Loss: 0.000936, lr : 0.008500\n",
      "Epoch: 2334/25000, Training Loss: 0.001135, lr : 0.008500\n",
      "Epoch: 2335/25000, Training Loss: 0.001117, lr : 0.008500\n",
      "Epoch: 2336/25000, Training Loss: 0.001166, lr : 0.008500\n",
      "Epoch: 2337/25000, Training Loss: 0.000902, lr : 0.008500\n",
      "Epoch: 2338/25000, Training Loss: 0.001356, lr : 0.008500\n",
      "Epoch: 2339/25000, Training Loss: 0.001194, lr : 0.008500\n",
      "Epoch: 2340/25000, Training Loss: 0.001081, lr : 0.008500\n",
      "Epoch: 2341/25000, Training Loss: 0.000971, lr : 0.008500\n",
      "Epoch: 2342/25000, Training Loss: 0.001419, lr : 0.008500\n",
      "Epoch: 2343/25000, Training Loss: 0.001323, lr : 0.008500\n",
      "Epoch: 2344/25000, Training Loss: 0.001175, lr : 0.008500\n",
      "Epoch: 2345/25000, Training Loss: 0.001431, lr : 0.008500\n",
      "Epoch: 2346/25000, Training Loss: 0.001006, lr : 0.008500\n",
      "Epoch: 2347/25000, Training Loss: 0.000972, lr : 0.008500\n",
      "Epoch: 2348/25000, Training Loss: 0.001439, lr : 0.008500\n",
      "Epoch: 2349/25000, Training Loss: 0.001085, lr : 0.008500\n",
      "Epoch: 2350/25000, Training Loss: 0.001073, lr : 0.008500\n",
      "Epoch: 2351/25000, Training Loss: 0.001081, lr : 0.008500\n",
      "Epoch: 2352/25000, Training Loss: 0.001079, lr : 0.008500\n",
      "Epoch: 2353/25000, Training Loss: 0.001274, lr : 0.008500\n",
      "Epoch: 2354/25000, Training Loss: 0.001096, lr : 0.008500\n",
      "Epoch: 2355/25000, Training Loss: 0.001181, lr : 0.008500\n",
      "Epoch: 2356/25000, Training Loss: 0.001469, lr : 0.008500\n",
      "Epoch: 2357/25000, Training Loss: 0.001229, lr : 0.008500\n",
      "Epoch: 2358/25000, Training Loss: 0.001761, lr : 0.008500\n",
      "Epoch: 2359/25000, Training Loss: 0.001419, lr : 0.008500\n",
      "Epoch: 2360/25000, Training Loss: 0.000983, lr : 0.008500\n",
      "Epoch: 2361/25000, Training Loss: 0.001190, lr : 0.008500\n",
      "Epoch: 2362/25000, Training Loss: 0.001203, lr : 0.008500\n",
      "Epoch: 2363/25000, Training Loss: 0.001156, lr : 0.008500\n",
      "Epoch: 2364/25000, Training Loss: 0.001247, lr : 0.008500\n",
      "Epoch: 2365/25000, Training Loss: 0.000922, lr : 0.008500\n",
      "Epoch: 2366/25000, Training Loss: 0.001629, lr : 0.008500\n",
      "Epoch: 2367/25000, Training Loss: 0.001256, lr : 0.008500\n",
      "Epoch: 2368/25000, Training Loss: 0.001319, lr : 0.008500\n",
      "Epoch: 2369/25000, Training Loss: 0.000963, lr : 0.008500\n",
      "Epoch: 2370/25000, Training Loss: 0.001438, lr : 0.008500\n",
      "Epoch: 2371/25000, Training Loss: 0.001363, lr : 0.008500\n",
      "Epoch: 2372/25000, Training Loss: 0.000998, lr : 0.008500\n",
      "Epoch: 2373/25000, Training Loss: 0.000767, lr : 0.008500\n",
      "Epoch: 2374/25000, Training Loss: 0.000929, lr : 0.008500\n",
      "Epoch: 2375/25000, Training Loss: 0.001099, lr : 0.008500\n",
      "Epoch: 2376/25000, Training Loss: 0.001227, lr : 0.008500\n",
      "Epoch: 2377/25000, Training Loss: 0.001066, lr : 0.008500\n",
      "Epoch: 2378/25000, Training Loss: 0.001213, lr : 0.008500\n",
      "Epoch: 2379/25000, Training Loss: 0.001136, lr : 0.008500\n",
      "Epoch: 2380/25000, Training Loss: 0.000831, lr : 0.008500\n",
      "Epoch: 2381/25000, Training Loss: 0.000958, lr : 0.008500\n",
      "Epoch: 2382/25000, Training Loss: 0.001067, lr : 0.008500\n",
      "Epoch: 2383/25000, Training Loss: 0.001376, lr : 0.008500\n",
      "Epoch: 2384/25000, Training Loss: 0.001157, lr : 0.008500\n",
      "Epoch: 2385/25000, Training Loss: 0.001285, lr : 0.008500\n",
      "Epoch: 2386/25000, Training Loss: 0.001185, lr : 0.008500\n",
      "Epoch: 2387/25000, Training Loss: 0.000900, lr : 0.008500\n",
      "Epoch: 2388/25000, Training Loss: 0.001198, lr : 0.008500\n",
      "Epoch: 2389/25000, Training Loss: 0.000964, lr : 0.008500\n",
      "Epoch: 2390/25000, Training Loss: 0.000825, lr : 0.008500\n",
      "Epoch: 2391/25000, Training Loss: 0.001015, lr : 0.008500\n",
      "Epoch: 2392/25000, Training Loss: 0.001117, lr : 0.008500\n",
      "Epoch: 2393/25000, Training Loss: 0.001122, lr : 0.008500\n",
      "Epoch: 2394/25000, Training Loss: 0.001119, lr : 0.008500\n",
      "Epoch: 2395/25000, Training Loss: 0.001412, lr : 0.008500\n",
      "Epoch: 2396/25000, Training Loss: 0.001034, lr : 0.008500\n",
      "Epoch: 2397/25000, Training Loss: 0.001084, lr : 0.008500\n",
      "Epoch: 2398/25000, Training Loss: 0.001271, lr : 0.008500\n",
      "Epoch: 2399/25000, Training Loss: 0.001057, lr : 0.008500\n",
      "Epoch: 2400/25000, Training Loss: 0.001066, lr : 0.008500\n",
      "Epoch: 2401/25000, Training Loss: 0.000942, lr : 0.008500\n",
      "Epoch: 2402/25000, Training Loss: 0.001404, lr : 0.008500\n",
      "Epoch: 2403/25000, Training Loss: 0.001005, lr : 0.008500\n",
      "Epoch: 2404/25000, Training Loss: 0.001003, lr : 0.008500\n",
      "Epoch: 2405/25000, Training Loss: 0.001038, lr : 0.008500\n",
      "Epoch: 2406/25000, Training Loss: 0.000879, lr : 0.008500\n",
      "Epoch: 2407/25000, Training Loss: 0.001008, lr : 0.008500\n",
      "Epoch: 2408/25000, Training Loss: 0.001187, lr : 0.008500\n",
      "Epoch: 2409/25000, Training Loss: 0.001159, lr : 0.008500\n",
      "Epoch: 2410/25000, Training Loss: 0.001303, lr : 0.008500\n",
      "Epoch: 2411/25000, Training Loss: 0.001310, lr : 0.008500\n",
      "Epoch: 2412/25000, Training Loss: 0.001145, lr : 0.008500\n",
      "Epoch: 2413/25000, Training Loss: 0.001176, lr : 0.008500\n",
      "Epoch: 2414/25000, Training Loss: 0.001383, lr : 0.008500\n",
      "Epoch: 2415/25000, Training Loss: 0.001180, lr : 0.008500\n",
      "Epoch: 2416/25000, Training Loss: 0.001119, lr : 0.008500\n",
      "Epoch: 2417/25000, Training Loss: 0.001018, lr : 0.008500\n",
      "Epoch: 2418/25000, Training Loss: 0.001495, lr : 0.008500\n",
      "Epoch: 2419/25000, Training Loss: 0.001181, lr : 0.008500\n",
      "Epoch: 2420/25000, Training Loss: 0.000837, lr : 0.008500\n",
      "Epoch: 2421/25000, Training Loss: 0.001217, lr : 0.008500\n",
      "Epoch: 2422/25000, Training Loss: 0.001102, lr : 0.008500\n",
      "Epoch: 2423/25000, Training Loss: 0.000949, lr : 0.008500\n",
      "Epoch: 2424/25000, Training Loss: 0.000990, lr : 0.008500\n",
      "Epoch: 2425/25000, Training Loss: 0.001248, lr : 0.008500\n",
      "Epoch: 2426/25000, Training Loss: 0.001289, lr : 0.008500\n",
      "Epoch: 2427/25000, Training Loss: 0.001010, lr : 0.008500\n",
      "Epoch: 2428/25000, Training Loss: 0.000842, lr : 0.008500\n",
      "Epoch: 2429/25000, Training Loss: 0.001155, lr : 0.008500\n",
      "Epoch: 2430/25000, Training Loss: 0.000915, lr : 0.008500\n",
      "Epoch: 2431/25000, Training Loss: 0.000744, lr : 0.008500\n",
      "Epoch: 2432/25000, Training Loss: 0.000754, lr : 0.008500\n",
      "Epoch: 2433/25000, Training Loss: 0.000995, lr : 0.008500\n",
      "Epoch: 2434/25000, Training Loss: 0.001331, lr : 0.008500\n",
      "Epoch: 2435/25000, Training Loss: 0.000935, lr : 0.008500\n",
      "Epoch: 2436/25000, Training Loss: 0.001110, lr : 0.008500\n",
      "Epoch: 2437/25000, Training Loss: 0.000899, lr : 0.008500\n",
      "Epoch: 2438/25000, Training Loss: 0.001094, lr : 0.008500\n",
      "Epoch: 2439/25000, Training Loss: 0.000871, lr : 0.008500\n",
      "Epoch: 2440/25000, Training Loss: 0.001214, lr : 0.008500\n",
      "Epoch: 2441/25000, Training Loss: 0.001199, lr : 0.008500\n",
      "Epoch: 2442/25000, Training Loss: 0.000940, lr : 0.008500\n",
      "Epoch: 2443/25000, Training Loss: 0.001227, lr : 0.008500\n",
      "Epoch: 2444/25000, Training Loss: 0.000751, lr : 0.008500\n",
      "Epoch: 2445/25000, Training Loss: 0.000873, lr : 0.008500\n",
      "Epoch: 2446/25000, Training Loss: 0.001272, lr : 0.008500\n",
      "Epoch: 2447/25000, Training Loss: 0.000880, lr : 0.008500\n",
      "Epoch: 2448/25000, Training Loss: 0.001108, lr : 0.008500\n",
      "Epoch: 2449/25000, Training Loss: 0.001133, lr : 0.008500\n",
      "Epoch: 2450/25000, Training Loss: 0.001259, lr : 0.008500\n",
      "Epoch: 2451/25000, Training Loss: 0.000781, lr : 0.008500\n",
      "Epoch: 2452/25000, Training Loss: 0.001007, lr : 0.008500\n",
      "Epoch: 2453/25000, Training Loss: 0.001019, lr : 0.008500\n",
      "Epoch: 2454/25000, Training Loss: 0.001246, lr : 0.008500\n",
      "Epoch: 2455/25000, Training Loss: 0.001280, lr : 0.008500\n",
      "Epoch: 2456/25000, Training Loss: 0.000908, lr : 0.008500\n",
      "Epoch: 2457/25000, Training Loss: 0.001051, lr : 0.008500\n",
      "Epoch: 2458/25000, Training Loss: 0.001351, lr : 0.008500\n",
      "Epoch: 2459/25000, Training Loss: 0.001047, lr : 0.008500\n",
      "Epoch: 2460/25000, Training Loss: 0.001297, lr : 0.008500\n",
      "Epoch: 2461/25000, Training Loss: 0.001339, lr : 0.008500\n",
      "Epoch: 2462/25000, Training Loss: 0.001146, lr : 0.008500\n",
      "Epoch: 2463/25000, Training Loss: 0.001158, lr : 0.008500\n",
      "Epoch: 2464/25000, Training Loss: 0.000998, lr : 0.008500\n",
      "Epoch: 2465/25000, Training Loss: 0.000926, lr : 0.008500\n",
      "Epoch: 2466/25000, Training Loss: 0.001087, lr : 0.008500\n",
      "Epoch: 2467/25000, Training Loss: 0.000754, lr : 0.008500\n",
      "Epoch: 2468/25000, Training Loss: 0.000906, lr : 0.008500\n",
      "Epoch: 2469/25000, Training Loss: 0.000924, lr : 0.008500\n",
      "Epoch: 2470/25000, Training Loss: 0.001155, lr : 0.008500\n",
      "Epoch: 2471/25000, Training Loss: 0.000841, lr : 0.008500\n",
      "Epoch: 2472/25000, Training Loss: 0.001243, lr : 0.008500\n",
      "Epoch: 2473/25000, Training Loss: 0.000988, lr : 0.008500\n",
      "Epoch: 2474/25000, Training Loss: 0.001081, lr : 0.008500\n",
      "Epoch: 2475/25000, Training Loss: 0.001416, lr : 0.008500\n",
      "Epoch: 2476/25000, Training Loss: 0.001083, lr : 0.008500\n",
      "Epoch: 2477/25000, Training Loss: 0.000930, lr : 0.008500\n",
      "Epoch: 2478/25000, Training Loss: 0.000935, lr : 0.008500\n",
      "Epoch: 2479/25000, Training Loss: 0.000945, lr : 0.008500\n",
      "Epoch: 2480/25000, Training Loss: 0.001050, lr : 0.008500\n",
      "Epoch: 2481/25000, Training Loss: 0.001103, lr : 0.008500\n",
      "Epoch: 2482/25000, Training Loss: 0.001216, lr : 0.008500\n",
      "Epoch: 2483/25000, Training Loss: 0.000887, lr : 0.008500\n",
      "Epoch: 2484/25000, Training Loss: 0.001030, lr : 0.008500\n",
      "Epoch: 2485/25000, Training Loss: 0.001439, lr : 0.008500\n",
      "Epoch: 2486/25000, Training Loss: 0.000858, lr : 0.008500\n",
      "Epoch: 2487/25000, Training Loss: 0.001172, lr : 0.008500\n",
      "Epoch: 2488/25000, Training Loss: 0.000843, lr : 0.008500\n",
      "Epoch: 2489/25000, Training Loss: 0.000943, lr : 0.008500\n",
      "Epoch: 2490/25000, Training Loss: 0.001216, lr : 0.008500\n",
      "Epoch: 2491/25000, Training Loss: 0.001149, lr : 0.008500\n",
      "Epoch: 2492/25000, Training Loss: 0.001394, lr : 0.008500\n",
      "Epoch: 2493/25000, Training Loss: 0.001365, lr : 0.008500\n",
      "Epoch: 2494/25000, Training Loss: 0.001108, lr : 0.008500\n",
      "Epoch: 2495/25000, Training Loss: 0.000913, lr : 0.008500\n",
      "Epoch: 2496/25000, Training Loss: 0.001324, lr : 0.008500\n",
      "Epoch: 2497/25000, Training Loss: 0.000797, lr : 0.008500\n",
      "Epoch: 2498/25000, Training Loss: 0.001139, lr : 0.008500\n",
      "Epoch: 2499/25000, Training Loss: 0.000955, lr : 0.008500\n",
      "Epoch: 2500/25000, Training Loss: 0.001109, lr : 0.008500\n",
      "Epoch: 2501/25000, Training Loss: 0.001183, lr : 0.008500\n",
      "Epoch: 2502/25000, Training Loss: 0.001458, lr : 0.008500\n",
      "Epoch: 2503/25000, Training Loss: 0.001240, lr : 0.008500\n",
      "Epoch: 2504/25000, Training Loss: 0.001273, lr : 0.008500\n",
      "Epoch: 2505/25000, Training Loss: 0.001173, lr : 0.008500\n",
      "Epoch: 2506/25000, Training Loss: 0.000907, lr : 0.008500\n",
      "Epoch: 2507/25000, Training Loss: 0.001385, lr : 0.008500\n",
      "Epoch: 2508/25000, Training Loss: 0.001050, lr : 0.008500\n",
      "Epoch: 2509/25000, Training Loss: 0.001369, lr : 0.008500\n",
      "Epoch: 2510/25000, Training Loss: 0.001557, lr : 0.008500\n",
      "Epoch: 2511/25000, Training Loss: 0.001271, lr : 0.008500\n",
      "Epoch: 2512/25000, Training Loss: 0.000831, lr : 0.008500\n",
      "Epoch: 2513/25000, Training Loss: 0.001020, lr : 0.008500\n",
      "Epoch: 2514/25000, Training Loss: 0.000901, lr : 0.008500\n",
      "Epoch: 2515/25000, Training Loss: 0.000804, lr : 0.008500\n",
      "Epoch: 2516/25000, Training Loss: 0.001327, lr : 0.008500\n",
      "Epoch: 2517/25000, Training Loss: 0.001107, lr : 0.008500\n",
      "Epoch: 2518/25000, Training Loss: 0.001047, lr : 0.008500\n",
      "Epoch: 2519/25000, Training Loss: 0.000858, lr : 0.008500\n",
      "Epoch: 2520/25000, Training Loss: 0.000995, lr : 0.008500\n",
      "Epoch: 2521/25000, Training Loss: 0.000964, lr : 0.008500\n",
      "Epoch: 2522/25000, Training Loss: 0.000906, lr : 0.008500\n",
      "Epoch: 2523/25000, Training Loss: 0.000998, lr : 0.008500\n",
      "Epoch: 2524/25000, Training Loss: 0.001182, lr : 0.008500\n",
      "Epoch: 2525/25000, Training Loss: 0.001248, lr : 0.008500\n",
      "Epoch: 2526/25000, Training Loss: 0.000862, lr : 0.008500\n",
      "Epoch: 2527/25000, Training Loss: 0.001341, lr : 0.008500\n",
      "Epoch: 2528/25000, Training Loss: 0.001188, lr : 0.008500\n",
      "Epoch: 2529/25000, Training Loss: 0.001244, lr : 0.008500\n",
      "Epoch: 2530/25000, Training Loss: 0.001370, lr : 0.008500\n",
      "Epoch: 2531/25000, Training Loss: 0.000858, lr : 0.008500\n",
      "Epoch: 2532/25000, Training Loss: 0.000999, lr : 0.008500\n",
      "Epoch: 2533/25000, Training Loss: 0.001119, lr : 0.008500\n",
      "Epoch: 2534/25000, Training Loss: 0.001301, lr : 0.008500\n",
      "Epoch: 2535/25000, Training Loss: 0.001040, lr : 0.008500\n",
      "Epoch: 2536/25000, Training Loss: 0.001244, lr : 0.008500\n",
      "Epoch: 2537/25000, Training Loss: 0.000864, lr : 0.008500\n",
      "Epoch: 2538/25000, Training Loss: 0.001018, lr : 0.008500\n",
      "Epoch: 2539/25000, Training Loss: 0.001243, lr : 0.008500\n",
      "Epoch: 2540/25000, Training Loss: 0.000938, lr : 0.008500\n",
      "Epoch: 2541/25000, Training Loss: 0.001232, lr : 0.008500\n",
      "Epoch: 2542/25000, Training Loss: 0.001051, lr : 0.008500\n",
      "Epoch: 2543/25000, Training Loss: 0.000731, lr : 0.008500\n",
      "Epoch: 2544/25000, Training Loss: 0.000866, lr : 0.008500\n",
      "Epoch: 2545/25000, Training Loss: 0.001042, lr : 0.008500\n",
      "Epoch: 2546/25000, Training Loss: 0.000991, lr : 0.008500\n",
      "Epoch: 2547/25000, Training Loss: 0.001101, lr : 0.008500\n",
      "Epoch: 2548/25000, Training Loss: 0.000999, lr : 0.008500\n",
      "Epoch: 2549/25000, Training Loss: 0.000837, lr : 0.008500\n",
      "Epoch: 2550/25000, Training Loss: 0.001371, lr : 0.008500\n",
      "Epoch: 2551/25000, Training Loss: 0.000730, lr : 0.008500\n",
      "Epoch: 2552/25000, Training Loss: 0.001136, lr : 0.008500\n",
      "Epoch: 2553/25000, Training Loss: 0.000916, lr : 0.008500\n",
      "Epoch: 2554/25000, Training Loss: 0.000664, lr : 0.008500\n",
      "Epoch: 2555/25000, Training Loss: 0.000961, lr : 0.008500\n",
      "Epoch: 2556/25000, Training Loss: 0.001099, lr : 0.008500\n",
      "Epoch: 2557/25000, Training Loss: 0.000945, lr : 0.008500\n",
      "Epoch: 2558/25000, Training Loss: 0.000946, lr : 0.008500\n",
      "Epoch: 2559/25000, Training Loss: 0.001254, lr : 0.008500\n",
      "Epoch: 2560/25000, Training Loss: 0.000866, lr : 0.008500\n",
      "Epoch: 2561/25000, Training Loss: 0.000682, lr : 0.008500\n",
      "Epoch: 2562/25000, Training Loss: 0.000908, lr : 0.008500\n",
      "Epoch: 2563/25000, Training Loss: 0.000825, lr : 0.008500\n",
      "Epoch: 2564/25000, Training Loss: 0.001088, lr : 0.008500\n",
      "Epoch: 2565/25000, Training Loss: 0.000988, lr : 0.008500\n",
      "Epoch: 2566/25000, Training Loss: 0.001311, lr : 0.008500\n",
      "Epoch: 2567/25000, Training Loss: 0.001512, lr : 0.008500\n",
      "Epoch: 2568/25000, Training Loss: 0.001196, lr : 0.008500\n",
      "Epoch: 2569/25000, Training Loss: 0.001422, lr : 0.008500\n",
      "Epoch: 2570/25000, Training Loss: 0.001335, lr : 0.008500\n",
      "Epoch: 2571/25000, Training Loss: 0.001103, lr : 0.008500\n",
      "Epoch: 2572/25000, Training Loss: 0.001253, lr : 0.008500\n",
      "Epoch: 2573/25000, Training Loss: 0.001319, lr : 0.008500\n",
      "Epoch: 2574/25000, Training Loss: 0.001370, lr : 0.008500\n",
      "Epoch: 2575/25000, Training Loss: 0.001519, lr : 0.008500\n",
      "Epoch: 2576/25000, Training Loss: 0.001368, lr : 0.008500\n",
      "Epoch: 2577/25000, Training Loss: 0.001316, lr : 0.008500\n",
      "Epoch: 2578/25000, Training Loss: 0.001406, lr : 0.008500\n",
      "Epoch: 2579/25000, Training Loss: 0.001210, lr : 0.008500\n",
      "Epoch: 2580/25000, Training Loss: 0.000935, lr : 0.008500\n",
      "Epoch: 2581/25000, Training Loss: 0.001082, lr : 0.008500\n",
      "Epoch: 2582/25000, Training Loss: 0.001246, lr : 0.008500\n",
      "Epoch: 2583/25000, Training Loss: 0.001454, lr : 0.008500\n",
      "Epoch: 2584/25000, Training Loss: 0.001152, lr : 0.008500\n",
      "Epoch: 2585/25000, Training Loss: 0.000958, lr : 0.008500\n",
      "Epoch: 2586/25000, Training Loss: 0.001362, lr : 0.008500\n",
      "Epoch: 2587/25000, Training Loss: 0.001068, lr : 0.008500\n",
      "Epoch: 2588/25000, Training Loss: 0.001202, lr : 0.008500\n",
      "Epoch: 2589/25000, Training Loss: 0.001248, lr : 0.008500\n",
      "Epoch: 2590/25000, Training Loss: 0.001326, lr : 0.008500\n",
      "Epoch: 2591/25000, Training Loss: 0.001397, lr : 0.008500\n",
      "Epoch: 2592/25000, Training Loss: 0.001306, lr : 0.008500\n",
      "Epoch: 2593/25000, Training Loss: 0.001043, lr : 0.008500\n",
      "Epoch: 2594/25000, Training Loss: 0.001128, lr : 0.008500\n",
      "Epoch: 2595/25000, Training Loss: 0.001253, lr : 0.008500\n",
      "Epoch: 2596/25000, Training Loss: 0.001203, lr : 0.008500\n",
      "Epoch: 2597/25000, Training Loss: 0.001055, lr : 0.008500\n",
      "Epoch: 2598/25000, Training Loss: 0.001298, lr : 0.008500\n",
      "Epoch: 2599/25000, Training Loss: 0.001029, lr : 0.008500\n",
      "Epoch: 2600/25000, Training Loss: 0.000952, lr : 0.008500\n",
      "Epoch: 2601/25000, Training Loss: 0.001220, lr : 0.008500\n",
      "Epoch: 2602/25000, Training Loss: 0.001020, lr : 0.008500\n",
      "Epoch: 2603/25000, Training Loss: 0.001181, lr : 0.008500\n",
      "Epoch: 2604/25000, Training Loss: 0.001084, lr : 0.008500\n",
      "Epoch: 2605/25000, Training Loss: 0.000994, lr : 0.008500\n",
      "Epoch: 2606/25000, Training Loss: 0.001171, lr : 0.008500\n",
      "Epoch: 2607/25000, Training Loss: 0.001208, lr : 0.008500\n",
      "Epoch: 2608/25000, Training Loss: 0.001035, lr : 0.008500\n",
      "Epoch: 2609/25000, Training Loss: 0.001189, lr : 0.008500\n",
      "Epoch: 2610/25000, Training Loss: 0.001087, lr : 0.008500\n",
      "Epoch: 2611/25000, Training Loss: 0.000693, lr : 0.008500\n",
      "Epoch: 2612/25000, Training Loss: 0.001082, lr : 0.008500\n",
      "Epoch: 2613/25000, Training Loss: 0.001097, lr : 0.008500\n",
      "Epoch: 2614/25000, Training Loss: 0.000999, lr : 0.008500\n",
      "Epoch: 2615/25000, Training Loss: 0.000947, lr : 0.008500\n",
      "Epoch: 2616/25000, Training Loss: 0.000930, lr : 0.008500\n",
      "Epoch: 2617/25000, Training Loss: 0.000969, lr : 0.008500\n",
      "Epoch: 2618/25000, Training Loss: 0.001300, lr : 0.008500\n",
      "Epoch: 2619/25000, Training Loss: 0.001357, lr : 0.008500\n",
      "Epoch: 2620/25000, Training Loss: 0.001008, lr : 0.008500\n",
      "Epoch: 2621/25000, Training Loss: 0.001047, lr : 0.008500\n",
      "Epoch: 2622/25000, Training Loss: 0.000892, lr : 0.008500\n",
      "Epoch: 2623/25000, Training Loss: 0.001319, lr : 0.008500\n",
      "Epoch: 2624/25000, Training Loss: 0.001039, lr : 0.008500\n",
      "Epoch: 2625/25000, Training Loss: 0.001226, lr : 0.008500\n",
      "Epoch: 2626/25000, Training Loss: 0.001083, lr : 0.008500\n",
      "Epoch: 2627/25000, Training Loss: 0.001007, lr : 0.008500\n",
      "Epoch: 2628/25000, Training Loss: 0.000813, lr : 0.008500\n",
      "Epoch: 2629/25000, Training Loss: 0.001407, lr : 0.008500\n",
      "Epoch: 2630/25000, Training Loss: 0.000886, lr : 0.008500\n",
      "Epoch: 2631/25000, Training Loss: 0.000907, lr : 0.008500\n",
      "Epoch: 2632/25000, Training Loss: 0.000685, lr : 0.008500\n",
      "Epoch: 2633/25000, Training Loss: 0.000892, lr : 0.008500\n",
      "Epoch: 2634/25000, Training Loss: 0.000891, lr : 0.008500\n",
      "Epoch: 2635/25000, Training Loss: 0.000723, lr : 0.008500\n",
      "Epoch: 2636/25000, Training Loss: 0.000830, lr : 0.008500\n",
      "Epoch: 2637/25000, Training Loss: 0.000726, lr : 0.008500\n",
      "Epoch: 2638/25000, Training Loss: 0.000858, lr : 0.008500\n",
      "Epoch: 2639/25000, Training Loss: 0.000841, lr : 0.008500\n",
      "Epoch: 2640/25000, Training Loss: 0.001161, lr : 0.008500\n",
      "Epoch: 2641/25000, Training Loss: 0.001091, lr : 0.008500\n",
      "Epoch: 2642/25000, Training Loss: 0.000888, lr : 0.008500\n",
      "Epoch: 2643/25000, Training Loss: 0.000819, lr : 0.008500\n",
      "Epoch: 2644/25000, Training Loss: 0.000981, lr : 0.008500\n",
      "Epoch: 2645/25000, Training Loss: 0.001031, lr : 0.008500\n",
      "Epoch: 2646/25000, Training Loss: 0.001226, lr : 0.008500\n",
      "Epoch: 2647/25000, Training Loss: 0.000973, lr : 0.008500\n",
      "Epoch: 2648/25000, Training Loss: 0.001018, lr : 0.008500\n",
      "Epoch: 2649/25000, Training Loss: 0.000952, lr : 0.008500\n",
      "Epoch: 2650/25000, Training Loss: 0.001051, lr : 0.008500\n",
      "Epoch: 2651/25000, Training Loss: 0.000853, lr : 0.008500\n",
      "Epoch: 2652/25000, Training Loss: 0.000875, lr : 0.008500\n",
      "Epoch: 2653/25000, Training Loss: 0.001253, lr : 0.008500\n",
      "Epoch: 2654/25000, Training Loss: 0.000772, lr : 0.008500\n",
      "Epoch: 2655/25000, Training Loss: 0.001145, lr : 0.008500\n",
      "Epoch: 2656/25000, Training Loss: 0.000951, lr : 0.008500\n",
      "Epoch: 2657/25000, Training Loss: 0.000897, lr : 0.008500\n",
      "Epoch: 2658/25000, Training Loss: 0.001224, lr : 0.008500\n",
      "Epoch: 2659/25000, Training Loss: 0.000933, lr : 0.008500\n",
      "Epoch: 2660/25000, Training Loss: 0.000915, lr : 0.008500\n",
      "Epoch: 2661/25000, Training Loss: 0.001125, lr : 0.008500\n",
      "Epoch: 2662/25000, Training Loss: 0.001199, lr : 0.008500\n",
      "Epoch: 2663/25000, Training Loss: 0.001045, lr : 0.008500\n",
      "Epoch: 2664/25000, Training Loss: 0.001159, lr : 0.008500\n",
      "Epoch: 2665/25000, Training Loss: 0.000916, lr : 0.008500\n",
      "Epoch: 2666/25000, Training Loss: 0.000654, lr : 0.008500\n",
      "Epoch: 2667/25000, Training Loss: 0.000946, lr : 0.008500\n",
      "Epoch: 2668/25000, Training Loss: 0.000813, lr : 0.008500\n",
      "Epoch: 2669/25000, Training Loss: 0.000899, lr : 0.008500\n",
      "Epoch: 2670/25000, Training Loss: 0.000847, lr : 0.008500\n",
      "Epoch: 2671/25000, Training Loss: 0.000988, lr : 0.008500\n",
      "Epoch: 2672/25000, Training Loss: 0.000862, lr : 0.008500\n",
      "Epoch: 2673/25000, Training Loss: 0.000990, lr : 0.008500\n",
      "Epoch: 2674/25000, Training Loss: 0.000857, lr : 0.008500\n",
      "Epoch: 2675/25000, Training Loss: 0.000932, lr : 0.008500\n",
      "Epoch: 2676/25000, Training Loss: 0.000959, lr : 0.008500\n",
      "Epoch: 2677/25000, Training Loss: 0.000920, lr : 0.008500\n",
      "Epoch: 2678/25000, Training Loss: 0.001010, lr : 0.008500\n",
      "Epoch: 2679/25000, Training Loss: 0.000930, lr : 0.008500\n",
      "Epoch: 2680/25000, Training Loss: 0.000874, lr : 0.008500\n",
      "Epoch: 2681/25000, Training Loss: 0.000877, lr : 0.008500\n",
      "Epoch: 2682/25000, Training Loss: 0.001042, lr : 0.008500\n",
      "Epoch: 2683/25000, Training Loss: 0.000806, lr : 0.008500\n",
      "Epoch: 2684/25000, Training Loss: 0.001007, lr : 0.008500\n",
      "Epoch: 2685/25000, Training Loss: 0.000744, lr : 0.008500\n",
      "Epoch: 2686/25000, Training Loss: 0.001063, lr : 0.008500\n",
      "Epoch: 2687/25000, Training Loss: 0.000922, lr : 0.008500\n",
      "Epoch: 2688/25000, Training Loss: 0.001362, lr : 0.008500\n",
      "Epoch: 2689/25000, Training Loss: 0.000932, lr : 0.008500\n",
      "Epoch: 2690/25000, Training Loss: 0.001108, lr : 0.008500\n",
      "Epoch: 2691/25000, Training Loss: 0.001026, lr : 0.008500\n",
      "Epoch: 2692/25000, Training Loss: 0.001044, lr : 0.008500\n",
      "Epoch: 2693/25000, Training Loss: 0.001060, lr : 0.008500\n",
      "Epoch: 2694/25000, Training Loss: 0.001208, lr : 0.008500\n",
      "Epoch: 2695/25000, Training Loss: 0.000912, lr : 0.008500\n",
      "Epoch: 2696/25000, Training Loss: 0.000923, lr : 0.008500\n",
      "Epoch: 2697/25000, Training Loss: 0.000897, lr : 0.008500\n",
      "Epoch: 2698/25000, Training Loss: 0.000907, lr : 0.008500\n",
      "Epoch: 2699/25000, Training Loss: 0.000863, lr : 0.008500\n",
      "Epoch: 2700/25000, Training Loss: 0.000943, lr : 0.008500\n",
      "Epoch: 2701/25000, Training Loss: 0.000787, lr : 0.008500\n",
      "Epoch: 2702/25000, Training Loss: 0.000871, lr : 0.008500\n",
      "Epoch: 2703/25000, Training Loss: 0.000890, lr : 0.008500\n",
      "Epoch: 2704/25000, Training Loss: 0.001088, lr : 0.008500\n",
      "Epoch: 2705/25000, Training Loss: 0.001283, lr : 0.008500\n",
      "Epoch: 2706/25000, Training Loss: 0.001225, lr : 0.008500\n",
      "Epoch: 2707/25000, Training Loss: 0.001003, lr : 0.008500\n",
      "Epoch: 2708/25000, Training Loss: 0.001041, lr : 0.008500\n",
      "Epoch: 2709/25000, Training Loss: 0.001034, lr : 0.008500\n",
      "Epoch: 2710/25000, Training Loss: 0.000971, lr : 0.008500\n",
      "Epoch: 2711/25000, Training Loss: 0.001185, lr : 0.008500\n",
      "Epoch: 2712/25000, Training Loss: 0.001433, lr : 0.008500\n",
      "Epoch: 2713/25000, Training Loss: 0.000893, lr : 0.008500\n",
      "Epoch: 2714/25000, Training Loss: 0.001135, lr : 0.008500\n",
      "Epoch: 2715/25000, Training Loss: 0.000956, lr : 0.008500\n",
      "Epoch: 2716/25000, Training Loss: 0.000978, lr : 0.008500\n",
      "Epoch: 2717/25000, Training Loss: 0.000805, lr : 0.008500\n",
      "Epoch: 2718/25000, Training Loss: 0.001330, lr : 0.008500\n",
      "Epoch: 2719/25000, Training Loss: 0.000841, lr : 0.008500\n",
      "Epoch: 2720/25000, Training Loss: 0.001084, lr : 0.008500\n",
      "Epoch: 2721/25000, Training Loss: 0.001030, lr : 0.008500\n",
      "Epoch: 2722/25000, Training Loss: 0.000971, lr : 0.008500\n",
      "Epoch: 2723/25000, Training Loss: 0.001083, lr : 0.008500\n",
      "Epoch: 2724/25000, Training Loss: 0.001133, lr : 0.008500\n",
      "Epoch: 2725/25000, Training Loss: 0.000931, lr : 0.008500\n",
      "Epoch: 2726/25000, Training Loss: 0.000787, lr : 0.008500\n",
      "Epoch: 2727/25000, Training Loss: 0.000921, lr : 0.008500\n",
      "Epoch: 2728/25000, Training Loss: 0.000954, lr : 0.008500\n",
      "Epoch: 2729/25000, Training Loss: 0.000655, lr : 0.008500\n",
      "Epoch: 2730/25000, Training Loss: 0.000891, lr : 0.008500\n",
      "Epoch: 2731/25000, Training Loss: 0.001010, lr : 0.008500\n",
      "Epoch: 2732/25000, Training Loss: 0.001170, lr : 0.008500\n",
      "Epoch: 2733/25000, Training Loss: 0.000672, lr : 0.008500\n",
      "Epoch: 2734/25000, Training Loss: 0.000686, lr : 0.008500\n",
      "Epoch: 2735/25000, Training Loss: 0.000761, lr : 0.008500\n",
      "Epoch: 2736/25000, Training Loss: 0.001006, lr : 0.008500\n",
      "Epoch: 2737/25000, Training Loss: 0.000996, lr : 0.008500\n",
      "Epoch: 2738/25000, Training Loss: 0.000909, lr : 0.008500\n",
      "Epoch: 2739/25000, Training Loss: 0.000821, lr : 0.008500\n",
      "Epoch: 2740/25000, Training Loss: 0.000863, lr : 0.008500\n",
      "Epoch: 2741/25000, Training Loss: 0.001089, lr : 0.008500\n",
      "Epoch: 2742/25000, Training Loss: 0.000853, lr : 0.008500\n",
      "Epoch: 2743/25000, Training Loss: 0.001030, lr : 0.008500\n",
      "Epoch: 2744/25000, Training Loss: 0.001083, lr : 0.008500\n",
      "Epoch: 2745/25000, Training Loss: 0.000938, lr : 0.008500\n",
      "Epoch: 2746/25000, Training Loss: 0.000845, lr : 0.008500\n",
      "Epoch: 2747/25000, Training Loss: 0.000677, lr : 0.008500\n",
      "Epoch: 2748/25000, Training Loss: 0.000988, lr : 0.008500\n",
      "Epoch: 2749/25000, Training Loss: 0.000848, lr : 0.008500\n",
      "Epoch: 2750/25000, Training Loss: 0.000946, lr : 0.008500\n",
      "Epoch: 2751/25000, Training Loss: 0.001145, lr : 0.008500\n",
      "Epoch: 2752/25000, Training Loss: 0.000699, lr : 0.008500\n",
      "Epoch: 2753/25000, Training Loss: 0.000821, lr : 0.008500\n",
      "Epoch: 2754/25000, Training Loss: 0.001076, lr : 0.008500\n",
      "Epoch: 2755/25000, Training Loss: 0.001070, lr : 0.008500\n",
      "Epoch: 2756/25000, Training Loss: 0.000838, lr : 0.008500\n",
      "Epoch: 2757/25000, Training Loss: 0.001148, lr : 0.008500\n",
      "Epoch: 2758/25000, Training Loss: 0.000973, lr : 0.008500\n",
      "Epoch: 2759/25000, Training Loss: 0.000911, lr : 0.008500\n",
      "Epoch: 2760/25000, Training Loss: 0.000904, lr : 0.008500\n",
      "Epoch: 2761/25000, Training Loss: 0.001119, lr : 0.008500\n",
      "Epoch: 2762/25000, Training Loss: 0.001142, lr : 0.008500\n",
      "Epoch: 2763/25000, Training Loss: 0.001079, lr : 0.008500\n",
      "Epoch: 2764/25000, Training Loss: 0.001094, lr : 0.008500\n",
      "Epoch: 2765/25000, Training Loss: 0.000784, lr : 0.008500\n",
      "Epoch: 2766/25000, Training Loss: 0.000975, lr : 0.008500\n",
      "Epoch: 2767/25000, Training Loss: 0.000937, lr : 0.008500\n",
      "Epoch: 2768/25000, Training Loss: 0.000663, lr : 0.008500\n",
      "Epoch: 2769/25000, Training Loss: 0.000864, lr : 0.008500\n",
      "Epoch: 2770/25000, Training Loss: 0.001084, lr : 0.008500\n",
      "Epoch: 2771/25000, Training Loss: 0.000962, lr : 0.008500\n",
      "Epoch: 2772/25000, Training Loss: 0.001113, lr : 0.008500\n",
      "Epoch: 2773/25000, Training Loss: 0.000636, lr : 0.008500\n",
      "Epoch: 2774/25000, Training Loss: 0.000793, lr : 0.008500\n",
      "Epoch: 2775/25000, Training Loss: 0.000758, lr : 0.008500\n",
      "Epoch: 2776/25000, Training Loss: 0.000863, lr : 0.008500\n",
      "Epoch: 2777/25000, Training Loss: 0.000804, lr : 0.008500\n",
      "Epoch: 2778/25000, Training Loss: 0.001243, lr : 0.008500\n",
      "Epoch: 2779/25000, Training Loss: 0.000921, lr : 0.008500\n",
      "Epoch: 2780/25000, Training Loss: 0.001105, lr : 0.008500\n",
      "Epoch: 2781/25000, Training Loss: 0.000718, lr : 0.008500\n",
      "Epoch: 2782/25000, Training Loss: 0.000963, lr : 0.008500\n",
      "Epoch: 2783/25000, Training Loss: 0.001251, lr : 0.008500\n",
      "Epoch: 2784/25000, Training Loss: 0.001239, lr : 0.008500\n",
      "Epoch: 2785/25000, Training Loss: 0.000863, lr : 0.008500\n",
      "Epoch: 2786/25000, Training Loss: 0.000927, lr : 0.008500\n",
      "Epoch: 2787/25000, Training Loss: 0.001039, lr : 0.008500\n",
      "Epoch: 2788/25000, Training Loss: 0.001039, lr : 0.008500\n",
      "Epoch: 2789/25000, Training Loss: 0.000909, lr : 0.008500\n",
      "Epoch: 2790/25000, Training Loss: 0.001178, lr : 0.008500\n",
      "Epoch: 2791/25000, Training Loss: 0.001302, lr : 0.008500\n",
      "Epoch: 2792/25000, Training Loss: 0.001131, lr : 0.008500\n",
      "Epoch: 2793/25000, Training Loss: 0.001166, lr : 0.008500\n",
      "Epoch: 2794/25000, Training Loss: 0.000970, lr : 0.008500\n",
      "Epoch: 2795/25000, Training Loss: 0.000607, lr : 0.008500\n",
      "Epoch: 2796/25000, Training Loss: 0.001175, lr : 0.008500\n",
      "Epoch: 2797/25000, Training Loss: 0.001002, lr : 0.008500\n",
      "Epoch: 2798/25000, Training Loss: 0.001105, lr : 0.008500\n",
      "Epoch: 2799/25000, Training Loss: 0.001052, lr : 0.008500\n",
      "Epoch: 2800/25000, Training Loss: 0.000946, lr : 0.008500\n",
      "Epoch: 2801/25000, Training Loss: 0.000809, lr : 0.008500\n",
      "Epoch: 2802/25000, Training Loss: 0.001155, lr : 0.008500\n",
      "Epoch: 2803/25000, Training Loss: 0.000794, lr : 0.008500\n",
      "Epoch: 2804/25000, Training Loss: 0.000912, lr : 0.008500\n",
      "Epoch: 2805/25000, Training Loss: 0.000602, lr : 0.008500\n",
      "Epoch: 2806/25000, Training Loss: 0.000838, lr : 0.008500\n",
      "Epoch: 2807/25000, Training Loss: 0.001108, lr : 0.008500\n",
      "Epoch: 2808/25000, Training Loss: 0.000735, lr : 0.008500\n",
      "Epoch: 2809/25000, Training Loss: 0.000726, lr : 0.008500\n",
      "Epoch: 2810/25000, Training Loss: 0.001080, lr : 0.008500\n",
      "Epoch: 2811/25000, Training Loss: 0.000711, lr : 0.008500\n",
      "Epoch: 2812/25000, Training Loss: 0.001135, lr : 0.008500\n",
      "Epoch: 2813/25000, Training Loss: 0.001036, lr : 0.008500\n",
      "Epoch: 2814/25000, Training Loss: 0.000979, lr : 0.008500\n",
      "Epoch: 2815/25000, Training Loss: 0.001082, lr : 0.008500\n",
      "Epoch: 2816/25000, Training Loss: 0.000968, lr : 0.008500\n",
      "Epoch: 2817/25000, Training Loss: 0.001205, lr : 0.008500\n",
      "Epoch: 2818/25000, Training Loss: 0.000989, lr : 0.008500\n",
      "Epoch: 2819/25000, Training Loss: 0.001263, lr : 0.008500\n",
      "Epoch: 2820/25000, Training Loss: 0.001018, lr : 0.008500\n",
      "Epoch: 2821/25000, Training Loss: 0.001174, lr : 0.008500\n",
      "Epoch: 2822/25000, Training Loss: 0.000886, lr : 0.008500\n",
      "Epoch: 2823/25000, Training Loss: 0.001116, lr : 0.008500\n",
      "Epoch: 2824/25000, Training Loss: 0.000891, lr : 0.008500\n",
      "Epoch: 2825/25000, Training Loss: 0.000950, lr : 0.008500\n",
      "Epoch: 2826/25000, Training Loss: 0.001051, lr : 0.008500\n",
      "Epoch: 2827/25000, Training Loss: 0.000726, lr : 0.008500\n",
      "Epoch: 2828/25000, Training Loss: 0.001008, lr : 0.008500\n",
      "Epoch: 2829/25000, Training Loss: 0.001001, lr : 0.008500\n",
      "Epoch: 2830/25000, Training Loss: 0.000907, lr : 0.008500\n",
      "Epoch: 2831/25000, Training Loss: 0.001059, lr : 0.008500\n",
      "Epoch: 2832/25000, Training Loss: 0.000915, lr : 0.008500\n",
      "Epoch: 2833/25000, Training Loss: 0.000926, lr : 0.008500\n",
      "Epoch: 2834/25000, Training Loss: 0.001061, lr : 0.008500\n",
      "Epoch: 2835/25000, Training Loss: 0.000965, lr : 0.008500\n",
      "Epoch: 2836/25000, Training Loss: 0.000885, lr : 0.008500\n",
      "Epoch: 2837/25000, Training Loss: 0.000778, lr : 0.008500\n",
      "Epoch: 2838/25000, Training Loss: 0.001060, lr : 0.008500\n",
      "Epoch: 2839/25000, Training Loss: 0.000779, lr : 0.008500\n",
      "Epoch: 2840/25000, Training Loss: 0.001131, lr : 0.008500\n",
      "Epoch: 2841/25000, Training Loss: 0.000724, lr : 0.008500\n",
      "Epoch: 2842/25000, Training Loss: 0.000867, lr : 0.008500\n",
      "Epoch: 2843/25000, Training Loss: 0.001250, lr : 0.008500\n",
      "Epoch: 2844/25000, Training Loss: 0.000937, lr : 0.008500\n",
      "Epoch: 2845/25000, Training Loss: 0.000971, lr : 0.008500\n",
      "Epoch: 2846/25000, Training Loss: 0.000950, lr : 0.008500\n",
      "Epoch: 2847/25000, Training Loss: 0.001145, lr : 0.008500\n",
      "Epoch: 2848/25000, Training Loss: 0.001134, lr : 0.008500\n",
      "Epoch: 2849/25000, Training Loss: 0.000910, lr : 0.008500\n",
      "Epoch: 2850/25000, Training Loss: 0.001076, lr : 0.008500\n",
      "Epoch: 2851/25000, Training Loss: 0.000894, lr : 0.008500\n",
      "Epoch: 2852/25000, Training Loss: 0.001046, lr : 0.008500\n",
      "Epoch: 2853/25000, Training Loss: 0.001105, lr : 0.008500\n",
      "Epoch: 2854/25000, Training Loss: 0.000964, lr : 0.008500\n",
      "Epoch: 2855/25000, Training Loss: 0.000830, lr : 0.008500\n",
      "Epoch: 2856/25000, Training Loss: 0.000978, lr : 0.008500\n",
      "Epoch: 2857/25000, Training Loss: 0.001145, lr : 0.008500\n",
      "Epoch: 2858/25000, Training Loss: 0.000815, lr : 0.008500\n",
      "Epoch: 2859/25000, Training Loss: 0.000784, lr : 0.008500\n",
      "Epoch: 2860/25000, Training Loss: 0.000801, lr : 0.008500\n",
      "Epoch: 2861/25000, Training Loss: 0.000727, lr : 0.008500\n",
      "Epoch: 2862/25000, Training Loss: 0.000987, lr : 0.008500\n",
      "Epoch: 2863/25000, Training Loss: 0.000922, lr : 0.008500\n",
      "Epoch: 2864/25000, Training Loss: 0.000992, lr : 0.008500\n",
      "Epoch: 2865/25000, Training Loss: 0.001049, lr : 0.008500\n",
      "Epoch: 2866/25000, Training Loss: 0.000776, lr : 0.008500\n",
      "Epoch: 2867/25000, Training Loss: 0.001110, lr : 0.008500\n",
      "Epoch: 2868/25000, Training Loss: 0.000958, lr : 0.008500\n",
      "Epoch: 2869/25000, Training Loss: 0.000892, lr : 0.008500\n",
      "Epoch: 2870/25000, Training Loss: 0.001020, lr : 0.008500\n",
      "Epoch: 2871/25000, Training Loss: 0.000826, lr : 0.008500\n",
      "Epoch: 2872/25000, Training Loss: 0.000925, lr : 0.008500\n",
      "Epoch: 2873/25000, Training Loss: 0.000933, lr : 0.008500\n",
      "Epoch: 2874/25000, Training Loss: 0.000801, lr : 0.008500\n",
      "Epoch: 2875/25000, Training Loss: 0.001005, lr : 0.008500\n",
      "Epoch: 2876/25000, Training Loss: 0.000803, lr : 0.008500\n",
      "Epoch: 2877/25000, Training Loss: 0.001015, lr : 0.008500\n",
      "Epoch: 2878/25000, Training Loss: 0.001002, lr : 0.008500\n",
      "Epoch: 2879/25000, Training Loss: 0.001091, lr : 0.008500\n",
      "Epoch: 2880/25000, Training Loss: 0.000995, lr : 0.008500\n",
      "Epoch: 2881/25000, Training Loss: 0.000945, lr : 0.008500\n",
      "Epoch: 2882/25000, Training Loss: 0.000768, lr : 0.008500\n",
      "Epoch: 2883/25000, Training Loss: 0.000954, lr : 0.008500\n",
      "Epoch: 2884/25000, Training Loss: 0.000775, lr : 0.008500\n",
      "Epoch: 2885/25000, Training Loss: 0.000638, lr : 0.008500\n",
      "Epoch: 2886/25000, Training Loss: 0.000773, lr : 0.008500\n",
      "Epoch: 2887/25000, Training Loss: 0.000733, lr : 0.008500\n",
      "Epoch: 2888/25000, Training Loss: 0.000888, lr : 0.008500\n",
      "Epoch: 2889/25000, Training Loss: 0.000883, lr : 0.008500\n",
      "Epoch: 2890/25000, Training Loss: 0.000981, lr : 0.008500\n",
      "Epoch: 2891/25000, Training Loss: 0.000807, lr : 0.008500\n",
      "Epoch: 2892/25000, Training Loss: 0.000880, lr : 0.008500\n",
      "Epoch: 2893/25000, Training Loss: 0.000893, lr : 0.008500\n",
      "Epoch: 2894/25000, Training Loss: 0.001096, lr : 0.008500\n",
      "Epoch: 2895/25000, Training Loss: 0.001059, lr : 0.008500\n",
      "Epoch: 2896/25000, Training Loss: 0.000891, lr : 0.008500\n",
      "Epoch: 2897/25000, Training Loss: 0.000978, lr : 0.008500\n",
      "Epoch: 2898/25000, Training Loss: 0.000724, lr : 0.008500\n",
      "Epoch: 2899/25000, Training Loss: 0.000968, lr : 0.008500\n",
      "Epoch: 2900/25000, Training Loss: 0.000862, lr : 0.008500\n",
      "Epoch: 2901/25000, Training Loss: 0.000944, lr : 0.008500\n",
      "Epoch: 2902/25000, Training Loss: 0.001031, lr : 0.008500\n",
      "Epoch: 2903/25000, Training Loss: 0.000919, lr : 0.008500\n",
      "Epoch: 2904/25000, Training Loss: 0.000870, lr : 0.008500\n",
      "Epoch: 2905/25000, Training Loss: 0.000976, lr : 0.008500\n",
      "Epoch: 2906/25000, Training Loss: 0.001109, lr : 0.008500\n",
      "Epoch: 2907/25000, Training Loss: 0.001151, lr : 0.008500\n",
      "Epoch: 2908/25000, Training Loss: 0.001164, lr : 0.008500\n",
      "Epoch: 2909/25000, Training Loss: 0.000936, lr : 0.008500\n",
      "Epoch: 2910/25000, Training Loss: 0.000962, lr : 0.008500\n",
      "Epoch: 2911/25000, Training Loss: 0.001070, lr : 0.008500\n",
      "Epoch: 2912/25000, Training Loss: 0.000962, lr : 0.008500\n",
      "Epoch: 2913/25000, Training Loss: 0.001231, lr : 0.008500\n",
      "Epoch: 2914/25000, Training Loss: 0.000990, lr : 0.008500\n",
      "Epoch: 2915/25000, Training Loss: 0.001022, lr : 0.008500\n",
      "Epoch: 2916/25000, Training Loss: 0.001008, lr : 0.008500\n",
      "Epoch: 2917/25000, Training Loss: 0.000960, lr : 0.008500\n",
      "Epoch: 2918/25000, Training Loss: 0.001005, lr : 0.008500\n",
      "Epoch: 2919/25000, Training Loss: 0.000994, lr : 0.008500\n",
      "Epoch: 2920/25000, Training Loss: 0.001055, lr : 0.008500\n",
      "Epoch: 2921/25000, Training Loss: 0.000982, lr : 0.008500\n",
      "Epoch: 2922/25000, Training Loss: 0.000929, lr : 0.008500\n",
      "Epoch: 2923/25000, Training Loss: 0.001047, lr : 0.008500\n",
      "Epoch: 2924/25000, Training Loss: 0.001021, lr : 0.008500\n",
      "Epoch: 2925/25000, Training Loss: 0.001006, lr : 0.008500\n",
      "Epoch: 2926/25000, Training Loss: 0.001078, lr : 0.008500\n",
      "Epoch: 2927/25000, Training Loss: 0.000923, lr : 0.008500\n",
      "Epoch: 2928/25000, Training Loss: 0.001054, lr : 0.008500\n",
      "Epoch: 2929/25000, Training Loss: 0.001135, lr : 0.008500\n",
      "Epoch: 2930/25000, Training Loss: 0.000878, lr : 0.008500\n",
      "Epoch: 2931/25000, Training Loss: 0.001135, lr : 0.008500\n",
      "Epoch: 2932/25000, Training Loss: 0.000758, lr : 0.008500\n",
      "Epoch: 2933/25000, Training Loss: 0.000782, lr : 0.008500\n",
      "Epoch: 2934/25000, Training Loss: 0.001161, lr : 0.008500\n",
      "Epoch: 2935/25000, Training Loss: 0.000867, lr : 0.008500\n",
      "Epoch: 2936/25000, Training Loss: 0.000870, lr : 0.008500\n",
      "Epoch: 2937/25000, Training Loss: 0.000841, lr : 0.008500\n",
      "Epoch: 2938/25000, Training Loss: 0.000868, lr : 0.008500\n",
      "Epoch: 2939/25000, Training Loss: 0.000946, lr : 0.008500\n",
      "Epoch: 2940/25000, Training Loss: 0.000674, lr : 0.008500\n",
      "Epoch: 2941/25000, Training Loss: 0.000958, lr : 0.008500\n",
      "Epoch: 2942/25000, Training Loss: 0.000708, lr : 0.008500\n",
      "Epoch: 2943/25000, Training Loss: 0.000798, lr : 0.008500\n",
      "Epoch: 2944/25000, Training Loss: 0.000790, lr : 0.008500\n",
      "Epoch: 2945/25000, Training Loss: 0.000847, lr : 0.008500\n",
      "Epoch: 2946/25000, Training Loss: 0.000709, lr : 0.008500\n",
      "Epoch: 2947/25000, Training Loss: 0.000726, lr : 0.008500\n",
      "Epoch: 2948/25000, Training Loss: 0.001025, lr : 0.008500\n",
      "Epoch: 2949/25000, Training Loss: 0.000579, lr : 0.008500\n",
      "Epoch: 2950/25000, Training Loss: 0.000929, lr : 0.008500\n",
      "Epoch: 2951/25000, Training Loss: 0.000771, lr : 0.008500\n",
      "Epoch: 2952/25000, Training Loss: 0.001221, lr : 0.008500\n",
      "Epoch: 2953/25000, Training Loss: 0.000792, lr : 0.008500\n",
      "Epoch: 2954/25000, Training Loss: 0.000820, lr : 0.008500\n",
      "Epoch: 2955/25000, Training Loss: 0.000938, lr : 0.008500\n",
      "Epoch: 2956/25000, Training Loss: 0.001067, lr : 0.008500\n",
      "Epoch: 2957/25000, Training Loss: 0.001047, lr : 0.008500\n",
      "Epoch: 2958/25000, Training Loss: 0.000786, lr : 0.008500\n",
      "Epoch: 2959/25000, Training Loss: 0.000604, lr : 0.008500\n",
      "Epoch: 2960/25000, Training Loss: 0.000918, lr : 0.008500\n",
      "Epoch: 2961/25000, Training Loss: 0.000964, lr : 0.008500\n",
      "Epoch: 2962/25000, Training Loss: 0.000827, lr : 0.008500\n",
      "Epoch: 2963/25000, Training Loss: 0.000943, lr : 0.008500\n",
      "Epoch: 2964/25000, Training Loss: 0.001135, lr : 0.008500\n",
      "Epoch: 2965/25000, Training Loss: 0.001196, lr : 0.008500\n",
      "Epoch: 2966/25000, Training Loss: 0.001068, lr : 0.008500\n",
      "Epoch: 2967/25000, Training Loss: 0.001077, lr : 0.008500\n",
      "Epoch: 2968/25000, Training Loss: 0.001065, lr : 0.008500\n",
      "Epoch: 2969/25000, Training Loss: 0.001061, lr : 0.008500\n",
      "Epoch: 2970/25000, Training Loss: 0.000952, lr : 0.008500\n",
      "Epoch: 2971/25000, Training Loss: 0.000926, lr : 0.008500\n",
      "Epoch: 2972/25000, Training Loss: 0.001023, lr : 0.008500\n",
      "Epoch: 2973/25000, Training Loss: 0.001058, lr : 0.008500\n",
      "Epoch: 2974/25000, Training Loss: 0.000791, lr : 0.008500\n",
      "Epoch: 2975/25000, Training Loss: 0.001082, lr : 0.008500\n",
      "Epoch: 2976/25000, Training Loss: 0.001023, lr : 0.008500\n",
      "Epoch: 2977/25000, Training Loss: 0.000882, lr : 0.008500\n",
      "Epoch: 2978/25000, Training Loss: 0.000873, lr : 0.008500\n",
      "Epoch: 2979/25000, Training Loss: 0.000811, lr : 0.008500\n",
      "Epoch: 2980/25000, Training Loss: 0.000800, lr : 0.008500\n",
      "Epoch: 2981/25000, Training Loss: 0.000663, lr : 0.008500\n",
      "Epoch: 2982/25000, Training Loss: 0.000980, lr : 0.008500\n",
      "Epoch: 2983/25000, Training Loss: 0.000930, lr : 0.008500\n",
      "Epoch: 2984/25000, Training Loss: 0.001002, lr : 0.008500\n",
      "Epoch: 2985/25000, Training Loss: 0.001007, lr : 0.008500\n",
      "Epoch: 2986/25000, Training Loss: 0.000777, lr : 0.008500\n",
      "Epoch: 2987/25000, Training Loss: 0.000923, lr : 0.008500\n",
      "Epoch: 2988/25000, Training Loss: 0.000896, lr : 0.008500\n",
      "Epoch: 2989/25000, Training Loss: 0.000857, lr : 0.008500\n",
      "Epoch: 2990/25000, Training Loss: 0.001183, lr : 0.008500\n",
      "Epoch: 2991/25000, Training Loss: 0.000754, lr : 0.008500\n",
      "Epoch: 2992/25000, Training Loss: 0.000826, lr : 0.008500\n",
      "Epoch: 2993/25000, Training Loss: 0.000765, lr : 0.008500\n",
      "Epoch: 2994/25000, Training Loss: 0.000995, lr : 0.008500\n",
      "Epoch: 2995/25000, Training Loss: 0.000797, lr : 0.008500\n",
      "Epoch: 2996/25000, Training Loss: 0.001192, lr : 0.008500\n",
      "Epoch: 2997/25000, Training Loss: 0.000824, lr : 0.008500\n",
      "Epoch: 2998/25000, Training Loss: 0.000694, lr : 0.008500\n",
      "Epoch: 2999/25000, Training Loss: 0.001083, lr : 0.008500\n",
      "Epoch: 3000/25000, Training Loss: 0.001039, lr : 0.008500\n",
      "Epoch: 3001/25000, Training Loss: 0.001046, lr : 0.008500\n",
      "Epoch: 3002/25000, Training Loss: 0.000883, lr : 0.008500\n",
      "Epoch: 3003/25000, Training Loss: 0.001265, lr : 0.008500\n",
      "Epoch: 3004/25000, Training Loss: 0.001244, lr : 0.008500\n",
      "Epoch: 3005/25000, Training Loss: 0.001090, lr : 0.008500\n",
      "Epoch: 3006/25000, Training Loss: 0.001112, lr : 0.008500\n",
      "Epoch: 3007/25000, Training Loss: 0.000685, lr : 0.008500\n",
      "Epoch: 3008/25000, Training Loss: 0.001034, lr : 0.008500\n",
      "Epoch: 3009/25000, Training Loss: 0.000884, lr : 0.008500\n",
      "Epoch: 3010/25000, Training Loss: 0.000937, lr : 0.008500\n",
      "Epoch: 3011/25000, Training Loss: 0.000840, lr : 0.008500\n",
      "Epoch: 3012/25000, Training Loss: 0.000979, lr : 0.008500\n",
      "Epoch: 3013/25000, Training Loss: 0.000904, lr : 0.008500\n",
      "Epoch: 3014/25000, Training Loss: 0.000795, lr : 0.008500\n",
      "Epoch: 3015/25000, Training Loss: 0.000728, lr : 0.008500\n",
      "Epoch: 3016/25000, Training Loss: 0.000911, lr : 0.008500\n",
      "Epoch: 3017/25000, Training Loss: 0.000957, lr : 0.008500\n",
      "Epoch: 3018/25000, Training Loss: 0.000706, lr : 0.008500\n",
      "Epoch: 3019/25000, Training Loss: 0.001156, lr : 0.008500\n",
      "Epoch: 3020/25000, Training Loss: 0.000964, lr : 0.008500\n",
      "Epoch: 3021/25000, Training Loss: 0.000947, lr : 0.008500\n",
      "Epoch: 3022/25000, Training Loss: 0.000941, lr : 0.008500\n",
      "Epoch: 3023/25000, Training Loss: 0.001027, lr : 0.008500\n",
      "Epoch: 3024/25000, Training Loss: 0.000734, lr : 0.008500\n",
      "Epoch: 3025/25000, Training Loss: 0.001182, lr : 0.008500\n",
      "Epoch: 3026/25000, Training Loss: 0.001208, lr : 0.008500\n",
      "Epoch: 3027/25000, Training Loss: 0.000748, lr : 0.008500\n",
      "Epoch: 3028/25000, Training Loss: 0.001182, lr : 0.008500\n",
      "Epoch: 3029/25000, Training Loss: 0.000869, lr : 0.008500\n",
      "Epoch: 3030/25000, Training Loss: 0.000857, lr : 0.008500\n",
      "Epoch: 3031/25000, Training Loss: 0.000712, lr : 0.008500\n",
      "Epoch: 3032/25000, Training Loss: 0.000886, lr : 0.008500\n",
      "Epoch: 3033/25000, Training Loss: 0.000822, lr : 0.008500\n",
      "Epoch: 3034/25000, Training Loss: 0.000849, lr : 0.008500\n",
      "Epoch: 3035/25000, Training Loss: 0.000907, lr : 0.008500\n",
      "Epoch: 3036/25000, Training Loss: 0.000785, lr : 0.008500\n",
      "Epoch: 3037/25000, Training Loss: 0.000900, lr : 0.008500\n",
      "Epoch: 3038/25000, Training Loss: 0.000848, lr : 0.008500\n",
      "Epoch: 3039/25000, Training Loss: 0.000772, lr : 0.008500\n",
      "Epoch: 3040/25000, Training Loss: 0.000810, lr : 0.008500\n",
      "Epoch: 3041/25000, Training Loss: 0.000791, lr : 0.008500\n",
      "Epoch: 3042/25000, Training Loss: 0.000822, lr : 0.008500\n",
      "Epoch: 3043/25000, Training Loss: 0.000829, lr : 0.008500\n",
      "Epoch: 3044/25000, Training Loss: 0.001093, lr : 0.008500\n",
      "Epoch: 3045/25000, Training Loss: 0.001085, lr : 0.008500\n",
      "Epoch: 3046/25000, Training Loss: 0.000769, lr : 0.008500\n",
      "Epoch: 3047/25000, Training Loss: 0.000831, lr : 0.008500\n",
      "Epoch: 3048/25000, Training Loss: 0.000771, lr : 0.008500\n",
      "Epoch: 3049/25000, Training Loss: 0.000962, lr : 0.008500\n",
      "Epoch: 3050/25000, Training Loss: 0.000845, lr : 0.008500\n",
      "Epoch: 3051/25000, Training Loss: 0.000955, lr : 0.008500\n",
      "Epoch: 3052/25000, Training Loss: 0.000823, lr : 0.008500\n",
      "Epoch: 3053/25000, Training Loss: 0.001095, lr : 0.008500\n",
      "Epoch: 3054/25000, Training Loss: 0.000939, lr : 0.008500\n",
      "Epoch: 3055/25000, Training Loss: 0.001041, lr : 0.008500\n",
      "Epoch: 3056/25000, Training Loss: 0.000959, lr : 0.008500\n",
      "Epoch: 3057/25000, Training Loss: 0.000984, lr : 0.008500\n",
      "Epoch: 3058/25000, Training Loss: 0.000648, lr : 0.008500\n",
      "Epoch: 3059/25000, Training Loss: 0.000812, lr : 0.008500\n",
      "Epoch: 3060/25000, Training Loss: 0.000561, lr : 0.008500\n",
      "Epoch: 3061/25000, Training Loss: 0.000763, lr : 0.008500\n",
      "Epoch: 3062/25000, Training Loss: 0.000649, lr : 0.008500\n",
      "Epoch: 3063/25000, Training Loss: 0.000842, lr : 0.008500\n",
      "Epoch: 3064/25000, Training Loss: 0.000958, lr : 0.008500\n",
      "Epoch: 3065/25000, Training Loss: 0.000938, lr : 0.008500\n",
      "Epoch: 3066/25000, Training Loss: 0.001002, lr : 0.008500\n",
      "Epoch: 3067/25000, Training Loss: 0.000840, lr : 0.008500\n",
      "Epoch: 3068/25000, Training Loss: 0.000794, lr : 0.008500\n",
      "Epoch: 3069/25000, Training Loss: 0.000836, lr : 0.008500\n",
      "Epoch: 3070/25000, Training Loss: 0.000931, lr : 0.008500\n",
      "Epoch: 3071/25000, Training Loss: 0.000910, lr : 0.008500\n",
      "Epoch: 3072/25000, Training Loss: 0.000768, lr : 0.008500\n",
      "Epoch: 3073/25000, Training Loss: 0.001053, lr : 0.008500\n",
      "Epoch: 3074/25000, Training Loss: 0.000656, lr : 0.008500\n",
      "Epoch: 3075/25000, Training Loss: 0.000552, lr : 0.008500\n",
      "Epoch: 3076/25000, Training Loss: 0.000658, lr : 0.008500\n",
      "Epoch: 3077/25000, Training Loss: 0.000671, lr : 0.008500\n",
      "Epoch: 3078/25000, Training Loss: 0.000802, lr : 0.008500\n",
      "Epoch: 3079/25000, Training Loss: 0.000641, lr : 0.008500\n",
      "Epoch: 3080/25000, Training Loss: 0.000772, lr : 0.008500\n",
      "Epoch: 3081/25000, Training Loss: 0.001245, lr : 0.008500\n",
      "Epoch: 3082/25000, Training Loss: 0.000779, lr : 0.008500\n",
      "Epoch: 3083/25000, Training Loss: 0.001052, lr : 0.008500\n",
      "Epoch: 3084/25000, Training Loss: 0.000972, lr : 0.008500\n",
      "Epoch: 3085/25000, Training Loss: 0.000974, lr : 0.008500\n",
      "Epoch: 3086/25000, Training Loss: 0.000716, lr : 0.008500\n",
      "Epoch: 3087/25000, Training Loss: 0.000947, lr : 0.008500\n",
      "Epoch: 3088/25000, Training Loss: 0.001182, lr : 0.008500\n",
      "Epoch: 3089/25000, Training Loss: 0.000929, lr : 0.008500\n",
      "Epoch: 3090/25000, Training Loss: 0.001111, lr : 0.008500\n",
      "Epoch: 3091/25000, Training Loss: 0.000744, lr : 0.008500\n",
      "Epoch: 3092/25000, Training Loss: 0.001245, lr : 0.008500\n",
      "Epoch: 3093/25000, Training Loss: 0.000994, lr : 0.008500\n",
      "Epoch: 3094/25000, Training Loss: 0.000900, lr : 0.008500\n",
      "Epoch: 3095/25000, Training Loss: 0.000828, lr : 0.008500\n",
      "Epoch: 3096/25000, Training Loss: 0.000861, lr : 0.008500\n",
      "Epoch: 3097/25000, Training Loss: 0.000742, lr : 0.008500\n",
      "Epoch: 3098/25000, Training Loss: 0.000934, lr : 0.008500\n",
      "Epoch: 3099/25000, Training Loss: 0.000861, lr : 0.008500\n",
      "Epoch: 3100/25000, Training Loss: 0.000712, lr : 0.008500\n",
      "Epoch: 3101/25000, Training Loss: 0.000750, lr : 0.008500\n",
      "Epoch: 3102/25000, Training Loss: 0.001216, lr : 0.008500\n",
      "Epoch: 3103/25000, Training Loss: 0.000705, lr : 0.008500\n",
      "Epoch: 3104/25000, Training Loss: 0.000810, lr : 0.008500\n",
      "Epoch: 3105/25000, Training Loss: 0.000851, lr : 0.008500\n",
      "Epoch: 3106/25000, Training Loss: 0.000831, lr : 0.008500\n",
      "Epoch: 3107/25000, Training Loss: 0.001222, lr : 0.008500\n",
      "Epoch: 3108/25000, Training Loss: 0.000711, lr : 0.008500\n",
      "Epoch: 3109/25000, Training Loss: 0.000942, lr : 0.008500\n",
      "Epoch: 3110/25000, Training Loss: 0.000962, lr : 0.008500\n",
      "Epoch: 3111/25000, Training Loss: 0.000615, lr : 0.008500\n",
      "Epoch: 3112/25000, Training Loss: 0.001178, lr : 0.008500\n",
      "Epoch: 3113/25000, Training Loss: 0.000836, lr : 0.008500\n",
      "Epoch: 3114/25000, Training Loss: 0.000708, lr : 0.008500\n",
      "Epoch: 3115/25000, Training Loss: 0.000960, lr : 0.008500\n",
      "Epoch: 3116/25000, Training Loss: 0.000665, lr : 0.008500\n",
      "Epoch: 3117/25000, Training Loss: 0.000790, lr : 0.008500\n",
      "Epoch: 3118/25000, Training Loss: 0.000791, lr : 0.008500\n",
      "Epoch: 3119/25000, Training Loss: 0.000826, lr : 0.008500\n",
      "Epoch: 3120/25000, Training Loss: 0.000739, lr : 0.008500\n",
      "Epoch: 3121/25000, Training Loss: 0.000948, lr : 0.008500\n",
      "Epoch: 3122/25000, Training Loss: 0.000892, lr : 0.008500\n",
      "Epoch: 3123/25000, Training Loss: 0.000912, lr : 0.008500\n",
      "Epoch: 3124/25000, Training Loss: 0.000808, lr : 0.008500\n",
      "Epoch: 3125/25000, Training Loss: 0.000782, lr : 0.008500\n",
      "Epoch: 3126/25000, Training Loss: 0.000761, lr : 0.008500\n",
      "Epoch: 3127/25000, Training Loss: 0.000716, lr : 0.008500\n",
      "Epoch: 3128/25000, Training Loss: 0.000757, lr : 0.008500\n",
      "Epoch: 3129/25000, Training Loss: 0.000900, lr : 0.008500\n",
      "Epoch: 3130/25000, Training Loss: 0.000736, lr : 0.008500\n",
      "Epoch: 3131/25000, Training Loss: 0.000944, lr : 0.008500\n",
      "Epoch: 3132/25000, Training Loss: 0.000997, lr : 0.008500\n",
      "Epoch: 3133/25000, Training Loss: 0.000759, lr : 0.008500\n",
      "Epoch: 3134/25000, Training Loss: 0.001023, lr : 0.008500\n",
      "Epoch: 3135/25000, Training Loss: 0.000682, lr : 0.008500\n",
      "Epoch: 3136/25000, Training Loss: 0.000832, lr : 0.008500\n",
      "Epoch: 3137/25000, Training Loss: 0.000845, lr : 0.008500\n",
      "Epoch: 3138/25000, Training Loss: 0.000992, lr : 0.008500\n",
      "Epoch: 3139/25000, Training Loss: 0.000928, lr : 0.008500\n",
      "Epoch: 3140/25000, Training Loss: 0.000495, lr : 0.008500\n",
      "Epoch: 3141/25000, Training Loss: 0.000628, lr : 0.008500\n",
      "Epoch: 3142/25000, Training Loss: 0.000665, lr : 0.008500\n",
      "Epoch: 3143/25000, Training Loss: 0.000605, lr : 0.008500\n",
      "Epoch: 3144/25000, Training Loss: 0.000818, lr : 0.008500\n",
      "Epoch: 3145/25000, Training Loss: 0.000655, lr : 0.008500\n",
      "Epoch: 3146/25000, Training Loss: 0.000629, lr : 0.008500\n",
      "Epoch: 3147/25000, Training Loss: 0.000554, lr : 0.008500\n",
      "Epoch: 3148/25000, Training Loss: 0.000506, lr : 0.008500\n",
      "Epoch: 3149/25000, Training Loss: 0.000637, lr : 0.008500\n",
      "Epoch: 3150/25000, Training Loss: 0.000530, lr : 0.008500\n",
      "Epoch: 3151/25000, Training Loss: 0.000528, lr : 0.008500\n",
      "Epoch: 3152/25000, Training Loss: 0.000657, lr : 0.008500\n",
      "Epoch: 3153/25000, Training Loss: 0.000705, lr : 0.008500\n",
      "Epoch: 3154/25000, Training Loss: 0.000779, lr : 0.008500\n",
      "Epoch: 3155/25000, Training Loss: 0.000741, lr : 0.008500\n",
      "Epoch: 3156/25000, Training Loss: 0.000543, lr : 0.008500\n",
      "Epoch: 3157/25000, Training Loss: 0.000825, lr : 0.008500\n",
      "Epoch: 3158/25000, Training Loss: 0.000582, lr : 0.008500\n",
      "Epoch: 3159/25000, Training Loss: 0.000646, lr : 0.008500\n",
      "Epoch: 3160/25000, Training Loss: 0.000700, lr : 0.008500\n",
      "Epoch: 3161/25000, Training Loss: 0.000445, lr : 0.008500\n",
      "Epoch: 3162/25000, Training Loss: 0.000599, lr : 0.008500\n",
      "Epoch: 3163/25000, Training Loss: 0.000747, lr : 0.008500\n",
      "Epoch: 3164/25000, Training Loss: 0.000840, lr : 0.008500\n",
      "Epoch: 3165/25000, Training Loss: 0.000789, lr : 0.008500\n",
      "Epoch: 3166/25000, Training Loss: 0.000558, lr : 0.008500\n",
      "Epoch: 3167/25000, Training Loss: 0.000520, lr : 0.008500\n",
      "Epoch: 3168/25000, Training Loss: 0.000578, lr : 0.008500\n",
      "Epoch: 3169/25000, Training Loss: 0.000760, lr : 0.008500\n",
      "Epoch: 3170/25000, Training Loss: 0.000666, lr : 0.008500\n",
      "Epoch: 3171/25000, Training Loss: 0.000639, lr : 0.008500\n",
      "Epoch: 3172/25000, Training Loss: 0.000744, lr : 0.008500\n",
      "Epoch: 3173/25000, Training Loss: 0.000715, lr : 0.008500\n",
      "Epoch: 3174/25000, Training Loss: 0.000813, lr : 0.008500\n",
      "Epoch: 3175/25000, Training Loss: 0.000920, lr : 0.008500\n",
      "Epoch: 3176/25000, Training Loss: 0.000571, lr : 0.008500\n",
      "Epoch: 3177/25000, Training Loss: 0.000867, lr : 0.008500\n",
      "Epoch: 3178/25000, Training Loss: 0.000642, lr : 0.008500\n",
      "Epoch: 3179/25000, Training Loss: 0.000594, lr : 0.008500\n",
      "Epoch: 3180/25000, Training Loss: 0.000626, lr : 0.008500\n",
      "Epoch: 3181/25000, Training Loss: 0.000474, lr : 0.008500\n",
      "Epoch: 3182/25000, Training Loss: 0.000784, lr : 0.008500\n",
      "Epoch: 3183/25000, Training Loss: 0.000555, lr : 0.008500\n",
      "Epoch: 3184/25000, Training Loss: 0.000835, lr : 0.008500\n",
      "Epoch: 3185/25000, Training Loss: 0.000611, lr : 0.008500\n",
      "Epoch: 3186/25000, Training Loss: 0.000662, lr : 0.008500\n",
      "Epoch: 3187/25000, Training Loss: 0.000855, lr : 0.008500\n",
      "Epoch: 3188/25000, Training Loss: 0.000808, lr : 0.008500\n",
      "Epoch: 3189/25000, Training Loss: 0.000658, lr : 0.008500\n",
      "Epoch: 3190/25000, Training Loss: 0.000570, lr : 0.008500\n",
      "Epoch: 3191/25000, Training Loss: 0.000539, lr : 0.008500\n",
      "Epoch: 3192/25000, Training Loss: 0.000578, lr : 0.008500\n",
      "Epoch: 3193/25000, Training Loss: 0.000435, lr : 0.008500\n",
      "Epoch: 3194/25000, Training Loss: 0.000461, lr : 0.008500\n",
      "Epoch: 3195/25000, Training Loss: 0.000752, lr : 0.008500\n",
      "Epoch: 3196/25000, Training Loss: 0.000738, lr : 0.008500\n",
      "Epoch: 3197/25000, Training Loss: 0.000548, lr : 0.008500\n",
      "Epoch: 3198/25000, Training Loss: 0.000722, lr : 0.008500\n",
      "Epoch: 3199/25000, Training Loss: 0.000850, lr : 0.008500\n",
      "Epoch: 3200/25000, Training Loss: 0.000823, lr : 0.008500\n",
      "Epoch: 3201/25000, Training Loss: 0.000813, lr : 0.008500\n",
      "Epoch: 3202/25000, Training Loss: 0.000735, lr : 0.008500\n",
      "Epoch: 3203/25000, Training Loss: 0.000769, lr : 0.008500\n",
      "Epoch: 3204/25000, Training Loss: 0.000804, lr : 0.008500\n",
      "Epoch: 3205/25000, Training Loss: 0.000721, lr : 0.008500\n",
      "Epoch: 3206/25000, Training Loss: 0.000611, lr : 0.008500\n",
      "Epoch: 3207/25000, Training Loss: 0.000596, lr : 0.008500\n",
      "Epoch: 3208/25000, Training Loss: 0.000660, lr : 0.008500\n",
      "Epoch: 3209/25000, Training Loss: 0.000507, lr : 0.008500\n",
      "Epoch: 3210/25000, Training Loss: 0.000659, lr : 0.008500\n",
      "Epoch: 3211/25000, Training Loss: 0.000674, lr : 0.008500\n",
      "Epoch: 3212/25000, Training Loss: 0.000536, lr : 0.008500\n",
      "Epoch: 3213/25000, Training Loss: 0.000805, lr : 0.008500\n",
      "Epoch: 3214/25000, Training Loss: 0.000786, lr : 0.008500\n",
      "Epoch: 3215/25000, Training Loss: 0.000448, lr : 0.008500\n",
      "Epoch: 3216/25000, Training Loss: 0.000739, lr : 0.008500\n",
      "Epoch: 3217/25000, Training Loss: 0.000682, lr : 0.008500\n",
      "Epoch: 3218/25000, Training Loss: 0.000734, lr : 0.008500\n",
      "Epoch: 3219/25000, Training Loss: 0.000846, lr : 0.008500\n",
      "Epoch: 3220/25000, Training Loss: 0.000608, lr : 0.008500\n",
      "Epoch: 3221/25000, Training Loss: 0.000910, lr : 0.008500\n",
      "Epoch: 3222/25000, Training Loss: 0.000922, lr : 0.008500\n",
      "Epoch: 3223/25000, Training Loss: 0.000757, lr : 0.008500\n",
      "Epoch: 3224/25000, Training Loss: 0.000661, lr : 0.008500\n",
      "Epoch: 3225/25000, Training Loss: 0.000558, lr : 0.008500\n",
      "Epoch: 3226/25000, Training Loss: 0.000612, lr : 0.008500\n",
      "Epoch: 3227/25000, Training Loss: 0.000692, lr : 0.008500\n",
      "Epoch: 3228/25000, Training Loss: 0.000523, lr : 0.008500\n",
      "Epoch: 3229/25000, Training Loss: 0.000704, lr : 0.008500\n",
      "Epoch: 3230/25000, Training Loss: 0.000693, lr : 0.008500\n",
      "Epoch: 3231/25000, Training Loss: 0.000589, lr : 0.008500\n",
      "Epoch: 3232/25000, Training Loss: 0.000741, lr : 0.008500\n",
      "Epoch: 3233/25000, Training Loss: 0.000718, lr : 0.008500\n",
      "Epoch: 3234/25000, Training Loss: 0.000763, lr : 0.008500\n",
      "Epoch: 3235/25000, Training Loss: 0.000904, lr : 0.008500\n",
      "Epoch: 3236/25000, Training Loss: 0.000542, lr : 0.008500\n",
      "Epoch: 3237/25000, Training Loss: 0.000799, lr : 0.008500\n",
      "Epoch: 3238/25000, Training Loss: 0.000649, lr : 0.008500\n",
      "Epoch: 3239/25000, Training Loss: 0.000763, lr : 0.008500\n",
      "Epoch: 3240/25000, Training Loss: 0.000565, lr : 0.008500\n",
      "Epoch: 3241/25000, Training Loss: 0.000764, lr : 0.008500\n",
      "Epoch: 3242/25000, Training Loss: 0.000595, lr : 0.008500\n",
      "Epoch: 3243/25000, Training Loss: 0.000888, lr : 0.008500\n",
      "Epoch: 3244/25000, Training Loss: 0.000739, lr : 0.008500\n",
      "Epoch: 3245/25000, Training Loss: 0.000750, lr : 0.008500\n",
      "Epoch: 3246/25000, Training Loss: 0.000709, lr : 0.008500\n",
      "Epoch: 3247/25000, Training Loss: 0.000509, lr : 0.008500\n",
      "Epoch: 3248/25000, Training Loss: 0.000519, lr : 0.008500\n",
      "Epoch: 3249/25000, Training Loss: 0.000648, lr : 0.008500\n",
      "Epoch: 3250/25000, Training Loss: 0.000653, lr : 0.008500\n",
      "Epoch: 3251/25000, Training Loss: 0.000788, lr : 0.008500\n",
      "Epoch: 3252/25000, Training Loss: 0.000711, lr : 0.008500\n",
      "Epoch: 3253/25000, Training Loss: 0.000446, lr : 0.008500\n",
      "Epoch: 3254/25000, Training Loss: 0.000440, lr : 0.008500\n",
      "Epoch: 3255/25000, Training Loss: 0.000523, lr : 0.008500\n",
      "Epoch: 3256/25000, Training Loss: 0.000565, lr : 0.008500\n",
      "Epoch: 3257/25000, Training Loss: 0.000846, lr : 0.008500\n",
      "Epoch: 3258/25000, Training Loss: 0.001186, lr : 0.008500\n",
      "Epoch: 3259/25000, Training Loss: 0.000658, lr : 0.008500\n",
      "Epoch: 3260/25000, Training Loss: 0.000811, lr : 0.008500\n",
      "Epoch: 3261/25000, Training Loss: 0.000736, lr : 0.008500\n",
      "Epoch: 3262/25000, Training Loss: 0.000824, lr : 0.008500\n",
      "Epoch: 3263/25000, Training Loss: 0.000655, lr : 0.008500\n",
      "Epoch: 3264/25000, Training Loss: 0.000827, lr : 0.008500\n",
      "Epoch: 3265/25000, Training Loss: 0.000784, lr : 0.008500\n",
      "Epoch: 3266/25000, Training Loss: 0.000660, lr : 0.008500\n",
      "Epoch: 3267/25000, Training Loss: 0.000774, lr : 0.008500\n",
      "Epoch: 3268/25000, Training Loss: 0.000748, lr : 0.008500\n",
      "Epoch: 3269/25000, Training Loss: 0.000584, lr : 0.008500\n",
      "Epoch: 3270/25000, Training Loss: 0.000670, lr : 0.008500\n",
      "Epoch: 3271/25000, Training Loss: 0.000612, lr : 0.008500\n",
      "Epoch: 3272/25000, Training Loss: 0.000641, lr : 0.008500\n",
      "Epoch: 3273/25000, Training Loss: 0.000667, lr : 0.008500\n",
      "Epoch: 3274/25000, Training Loss: 0.000551, lr : 0.008500\n",
      "Epoch: 3275/25000, Training Loss: 0.000567, lr : 0.008500\n",
      "Epoch: 3276/25000, Training Loss: 0.000670, lr : 0.008500\n",
      "Epoch: 3277/25000, Training Loss: 0.000547, lr : 0.008500\n",
      "Epoch: 3278/25000, Training Loss: 0.000630, lr : 0.008500\n",
      "Epoch: 3279/25000, Training Loss: 0.000743, lr : 0.008500\n",
      "Epoch: 3280/25000, Training Loss: 0.000652, lr : 0.008500\n",
      "Epoch: 3281/25000, Training Loss: 0.000624, lr : 0.008500\n",
      "Epoch: 3282/25000, Training Loss: 0.000869, lr : 0.008500\n",
      "Epoch: 3283/25000, Training Loss: 0.000505, lr : 0.008500\n",
      "Epoch: 3284/25000, Training Loss: 0.000494, lr : 0.008500\n",
      "Epoch: 3285/25000, Training Loss: 0.000692, lr : 0.008500\n",
      "Epoch: 3286/25000, Training Loss: 0.000493, lr : 0.008500\n",
      "Epoch: 3287/25000, Training Loss: 0.000607, lr : 0.008500\n",
      "Epoch: 3288/25000, Training Loss: 0.000574, lr : 0.008500\n",
      "Epoch: 3289/25000, Training Loss: 0.000670, lr : 0.008500\n",
      "Epoch: 3290/25000, Training Loss: 0.000562, lr : 0.008500\n",
      "Epoch: 3291/25000, Training Loss: 0.000537, lr : 0.008500\n",
      "Epoch: 3292/25000, Training Loss: 0.000526, lr : 0.008500\n",
      "Epoch: 3293/25000, Training Loss: 0.000643, lr : 0.008500\n",
      "Epoch: 3294/25000, Training Loss: 0.000693, lr : 0.008500\n",
      "Epoch: 3295/25000, Training Loss: 0.000711, lr : 0.008500\n",
      "Epoch: 3296/25000, Training Loss: 0.000564, lr : 0.008500\n",
      "Epoch: 3297/25000, Training Loss: 0.000422, lr : 0.008500\n",
      "Epoch: 3298/25000, Training Loss: 0.000526, lr : 0.008500\n",
      "Epoch: 3299/25000, Training Loss: 0.000472, lr : 0.008500\n",
      "Epoch: 3300/25000, Training Loss: 0.000562, lr : 0.008500\n",
      "Epoch: 3301/25000, Training Loss: 0.000642, lr : 0.008500\n",
      "Epoch: 3302/25000, Training Loss: 0.000404, lr : 0.008500\n",
      "Epoch: 3303/25000, Training Loss: 0.000495, lr : 0.008500\n",
      "Epoch: 3304/25000, Training Loss: 0.000559, lr : 0.008500\n",
      "Epoch: 3305/25000, Training Loss: 0.000614, lr : 0.008500\n",
      "Epoch: 3306/25000, Training Loss: 0.000646, lr : 0.008500\n",
      "Epoch: 3307/25000, Training Loss: 0.000575, lr : 0.008500\n",
      "Epoch: 3308/25000, Training Loss: 0.000528, lr : 0.008500\n",
      "Epoch: 3309/25000, Training Loss: 0.000630, lr : 0.008500\n",
      "Epoch: 3310/25000, Training Loss: 0.000905, lr : 0.008500\n",
      "Epoch: 3311/25000, Training Loss: 0.000712, lr : 0.008500\n",
      "Epoch: 3312/25000, Training Loss: 0.000786, lr : 0.008500\n",
      "Epoch: 3313/25000, Training Loss: 0.000600, lr : 0.008500\n",
      "Epoch: 3314/25000, Training Loss: 0.000667, lr : 0.008500\n",
      "Epoch: 3315/25000, Training Loss: 0.000547, lr : 0.008500\n",
      "Epoch: 3316/25000, Training Loss: 0.000659, lr : 0.008500\n",
      "Epoch: 3317/25000, Training Loss: 0.000505, lr : 0.008500\n",
      "Epoch: 3318/25000, Training Loss: 0.000943, lr : 0.008500\n",
      "Epoch: 3319/25000, Training Loss: 0.000410, lr : 0.008500\n",
      "Epoch: 3320/25000, Training Loss: 0.000737, lr : 0.008500\n",
      "Epoch: 3321/25000, Training Loss: 0.000658, lr : 0.008500\n",
      "Epoch: 3322/25000, Training Loss: 0.000835, lr : 0.008500\n",
      "Epoch: 3323/25000, Training Loss: 0.000750, lr : 0.008500\n",
      "Epoch: 3324/25000, Training Loss: 0.000588, lr : 0.008500\n",
      "Epoch: 3325/25000, Training Loss: 0.000724, lr : 0.008500\n",
      "Epoch: 3326/25000, Training Loss: 0.000639, lr : 0.008500\n",
      "Epoch: 3327/25000, Training Loss: 0.000586, lr : 0.008500\n",
      "Epoch: 3328/25000, Training Loss: 0.000688, lr : 0.008500\n",
      "Epoch: 3329/25000, Training Loss: 0.000603, lr : 0.008500\n",
      "Epoch: 3330/25000, Training Loss: 0.000517, lr : 0.008500\n",
      "Epoch: 3331/25000, Training Loss: 0.000663, lr : 0.008500\n",
      "Epoch: 3332/25000, Training Loss: 0.000945, lr : 0.008500\n",
      "Epoch: 3333/25000, Training Loss: 0.000721, lr : 0.008500\n",
      "Epoch: 3334/25000, Training Loss: 0.000899, lr : 0.008500\n",
      "Epoch: 3335/25000, Training Loss: 0.000512, lr : 0.008500\n",
      "Epoch: 3336/25000, Training Loss: 0.000624, lr : 0.008500\n",
      "Epoch: 3337/25000, Training Loss: 0.000819, lr : 0.008500\n",
      "Epoch: 3338/25000, Training Loss: 0.000680, lr : 0.008500\n",
      "Epoch: 3339/25000, Training Loss: 0.000511, lr : 0.008500\n",
      "Epoch: 3340/25000, Training Loss: 0.000737, lr : 0.008500\n",
      "Epoch: 3341/25000, Training Loss: 0.000556, lr : 0.008500\n",
      "Epoch: 3342/25000, Training Loss: 0.000588, lr : 0.008500\n",
      "Epoch: 3343/25000, Training Loss: 0.000581, lr : 0.008500\n",
      "Epoch: 3344/25000, Training Loss: 0.000420, lr : 0.008500\n",
      "Epoch: 3345/25000, Training Loss: 0.000340, lr : 0.008500\n",
      "Epoch: 3346/25000, Training Loss: 0.000576, lr : 0.008500\n",
      "Epoch: 3347/25000, Training Loss: 0.000423, lr : 0.008500\n",
      "Epoch: 3348/25000, Training Loss: 0.000445, lr : 0.008500\n",
      "Epoch: 3349/25000, Training Loss: 0.000657, lr : 0.008500\n",
      "Epoch: 3350/25000, Training Loss: 0.000543, lr : 0.008500\n",
      "Epoch: 3351/25000, Training Loss: 0.000523, lr : 0.008500\n",
      "Epoch: 3352/25000, Training Loss: 0.000620, lr : 0.008500\n",
      "Epoch: 3353/25000, Training Loss: 0.000506, lr : 0.008500\n",
      "Epoch: 3354/25000, Training Loss: 0.000442, lr : 0.008500\n",
      "Epoch: 3355/25000, Training Loss: 0.000665, lr : 0.008500\n",
      "Epoch: 3356/25000, Training Loss: 0.000455, lr : 0.008500\n",
      "Epoch: 3357/25000, Training Loss: 0.000523, lr : 0.008500\n",
      "Epoch: 3358/25000, Training Loss: 0.000461, lr : 0.008500\n",
      "Epoch: 3359/25000, Training Loss: 0.000456, lr : 0.008500\n",
      "Epoch: 3360/25000, Training Loss: 0.000595, lr : 0.008500\n",
      "Epoch: 3361/25000, Training Loss: 0.000438, lr : 0.008500\n",
      "Epoch: 3362/25000, Training Loss: 0.000465, lr : 0.008500\n",
      "Epoch: 3363/25000, Training Loss: 0.000563, lr : 0.008500\n",
      "Epoch: 3364/25000, Training Loss: 0.000435, lr : 0.008500\n",
      "Epoch: 3365/25000, Training Loss: 0.000448, lr : 0.008500\n",
      "Epoch: 3366/25000, Training Loss: 0.000843, lr : 0.008500\n",
      "Epoch: 3367/25000, Training Loss: 0.000508, lr : 0.008500\n",
      "Epoch: 3368/25000, Training Loss: 0.000622, lr : 0.008500\n",
      "Epoch: 3369/25000, Training Loss: 0.000556, lr : 0.008500\n",
      "Epoch: 3370/25000, Training Loss: 0.000542, lr : 0.008500\n",
      "Epoch: 3371/25000, Training Loss: 0.000623, lr : 0.008500\n",
      "Epoch: 3372/25000, Training Loss: 0.000557, lr : 0.008500\n",
      "Epoch: 3373/25000, Training Loss: 0.000428, lr : 0.008500\n",
      "Epoch: 3374/25000, Training Loss: 0.000597, lr : 0.008500\n",
      "Epoch: 3375/25000, Training Loss: 0.000416, lr : 0.008500\n",
      "Epoch: 3376/25000, Training Loss: 0.000617, lr : 0.008500\n",
      "Epoch: 3377/25000, Training Loss: 0.000359, lr : 0.008500\n",
      "Epoch: 3378/25000, Training Loss: 0.000364, lr : 0.008500\n",
      "Epoch: 3379/25000, Training Loss: 0.000501, lr : 0.008500\n",
      "Epoch: 3380/25000, Training Loss: 0.000337, lr : 0.008500\n",
      "Epoch: 3381/25000, Training Loss: 0.000460, lr : 0.008500\n",
      "Epoch: 3382/25000, Training Loss: 0.000453, lr : 0.008500\n",
      "Epoch: 3383/25000, Training Loss: 0.000329, lr : 0.008500\n",
      "Epoch: 3384/25000, Training Loss: 0.000353, lr : 0.008500\n",
      "Epoch: 3385/25000, Training Loss: 0.000559, lr : 0.008500\n",
      "Epoch: 3386/25000, Training Loss: 0.000602, lr : 0.008500\n",
      "Epoch: 3387/25000, Training Loss: 0.000382, lr : 0.008500\n",
      "Epoch: 3388/25000, Training Loss: 0.000419, lr : 0.008500\n",
      "Epoch: 3389/25000, Training Loss: 0.000441, lr : 0.008500\n",
      "Epoch: 3390/25000, Training Loss: 0.000453, lr : 0.008500\n",
      "Epoch: 3391/25000, Training Loss: 0.000337, lr : 0.008500\n",
      "Epoch: 3392/25000, Training Loss: 0.000552, lr : 0.008500\n",
      "Epoch: 3393/25000, Training Loss: 0.000433, lr : 0.008500\n",
      "Epoch: 3394/25000, Training Loss: 0.000385, lr : 0.008500\n",
      "Epoch: 3395/25000, Training Loss: 0.000345, lr : 0.008500\n",
      "Epoch: 3396/25000, Training Loss: 0.000411, lr : 0.008500\n",
      "Epoch: 3397/25000, Training Loss: 0.000419, lr : 0.008500\n",
      "Epoch: 3398/25000, Training Loss: 0.000408, lr : 0.008500\n",
      "Epoch: 3399/25000, Training Loss: 0.000452, lr : 0.008500\n",
      "Epoch: 3400/25000, Training Loss: 0.000475, lr : 0.008500\n",
      "Epoch: 3401/25000, Training Loss: 0.000529, lr : 0.008500\n",
      "Epoch: 3402/25000, Training Loss: 0.000475, lr : 0.008500\n",
      "Epoch: 3403/25000, Training Loss: 0.000339, lr : 0.008500\n",
      "Epoch: 3404/25000, Training Loss: 0.000562, lr : 0.008500\n",
      "Epoch: 3405/25000, Training Loss: 0.000644, lr : 0.008500\n",
      "Epoch: 3406/25000, Training Loss: 0.000607, lr : 0.008500\n",
      "Epoch: 3407/25000, Training Loss: 0.000626, lr : 0.008500\n",
      "Epoch: 3408/25000, Training Loss: 0.000507, lr : 0.008500\n",
      "Epoch: 3409/25000, Training Loss: 0.000449, lr : 0.008500\n",
      "Epoch: 3410/25000, Training Loss: 0.000386, lr : 0.008500\n",
      "Epoch: 3411/25000, Training Loss: 0.000322, lr : 0.008500\n",
      "Epoch: 3412/25000, Training Loss: 0.000404, lr : 0.008500\n",
      "Epoch: 3413/25000, Training Loss: 0.000421, lr : 0.008500\n",
      "Epoch: 3414/25000, Training Loss: 0.000533, lr : 0.008500\n",
      "Epoch: 3415/25000, Training Loss: 0.000420, lr : 0.008500\n",
      "Epoch: 3416/25000, Training Loss: 0.000484, lr : 0.008500\n",
      "Epoch: 3417/25000, Training Loss: 0.000284, lr : 0.008500\n",
      "Epoch: 3418/25000, Training Loss: 0.000324, lr : 0.008500\n",
      "Epoch: 3419/25000, Training Loss: 0.000421, lr : 0.008500\n",
      "Epoch: 3420/25000, Training Loss: 0.000285, lr : 0.008500\n",
      "Epoch: 3421/25000, Training Loss: 0.000419, lr : 0.008500\n",
      "Epoch: 3422/25000, Training Loss: 0.000343, lr : 0.008500\n",
      "Epoch: 3423/25000, Training Loss: 0.000473, lr : 0.008500\n",
      "Epoch: 3424/25000, Training Loss: 0.000353, lr : 0.008500\n",
      "Epoch: 3425/25000, Training Loss: 0.000263, lr : 0.008500\n",
      "Epoch: 3426/25000, Training Loss: 0.000378, lr : 0.008500\n",
      "Epoch: 3427/25000, Training Loss: 0.000444, lr : 0.008500\n",
      "Epoch: 3428/25000, Training Loss: 0.000514, lr : 0.008500\n",
      "Epoch: 3429/25000, Training Loss: 0.000384, lr : 0.008500\n",
      "Epoch: 3430/25000, Training Loss: 0.000392, lr : 0.008500\n",
      "Epoch: 3431/25000, Training Loss: 0.000468, lr : 0.008500\n",
      "Epoch: 3432/25000, Training Loss: 0.000453, lr : 0.008500\n",
      "Epoch: 3433/25000, Training Loss: 0.000423, lr : 0.008500\n",
      "Epoch: 3434/25000, Training Loss: 0.000351, lr : 0.008500\n",
      "Epoch: 3435/25000, Training Loss: 0.000333, lr : 0.008500\n",
      "Epoch: 3436/25000, Training Loss: 0.000287, lr : 0.008500\n",
      "Epoch: 3437/25000, Training Loss: 0.000357, lr : 0.008500\n",
      "Epoch: 3438/25000, Training Loss: 0.000282, lr : 0.008500\n",
      "Epoch: 3439/25000, Training Loss: 0.000319, lr : 0.008500\n",
      "Epoch: 3440/25000, Training Loss: 0.000408, lr : 0.008500\n",
      "Epoch: 3441/25000, Training Loss: 0.000433, lr : 0.008500\n",
      "Epoch: 3442/25000, Training Loss: 0.000422, lr : 0.008500\n",
      "Epoch: 3443/25000, Training Loss: 0.000389, lr : 0.008500\n",
      "Epoch: 3444/25000, Training Loss: 0.000369, lr : 0.008500\n",
      "Epoch: 3445/25000, Training Loss: 0.000389, lr : 0.008500\n",
      "Epoch: 3446/25000, Training Loss: 0.000375, lr : 0.008500\n",
      "Epoch: 3447/25000, Training Loss: 0.000328, lr : 0.008500\n",
      "Epoch: 3448/25000, Training Loss: 0.000521, lr : 0.008500\n",
      "Epoch: 3449/25000, Training Loss: 0.000497, lr : 0.008500\n",
      "Epoch: 3450/25000, Training Loss: 0.000582, lr : 0.008500\n",
      "Epoch: 3451/25000, Training Loss: 0.000408, lr : 0.008500\n",
      "Epoch: 3452/25000, Training Loss: 0.000458, lr : 0.008500\n",
      "Epoch: 3453/25000, Training Loss: 0.000355, lr : 0.008500\n",
      "Epoch: 3454/25000, Training Loss: 0.000479, lr : 0.008500\n",
      "Epoch: 3455/25000, Training Loss: 0.000525, lr : 0.008500\n",
      "Epoch: 3456/25000, Training Loss: 0.000566, lr : 0.008500\n",
      "Epoch: 3457/25000, Training Loss: 0.000459, lr : 0.008500\n",
      "Epoch: 3458/25000, Training Loss: 0.000410, lr : 0.008500\n",
      "Epoch: 3459/25000, Training Loss: 0.000430, lr : 0.008500\n",
      "Epoch: 3460/25000, Training Loss: 0.000451, lr : 0.008500\n",
      "Epoch: 3461/25000, Training Loss: 0.000492, lr : 0.008500\n",
      "Epoch: 3462/25000, Training Loss: 0.000384, lr : 0.008500\n",
      "Epoch: 3463/25000, Training Loss: 0.000312, lr : 0.008500\n",
      "Epoch: 3464/25000, Training Loss: 0.000336, lr : 0.008500\n",
      "Epoch: 3465/25000, Training Loss: 0.000308, lr : 0.008500\n",
      "Epoch: 3466/25000, Training Loss: 0.000328, lr : 0.008500\n",
      "Epoch: 3467/25000, Training Loss: 0.000287, lr : 0.008500\n",
      "Epoch: 3468/25000, Training Loss: 0.000301, lr : 0.008500\n",
      "Epoch: 3469/25000, Training Loss: 0.000306, lr : 0.008500\n",
      "Epoch: 3470/25000, Training Loss: 0.000288, lr : 0.008500\n",
      "Epoch: 3471/25000, Training Loss: 0.000561, lr : 0.008500\n",
      "Epoch: 3472/25000, Training Loss: 0.000207, lr : 0.008500\n",
      "Epoch: 3473/25000, Training Loss: 0.000291, lr : 0.008500\n",
      "Epoch: 3474/25000, Training Loss: 0.000250, lr : 0.008500\n",
      "Epoch: 3475/25000, Training Loss: 0.000293, lr : 0.008500\n",
      "Epoch: 3476/25000, Training Loss: 0.000240, lr : 0.008500\n",
      "Epoch: 3477/25000, Training Loss: 0.000449, lr : 0.008500\n",
      "Epoch: 3478/25000, Training Loss: 0.000381, lr : 0.008500\n",
      "Epoch: 3479/25000, Training Loss: 0.000327, lr : 0.008500\n",
      "Epoch: 3480/25000, Training Loss: 0.000314, lr : 0.008500\n",
      "Epoch: 3481/25000, Training Loss: 0.000366, lr : 0.008500\n",
      "Epoch: 3482/25000, Training Loss: 0.000430, lr : 0.008500\n",
      "Epoch: 3483/25000, Training Loss: 0.000377, lr : 0.008500\n",
      "Epoch: 3484/25000, Training Loss: 0.000451, lr : 0.008500\n",
      "Epoch: 3485/25000, Training Loss: 0.000363, lr : 0.008500\n",
      "Epoch: 3486/25000, Training Loss: 0.000479, lr : 0.008500\n",
      "Epoch: 3487/25000, Training Loss: 0.000314, lr : 0.008500\n",
      "Epoch: 3488/25000, Training Loss: 0.000417, lr : 0.008500\n",
      "Epoch: 3489/25000, Training Loss: 0.000340, lr : 0.008500\n",
      "Epoch: 3490/25000, Training Loss: 0.000447, lr : 0.008500\n",
      "Epoch: 3491/25000, Training Loss: 0.000359, lr : 0.008500\n",
      "Epoch: 3492/25000, Training Loss: 0.000354, lr : 0.008500\n",
      "Epoch: 3493/25000, Training Loss: 0.000545, lr : 0.008500\n",
      "Epoch: 3494/25000, Training Loss: 0.000401, lr : 0.008500\n",
      "Epoch: 3495/25000, Training Loss: 0.000334, lr : 0.008500\n",
      "Epoch: 3496/25000, Training Loss: 0.000451, lr : 0.008500\n",
      "Epoch: 3497/25000, Training Loss: 0.000319, lr : 0.008500\n",
      "Epoch: 3498/25000, Training Loss: 0.000311, lr : 0.008500\n",
      "Epoch: 3499/25000, Training Loss: 0.000322, lr : 0.008500\n",
      "Epoch: 3500/25000, Training Loss: 0.000396, lr : 0.008500\n",
      "Epoch: 3501/25000, Training Loss: 0.000333, lr : 0.008500\n",
      "Epoch: 3502/25000, Training Loss: 0.000550, lr : 0.008500\n",
      "Epoch: 3503/25000, Training Loss: 0.000787, lr : 0.008500\n",
      "Epoch: 3504/25000, Training Loss: 0.000460, lr : 0.008500\n",
      "Epoch: 3505/25000, Training Loss: 0.000788, lr : 0.008500\n",
      "Epoch: 3506/25000, Training Loss: 0.000392, lr : 0.008500\n",
      "Epoch: 3507/25000, Training Loss: 0.000615, lr : 0.008500\n",
      "Epoch: 3508/25000, Training Loss: 0.000389, lr : 0.008500\n",
      "Epoch: 3509/25000, Training Loss: 0.000492, lr : 0.008500\n",
      "Epoch: 3510/25000, Training Loss: 0.000625, lr : 0.008500\n",
      "Epoch: 3511/25000, Training Loss: 0.000352, lr : 0.008500\n",
      "Epoch: 3512/25000, Training Loss: 0.000585, lr : 0.008500\n",
      "Epoch: 3513/25000, Training Loss: 0.000351, lr : 0.008500\n",
      "Epoch: 3514/25000, Training Loss: 0.000336, lr : 0.008500\n",
      "Epoch: 3515/25000, Training Loss: 0.000472, lr : 0.008500\n",
      "Epoch: 3516/25000, Training Loss: 0.000429, lr : 0.008500\n",
      "Epoch: 3517/25000, Training Loss: 0.000439, lr : 0.008500\n",
      "Epoch: 3518/25000, Training Loss: 0.000392, lr : 0.008500\n",
      "Epoch: 3519/25000, Training Loss: 0.000324, lr : 0.008500\n",
      "Epoch: 3520/25000, Training Loss: 0.000331, lr : 0.008500\n",
      "Epoch: 3521/25000, Training Loss: 0.000365, lr : 0.008500\n",
      "Epoch: 3522/25000, Training Loss: 0.000400, lr : 0.008500\n",
      "Epoch: 3523/25000, Training Loss: 0.000352, lr : 0.008500\n",
      "Epoch: 3524/25000, Training Loss: 0.000331, lr : 0.008500\n",
      "Epoch: 3525/25000, Training Loss: 0.000339, lr : 0.008500\n",
      "Epoch: 3526/25000, Training Loss: 0.000259, lr : 0.008500\n",
      "Epoch: 3527/25000, Training Loss: 0.000368, lr : 0.008500\n",
      "Epoch: 3528/25000, Training Loss: 0.000331, lr : 0.008500\n",
      "Epoch: 3529/25000, Training Loss: 0.000278, lr : 0.008500\n",
      "Epoch: 3530/25000, Training Loss: 0.000306, lr : 0.008500\n",
      "Epoch: 3531/25000, Training Loss: 0.000305, lr : 0.008500\n",
      "Epoch: 3532/25000, Training Loss: 0.000346, lr : 0.008500\n",
      "Epoch: 3533/25000, Training Loss: 0.000347, lr : 0.008500\n",
      "Epoch: 3534/25000, Training Loss: 0.000248, lr : 0.008500\n",
      "Epoch: 3535/25000, Training Loss: 0.000268, lr : 0.008500\n",
      "Epoch: 3536/25000, Training Loss: 0.000332, lr : 0.008500\n",
      "Epoch: 3537/25000, Training Loss: 0.000254, lr : 0.008500\n",
      "Epoch: 3538/25000, Training Loss: 0.000220, lr : 0.008500\n",
      "Epoch: 3539/25000, Training Loss: 0.000274, lr : 0.008500\n",
      "Epoch: 3540/25000, Training Loss: 0.000195, lr : 0.008500\n",
      "Epoch: 3541/25000, Training Loss: 0.000295, lr : 0.008500\n",
      "Epoch: 3542/25000, Training Loss: 0.000248, lr : 0.008500\n",
      "Epoch: 3543/25000, Training Loss: 0.000317, lr : 0.008500\n",
      "Epoch: 3544/25000, Training Loss: 0.000301, lr : 0.008500\n",
      "Epoch: 3545/25000, Training Loss: 0.000248, lr : 0.008500\n",
      "Epoch: 3546/25000, Training Loss: 0.000215, lr : 0.008500\n",
      "Epoch: 3547/25000, Training Loss: 0.000281, lr : 0.008500\n",
      "Epoch: 3548/25000, Training Loss: 0.000211, lr : 0.008500\n",
      "Epoch: 3549/25000, Training Loss: 0.000260, lr : 0.008500\n",
      "Epoch: 3550/25000, Training Loss: 0.000214, lr : 0.008500\n",
      "Epoch: 3551/25000, Training Loss: 0.000245, lr : 0.008500\n",
      "Epoch: 3552/25000, Training Loss: 0.000205, lr : 0.008500\n",
      "Epoch: 3553/25000, Training Loss: 0.000250, lr : 0.008500\n",
      "Epoch: 3554/25000, Training Loss: 0.000203, lr : 0.008500\n",
      "Epoch: 3555/25000, Training Loss: 0.000245, lr : 0.008500\n",
      "Epoch: 3556/25000, Training Loss: 0.000206, lr : 0.008500\n",
      "Epoch: 3557/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 3558/25000, Training Loss: 0.000231, lr : 0.008500\n",
      "Epoch: 3559/25000, Training Loss: 0.000262, lr : 0.008500\n",
      "Epoch: 3560/25000, Training Loss: 0.000223, lr : 0.008500\n",
      "Epoch: 3561/25000, Training Loss: 0.000263, lr : 0.008500\n",
      "Epoch: 3562/25000, Training Loss: 0.000233, lr : 0.008500\n",
      "Epoch: 3563/25000, Training Loss: 0.000214, lr : 0.008500\n",
      "Epoch: 3564/25000, Training Loss: 0.000213, lr : 0.008500\n",
      "Epoch: 3565/25000, Training Loss: 0.000210, lr : 0.008500\n",
      "Epoch: 3566/25000, Training Loss: 0.000275, lr : 0.008500\n",
      "Epoch: 3567/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 3568/25000, Training Loss: 0.000227, lr : 0.008500\n",
      "Epoch: 3569/25000, Training Loss: 0.000231, lr : 0.008500\n",
      "Epoch: 3570/25000, Training Loss: 0.000231, lr : 0.008500\n",
      "Epoch: 3571/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 3572/25000, Training Loss: 0.000190, lr : 0.008500\n",
      "Epoch: 3573/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 3574/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 3575/25000, Training Loss: 0.000172, lr : 0.008500\n",
      "Epoch: 3576/25000, Training Loss: 0.000175, lr : 0.008500\n",
      "Epoch: 3577/25000, Training Loss: 0.000194, lr : 0.008500\n",
      "Epoch: 3578/25000, Training Loss: 0.000172, lr : 0.008500\n",
      "Epoch: 3579/25000, Training Loss: 0.000151, lr : 0.008500\n",
      "Epoch: 3580/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 3581/25000, Training Loss: 0.000222, lr : 0.008500\n",
      "Epoch: 3582/25000, Training Loss: 0.000202, lr : 0.008500\n",
      "Epoch: 3583/25000, Training Loss: 0.000182, lr : 0.008500\n",
      "Epoch: 3584/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 3585/25000, Training Loss: 0.000177, lr : 0.008500\n",
      "Epoch: 3586/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 3587/25000, Training Loss: 0.000163, lr : 0.008500\n",
      "Epoch: 3588/25000, Training Loss: 0.000256, lr : 0.008500\n",
      "Epoch: 3589/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 3590/25000, Training Loss: 0.000219, lr : 0.008500\n",
      "Epoch: 3591/25000, Training Loss: 0.000192, lr : 0.008500\n",
      "Epoch: 3592/25000, Training Loss: 0.000248, lr : 0.008500\n",
      "Epoch: 3593/25000, Training Loss: 0.000167, lr : 0.008500\n",
      "Epoch: 3594/25000, Training Loss: 0.000242, lr : 0.008500\n",
      "Epoch: 3595/25000, Training Loss: 0.000210, lr : 0.008500\n",
      "Epoch: 3596/25000, Training Loss: 0.000188, lr : 0.008500\n",
      "Epoch: 3597/25000, Training Loss: 0.000179, lr : 0.008500\n",
      "Epoch: 3598/25000, Training Loss: 0.000207, lr : 0.008500\n",
      "Epoch: 3599/25000, Training Loss: 0.000199, lr : 0.008500\n",
      "Epoch: 3600/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 3601/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 3602/25000, Training Loss: 0.000188, lr : 0.008500\n",
      "Epoch: 3603/25000, Training Loss: 0.000193, lr : 0.008500\n",
      "Epoch: 3604/25000, Training Loss: 0.000159, lr : 0.008500\n",
      "Epoch: 3605/25000, Training Loss: 0.000201, lr : 0.008500\n",
      "Epoch: 3606/25000, Training Loss: 0.000238, lr : 0.008500\n",
      "Epoch: 3607/25000, Training Loss: 0.000239, lr : 0.008500\n",
      "Epoch: 3608/25000, Training Loss: 0.000219, lr : 0.008500\n",
      "Epoch: 3609/25000, Training Loss: 0.000192, lr : 0.008500\n",
      "Epoch: 3610/25000, Training Loss: 0.000192, lr : 0.008500\n",
      "Epoch: 3611/25000, Training Loss: 0.000190, lr : 0.008500\n",
      "Epoch: 3612/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 3613/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 3614/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 3615/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 3616/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 3617/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 3618/25000, Training Loss: 0.000279, lr : 0.008500\n",
      "Epoch: 3619/25000, Training Loss: 0.000241, lr : 0.008500\n",
      "Epoch: 3620/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 3621/25000, Training Loss: 0.000170, lr : 0.008500\n",
      "Epoch: 3622/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 3623/25000, Training Loss: 0.000209, lr : 0.008500\n",
      "Epoch: 3624/25000, Training Loss: 0.000205, lr : 0.008500\n",
      "Epoch: 3625/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 3626/25000, Training Loss: 0.000226, lr : 0.008500\n",
      "Epoch: 3627/25000, Training Loss: 0.000191, lr : 0.008500\n",
      "Epoch: 3628/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 3629/25000, Training Loss: 0.000184, lr : 0.008500\n",
      "Epoch: 3630/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 3631/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 3632/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 3633/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 3634/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 3635/25000, Training Loss: 0.000197, lr : 0.008500\n",
      "Epoch: 3636/25000, Training Loss: 0.000286, lr : 0.008500\n",
      "Epoch: 3637/25000, Training Loss: 0.000178, lr : 0.008500\n",
      "Epoch: 3638/25000, Training Loss: 0.000201, lr : 0.008500\n",
      "Epoch: 3639/25000, Training Loss: 0.000191, lr : 0.008500\n",
      "Epoch: 3640/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 3641/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 3642/25000, Training Loss: 0.000184, lr : 0.008500\n",
      "Epoch: 3643/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 3644/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 3645/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 3646/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 3647/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 3648/25000, Training Loss: 0.000228, lr : 0.008500\n",
      "Epoch: 3649/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 3650/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 3651/25000, Training Loss: 0.000235, lr : 0.008500\n",
      "Epoch: 3652/25000, Training Loss: 0.000204, lr : 0.008500\n",
      "Epoch: 3653/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 3654/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 3655/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 3656/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 3657/25000, Training Loss: 0.000205, lr : 0.008500\n",
      "Epoch: 3658/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 3659/25000, Training Loss: 0.000167, lr : 0.008500\n",
      "Epoch: 3660/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 3661/25000, Training Loss: 0.000161, lr : 0.008500\n",
      "Epoch: 3662/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 3663/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 3664/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 3665/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 3666/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 3667/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 3668/25000, Training Loss: 0.000214, lr : 0.008500\n",
      "Epoch: 3669/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 3670/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 3671/25000, Training Loss: 0.000187, lr : 0.008500\n",
      "Epoch: 3672/25000, Training Loss: 0.000204, lr : 0.008500\n",
      "Epoch: 3673/25000, Training Loss: 0.000212, lr : 0.008500\n",
      "Epoch: 3674/25000, Training Loss: 0.000331, lr : 0.008500\n",
      "Epoch: 3675/25000, Training Loss: 0.000158, lr : 0.008500\n",
      "Epoch: 3676/25000, Training Loss: 0.000238, lr : 0.008500\n",
      "Epoch: 3677/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 3678/25000, Training Loss: 0.000191, lr : 0.008500\n",
      "Epoch: 3679/25000, Training Loss: 0.000249, lr : 0.008500\n",
      "Epoch: 3680/25000, Training Loss: 0.000155, lr : 0.008500\n",
      "Epoch: 3681/25000, Training Loss: 0.000154, lr : 0.008500\n",
      "Epoch: 3682/25000, Training Loss: 0.000154, lr : 0.008500\n",
      "Epoch: 3683/25000, Training Loss: 0.000175, lr : 0.008500\n",
      "Epoch: 3684/25000, Training Loss: 0.000168, lr : 0.008500\n",
      "Epoch: 3685/25000, Training Loss: 0.000201, lr : 0.008500\n",
      "Epoch: 3686/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 3687/25000, Training Loss: 0.000228, lr : 0.008500\n",
      "Epoch: 3688/25000, Training Loss: 0.000246, lr : 0.008500\n",
      "Epoch: 3689/25000, Training Loss: 0.000318, lr : 0.008500\n",
      "Epoch: 3690/25000, Training Loss: 0.000412, lr : 0.008500\n",
      "Epoch: 3691/25000, Training Loss: 0.000421, lr : 0.008500\n",
      "Epoch: 3692/25000, Training Loss: 0.000378, lr : 0.008500\n",
      "Epoch: 3693/25000, Training Loss: 0.000283, lr : 0.008500\n",
      "Epoch: 3694/25000, Training Loss: 0.000236, lr : 0.008500\n",
      "Epoch: 3695/25000, Training Loss: 0.000224, lr : 0.008500\n",
      "Epoch: 3696/25000, Training Loss: 0.000210, lr : 0.008500\n",
      "Epoch: 3697/25000, Training Loss: 0.000219, lr : 0.008500\n",
      "Epoch: 3698/25000, Training Loss: 0.000204, lr : 0.008500\n",
      "Epoch: 3699/25000, Training Loss: 0.000228, lr : 0.008500\n",
      "Epoch: 3700/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 3701/25000, Training Loss: 0.000258, lr : 0.008500\n",
      "Epoch: 3702/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 3703/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 3704/25000, Training Loss: 0.000196, lr : 0.008500\n",
      "Epoch: 3705/25000, Training Loss: 0.000198, lr : 0.008500\n",
      "Epoch: 3706/25000, Training Loss: 0.000189, lr : 0.008500\n",
      "Epoch: 3707/25000, Training Loss: 0.000272, lr : 0.008500\n",
      "Epoch: 3708/25000, Training Loss: 0.000212, lr : 0.008500\n",
      "Epoch: 3709/25000, Training Loss: 0.000407, lr : 0.008500\n",
      "Epoch: 3710/25000, Training Loss: 0.000308, lr : 0.008500\n",
      "Epoch: 3711/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 3712/25000, Training Loss: 0.000302, lr : 0.008500\n",
      "Epoch: 3713/25000, Training Loss: 0.000231, lr : 0.008500\n",
      "Epoch: 3714/25000, Training Loss: 0.000253, lr : 0.008500\n",
      "Epoch: 3715/25000, Training Loss: 0.000321, lr : 0.008500\n",
      "Epoch: 3716/25000, Training Loss: 0.000312, lr : 0.008500\n",
      "Epoch: 3717/25000, Training Loss: 0.000285, lr : 0.008500\n",
      "Epoch: 3718/25000, Training Loss: 0.000319, lr : 0.008500\n",
      "Epoch: 3719/25000, Training Loss: 0.000278, lr : 0.008500\n",
      "Epoch: 3720/25000, Training Loss: 0.000225, lr : 0.008500\n",
      "Epoch: 3721/25000, Training Loss: 0.000415, lr : 0.008500\n",
      "Epoch: 3722/25000, Training Loss: 0.000267, lr : 0.008500\n",
      "Epoch: 3723/25000, Training Loss: 0.000317, lr : 0.008500\n",
      "Epoch: 3724/25000, Training Loss: 0.000266, lr : 0.008500\n",
      "Epoch: 3725/25000, Training Loss: 0.000278, lr : 0.008500\n",
      "Epoch: 3726/25000, Training Loss: 0.000380, lr : 0.008500\n",
      "Epoch: 3727/25000, Training Loss: 0.000189, lr : 0.008500\n",
      "Epoch: 3728/25000, Training Loss: 0.000285, lr : 0.008500\n",
      "Epoch: 3729/25000, Training Loss: 0.000193, lr : 0.008500\n",
      "Epoch: 3730/25000, Training Loss: 0.000203, lr : 0.008500\n",
      "Epoch: 3731/25000, Training Loss: 0.000163, lr : 0.008500\n",
      "Epoch: 3732/25000, Training Loss: 0.000209, lr : 0.008500\n",
      "Epoch: 3733/25000, Training Loss: 0.000202, lr : 0.008500\n",
      "Epoch: 3734/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 3735/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 3736/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 3737/25000, Training Loss: 0.000170, lr : 0.008500\n",
      "Epoch: 3738/25000, Training Loss: 0.000178, lr : 0.008500\n",
      "Epoch: 3739/25000, Training Loss: 0.000178, lr : 0.008500\n",
      "Epoch: 3740/25000, Training Loss: 0.000189, lr : 0.008500\n",
      "Epoch: 3741/25000, Training Loss: 0.000267, lr : 0.008500\n",
      "Epoch: 3742/25000, Training Loss: 0.000227, lr : 0.008500\n",
      "Epoch: 3743/25000, Training Loss: 0.000192, lr : 0.008500\n",
      "Epoch: 3744/25000, Training Loss: 0.000258, lr : 0.008500\n",
      "Epoch: 3745/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 3746/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 3747/25000, Training Loss: 0.000195, lr : 0.008500\n",
      "Epoch: 3748/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 3749/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 3750/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 3751/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 3752/25000, Training Loss: 0.000201, lr : 0.008500\n",
      "Epoch: 3753/25000, Training Loss: 0.000196, lr : 0.008500\n",
      "Epoch: 3754/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 3755/25000, Training Loss: 0.000197, lr : 0.008500\n",
      "Epoch: 3756/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 3757/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 3758/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 3759/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 3760/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 3761/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 3762/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 3763/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 3764/25000, Training Loss: 0.000151, lr : 0.008500\n",
      "Epoch: 3765/25000, Training Loss: 0.000196, lr : 0.008500\n",
      "Epoch: 3766/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 3767/25000, Training Loss: 0.000166, lr : 0.008500\n",
      "Epoch: 3768/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 3769/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 3770/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 3771/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 3772/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 3773/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 3774/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 3775/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 3776/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 3777/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 3778/25000, Training Loss: 0.000191, lr : 0.008500\n",
      "Epoch: 3779/25000, Training Loss: 0.000225, lr : 0.008500\n",
      "Epoch: 3780/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 3781/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 3782/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 3783/25000, Training Loss: 0.000205, lr : 0.008500\n",
      "Epoch: 3784/25000, Training Loss: 0.000189, lr : 0.008500\n",
      "Epoch: 3785/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 3786/25000, Training Loss: 0.000244, lr : 0.008500\n",
      "Epoch: 3787/25000, Training Loss: 0.000170, lr : 0.008500\n",
      "Epoch: 3788/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 3789/25000, Training Loss: 0.000190, lr : 0.008500\n",
      "Epoch: 3790/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 3791/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 3792/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 3793/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 3794/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 3795/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 3796/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 3797/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 3798/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 3799/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 3800/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 3801/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 3802/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 3803/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 3804/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 3805/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 3806/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 3807/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 3808/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 3809/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 3810/25000, Training Loss: 0.000168, lr : 0.008500\n",
      "Epoch: 3811/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 3812/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 3813/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 3814/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 3815/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 3816/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 3817/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 3818/25000, Training Loss: 0.000202, lr : 0.008500\n",
      "Epoch: 3819/25000, Training Loss: 0.000178, lr : 0.008500\n",
      "Epoch: 3820/25000, Training Loss: 0.000164, lr : 0.008500\n",
      "Epoch: 3821/25000, Training Loss: 0.000131, lr : 0.008500\n",
      "Epoch: 3822/25000, Training Loss: 0.000161, lr : 0.008500\n",
      "Epoch: 3823/25000, Training Loss: 0.000159, lr : 0.008500\n",
      "Epoch: 3824/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 3825/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 3826/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 3827/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 3828/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 3829/25000, Training Loss: 0.000172, lr : 0.008500\n",
      "Epoch: 3830/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 3831/25000, Training Loss: 0.000205, lr : 0.008500\n",
      "Epoch: 3832/25000, Training Loss: 0.000369, lr : 0.008500\n",
      "Epoch: 3833/25000, Training Loss: 0.000215, lr : 0.008500\n",
      "Epoch: 3834/25000, Training Loss: 0.000177, lr : 0.008500\n",
      "Epoch: 3835/25000, Training Loss: 0.000188, lr : 0.008500\n",
      "Epoch: 3836/25000, Training Loss: 0.000272, lr : 0.008500\n",
      "Epoch: 3837/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 3838/25000, Training Loss: 0.000229, lr : 0.008500\n",
      "Epoch: 3839/25000, Training Loss: 0.000306, lr : 0.008500\n",
      "Epoch: 3840/25000, Training Loss: 0.000182, lr : 0.008500\n",
      "Epoch: 3841/25000, Training Loss: 0.000199, lr : 0.008500\n",
      "Epoch: 3842/25000, Training Loss: 0.000178, lr : 0.008500\n",
      "Epoch: 3843/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 3844/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 3845/25000, Training Loss: 0.000237, lr : 0.008500\n",
      "Epoch: 3846/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 3847/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 3848/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 3849/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 3850/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 3851/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 3852/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 3853/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 3854/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 3855/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 3856/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 3857/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 3858/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 3859/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 3860/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 3861/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 3862/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 3863/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 3864/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 3865/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 3866/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 3867/25000, Training Loss: 0.000191, lr : 0.008500\n",
      "Epoch: 3868/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 3869/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 3870/25000, Training Loss: 0.000178, lr : 0.008500\n",
      "Epoch: 3871/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 3872/25000, Training Loss: 0.000401, lr : 0.008500\n",
      "Epoch: 3873/25000, Training Loss: 0.000527, lr : 0.008500\n",
      "Epoch: 3874/25000, Training Loss: 0.000341, lr : 0.008500\n",
      "Epoch: 3875/25000, Training Loss: 0.000293, lr : 0.008500\n",
      "Epoch: 3876/25000, Training Loss: 0.000361, lr : 0.008500\n",
      "Epoch: 3877/25000, Training Loss: 0.000288, lr : 0.008500\n",
      "Epoch: 3878/25000, Training Loss: 0.000224, lr : 0.008500\n",
      "Epoch: 3879/25000, Training Loss: 0.000235, lr : 0.008500\n",
      "Epoch: 3880/25000, Training Loss: 0.000203, lr : 0.008500\n",
      "Epoch: 3881/25000, Training Loss: 0.000185, lr : 0.008500\n",
      "Epoch: 3882/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 3883/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 3884/25000, Training Loss: 0.000211, lr : 0.008500\n",
      "Epoch: 3885/25000, Training Loss: 0.000294, lr : 0.008500\n",
      "Epoch: 3886/25000, Training Loss: 0.000184, lr : 0.008500\n",
      "Epoch: 3887/25000, Training Loss: 0.000290, lr : 0.008500\n",
      "Epoch: 3888/25000, Training Loss: 0.000309, lr : 0.008500\n",
      "Epoch: 3889/25000, Training Loss: 0.000219, lr : 0.008500\n",
      "Epoch: 3890/25000, Training Loss: 0.000283, lr : 0.008500\n",
      "Epoch: 3891/25000, Training Loss: 0.000393, lr : 0.008500\n",
      "Epoch: 3892/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 3893/25000, Training Loss: 0.000239, lr : 0.008500\n",
      "Epoch: 3894/25000, Training Loss: 0.000222, lr : 0.008500\n",
      "Epoch: 3895/25000, Training Loss: 0.000212, lr : 0.008500\n",
      "Epoch: 3896/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 3897/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 3898/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 3899/25000, Training Loss: 0.000161, lr : 0.008500\n",
      "Epoch: 3900/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 3901/25000, Training Loss: 0.000154, lr : 0.008500\n",
      "Epoch: 3902/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 3903/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 3904/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 3905/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 3906/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 3907/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 3908/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 3909/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 3910/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 3911/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 3912/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 3913/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 3914/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 3915/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 3916/25000, Training Loss: 0.000241, lr : 0.008500\n",
      "Epoch: 3917/25000, Training Loss: 0.000331, lr : 0.008500\n",
      "Epoch: 3918/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 3919/25000, Training Loss: 0.000242, lr : 0.008500\n",
      "Epoch: 3920/25000, Training Loss: 0.000283, lr : 0.008500\n",
      "Epoch: 3921/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 3922/25000, Training Loss: 0.000199, lr : 0.008500\n",
      "Epoch: 3923/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 3924/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 3925/25000, Training Loss: 0.000151, lr : 0.008500\n",
      "Epoch: 3926/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 3927/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 3928/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 3929/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 3930/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 3931/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 3932/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 3933/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 3934/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 3935/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 3936/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 3937/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 3938/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 3939/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 3940/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 3941/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 3942/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 3943/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 3944/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 3945/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 3946/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 3947/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 3948/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 3949/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 3950/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 3951/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 3952/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 3953/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 3954/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 3955/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 3956/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 3957/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 3958/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 3959/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 3960/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 3961/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 3962/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 3963/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 3964/25000, Training Loss: 0.000244, lr : 0.008500\n",
      "Epoch: 3965/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 3966/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 3967/25000, Training Loss: 0.000191, lr : 0.008500\n",
      "Epoch: 3968/25000, Training Loss: 0.000166, lr : 0.008500\n",
      "Epoch: 3969/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 3970/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 3971/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 3972/25000, Training Loss: 0.000164, lr : 0.008500\n",
      "Epoch: 3973/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 3974/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 3975/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 3976/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 3977/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 3978/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 3979/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 3980/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 3981/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 3982/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 3983/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 3984/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 3985/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 3986/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 3987/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 3988/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 3989/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 3990/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 3991/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 3992/25000, Training Loss: 0.000163, lr : 0.008500\n",
      "Epoch: 3993/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 3994/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 3995/25000, Training Loss: 0.000261, lr : 0.008500\n",
      "Epoch: 3996/25000, Training Loss: 0.000247, lr : 0.008500\n",
      "Epoch: 3997/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 3998/25000, Training Loss: 0.000290, lr : 0.008500\n",
      "Epoch: 3999/25000, Training Loss: 0.000212, lr : 0.008500\n",
      "Epoch: 4000/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 4001/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 4002/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 4003/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 4004/25000, Training Loss: 0.000131, lr : 0.008500\n",
      "Epoch: 4005/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 4006/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 4007/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 4008/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 4009/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 4010/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 4011/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 4012/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 4013/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 4014/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 4015/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 4016/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 4017/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4018/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 4019/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 4020/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 4021/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 4022/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 4023/25000, Training Loss: 0.000131, lr : 0.008500\n",
      "Epoch: 4024/25000, Training Loss: 0.000131, lr : 0.008500\n",
      "Epoch: 4025/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 4026/25000, Training Loss: 0.000207, lr : 0.008500\n",
      "Epoch: 4027/25000, Training Loss: 0.000221, lr : 0.008500\n",
      "Epoch: 4028/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 4029/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 4030/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 4031/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 4032/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 4033/25000, Training Loss: 0.000191, lr : 0.008500\n",
      "Epoch: 4034/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 4035/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 4036/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 4037/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 4038/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 4039/25000, Training Loss: 0.000185, lr : 0.008500\n",
      "Epoch: 4040/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 4041/25000, Training Loss: 0.000175, lr : 0.008500\n",
      "Epoch: 4042/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 4043/25000, Training Loss: 0.000166, lr : 0.008500\n",
      "Epoch: 4044/25000, Training Loss: 0.000199, lr : 0.008500\n",
      "Epoch: 4045/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 4046/25000, Training Loss: 0.000154, lr : 0.008500\n",
      "Epoch: 4047/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 4048/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 4049/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 4050/25000, Training Loss: 0.000208, lr : 0.008500\n",
      "Epoch: 4051/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 4052/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 4053/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 4054/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 4055/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 4056/25000, Training Loss: 0.000131, lr : 0.008500\n",
      "Epoch: 4057/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 4058/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 4059/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 4060/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 4061/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 4062/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 4063/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 4064/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 4065/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 4066/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 4067/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 4068/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 4069/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 4070/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 4071/25000, Training Loss: 0.000131, lr : 0.008500\n",
      "Epoch: 4072/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 4073/25000, Training Loss: 0.000163, lr : 0.008500\n",
      "Epoch: 4074/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 4075/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 4076/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 4077/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 4078/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 4079/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 4080/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 4081/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 4082/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 4083/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 4084/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 4085/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 4086/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 4087/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 4088/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 4089/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 4090/25000, Training Loss: 0.000189, lr : 0.008500\n",
      "Epoch: 4091/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 4092/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 4093/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 4094/25000, Training Loss: 0.000154, lr : 0.008500\n",
      "Epoch: 4095/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 4096/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 4097/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 4098/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 4099/25000, Training Loss: 0.000166, lr : 0.008500\n",
      "Epoch: 4100/25000, Training Loss: 0.000195, lr : 0.008500\n",
      "Epoch: 4101/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 4102/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 4103/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 4104/25000, Training Loss: 0.000205, lr : 0.008500\n",
      "Epoch: 4105/25000, Training Loss: 0.000196, lr : 0.008500\n",
      "Epoch: 4106/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 4107/25000, Training Loss: 0.000193, lr : 0.008500\n",
      "Epoch: 4108/25000, Training Loss: 0.000539, lr : 0.008500\n",
      "Epoch: 4109/25000, Training Loss: 0.000367, lr : 0.008500\n",
      "Epoch: 4110/25000, Training Loss: 0.000252, lr : 0.008500\n",
      "Epoch: 4111/25000, Training Loss: 0.000288, lr : 0.008500\n",
      "Epoch: 4112/25000, Training Loss: 0.000249, lr : 0.008500\n",
      "Epoch: 4113/25000, Training Loss: 0.000218, lr : 0.008500\n",
      "Epoch: 4114/25000, Training Loss: 0.000235, lr : 0.008500\n",
      "Epoch: 4115/25000, Training Loss: 0.000249, lr : 0.008500\n",
      "Epoch: 4116/25000, Training Loss: 0.000248, lr : 0.008500\n",
      "Epoch: 4117/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 4118/25000, Training Loss: 0.000193, lr : 0.008500\n",
      "Epoch: 4119/25000, Training Loss: 0.000272, lr : 0.008500\n",
      "Epoch: 4120/25000, Training Loss: 0.000236, lr : 0.008500\n",
      "Epoch: 4121/25000, Training Loss: 0.000193, lr : 0.008500\n",
      "Epoch: 4122/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 4123/25000, Training Loss: 0.000179, lr : 0.008500\n",
      "Epoch: 4124/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 4125/25000, Training Loss: 0.000155, lr : 0.008500\n",
      "Epoch: 4126/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 4127/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 4128/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 4129/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 4130/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 4131/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 4132/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 4133/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 4134/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 4135/25000, Training Loss: 0.000242, lr : 0.008500\n",
      "Epoch: 4136/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 4137/25000, Training Loss: 0.000257, lr : 0.008500\n",
      "Epoch: 4138/25000, Training Loss: 0.000161, lr : 0.008500\n",
      "Epoch: 4139/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 4140/25000, Training Loss: 0.000204, lr : 0.008500\n",
      "Epoch: 4141/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 4142/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 4143/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 4144/25000, Training Loss: 0.000270, lr : 0.008500\n",
      "Epoch: 4145/25000, Training Loss: 0.000249, lr : 0.008500\n",
      "Epoch: 4146/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 4147/25000, Training Loss: 0.000223, lr : 0.008500\n",
      "Epoch: 4148/25000, Training Loss: 0.000302, lr : 0.008500\n",
      "Epoch: 4149/25000, Training Loss: 0.000206, lr : 0.008500\n",
      "Epoch: 4150/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 4151/25000, Training Loss: 0.000187, lr : 0.008500\n",
      "Epoch: 4152/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 4153/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 4154/25000, Training Loss: 0.000204, lr : 0.008500\n",
      "Epoch: 4155/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 4156/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 4157/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 4158/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 4159/25000, Training Loss: 0.000226, lr : 0.008500\n",
      "Epoch: 4160/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 4161/25000, Training Loss: 0.000182, lr : 0.008500\n",
      "Epoch: 4162/25000, Training Loss: 0.000198, lr : 0.008500\n",
      "Epoch: 4163/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 4164/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 4165/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 4166/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 4167/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 4168/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 4169/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 4170/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 4171/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 4172/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 4173/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 4174/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 4175/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 4176/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 4177/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 4178/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 4179/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 4180/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 4181/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 4182/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4183/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 4184/25000, Training Loss: 0.000186, lr : 0.008500\n",
      "Epoch: 4185/25000, Training Loss: 0.000167, lr : 0.008500\n",
      "Epoch: 4186/25000, Training Loss: 0.000167, lr : 0.008500\n",
      "Epoch: 4187/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 4188/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 4189/25000, Training Loss: 0.000164, lr : 0.008500\n",
      "Epoch: 4190/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 4191/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 4192/25000, Training Loss: 0.000155, lr : 0.008500\n",
      "Epoch: 4193/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 4194/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 4195/25000, Training Loss: 0.000185, lr : 0.008500\n",
      "Epoch: 4196/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 4197/25000, Training Loss: 0.000252, lr : 0.008500\n",
      "Epoch: 4198/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 4199/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 4200/25000, Training Loss: 0.000267, lr : 0.008500\n",
      "Epoch: 4201/25000, Training Loss: 0.000154, lr : 0.008500\n",
      "Epoch: 4202/25000, Training Loss: 0.000273, lr : 0.008500\n",
      "Epoch: 4203/25000, Training Loss: 0.000274, lr : 0.008500\n",
      "Epoch: 4204/25000, Training Loss: 0.000237, lr : 0.008500\n",
      "Epoch: 4205/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 4206/25000, Training Loss: 0.000213, lr : 0.008500\n",
      "Epoch: 4207/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 4208/25000, Training Loss: 0.000213, lr : 0.008500\n",
      "Epoch: 4209/25000, Training Loss: 0.000177, lr : 0.008500\n",
      "Epoch: 4210/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 4211/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 4212/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 4213/25000, Training Loss: 0.000238, lr : 0.008500\n",
      "Epoch: 4214/25000, Training Loss: 0.000211, lr : 0.008500\n",
      "Epoch: 4215/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 4216/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 4217/25000, Training Loss: 0.000158, lr : 0.008500\n",
      "Epoch: 4218/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 4219/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 4220/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 4221/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 4222/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 4223/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 4224/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 4225/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 4226/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 4227/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 4228/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 4229/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 4230/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 4231/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4232/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 4233/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 4234/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 4235/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 4236/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 4237/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 4238/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 4239/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 4240/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 4241/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 4242/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 4243/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 4244/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 4245/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 4246/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 4247/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 4248/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 4249/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 4250/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 4251/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4252/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 4253/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 4254/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 4255/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 4256/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 4257/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 4258/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 4259/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 4260/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 4261/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 4262/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 4263/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 4264/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 4265/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 4266/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 4267/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 4268/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 4269/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 4270/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 4271/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 4272/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 4273/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 4274/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 4275/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 4276/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 4277/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 4278/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 4279/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 4280/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 4281/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 4282/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 4283/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 4284/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 4285/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 4286/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 4287/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 4288/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 4289/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 4290/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 4291/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 4292/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 4293/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 4294/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 4295/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 4296/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 4297/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 4298/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 4299/25000, Training Loss: 0.000359, lr : 0.008500\n",
      "Epoch: 4300/25000, Training Loss: 0.000378, lr : 0.008500\n",
      "Epoch: 4301/25000, Training Loss: 0.000195, lr : 0.008500\n",
      "Epoch: 4302/25000, Training Loss: 0.000261, lr : 0.008500\n",
      "Epoch: 4303/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 4304/25000, Training Loss: 0.000270, lr : 0.008500\n",
      "Epoch: 4305/25000, Training Loss: 0.000251, lr : 0.008500\n",
      "Epoch: 4306/25000, Training Loss: 0.000209, lr : 0.008500\n",
      "Epoch: 4307/25000, Training Loss: 0.000242, lr : 0.008500\n",
      "Epoch: 4308/25000, Training Loss: 0.000187, lr : 0.008500\n",
      "Epoch: 4309/25000, Training Loss: 0.000161, lr : 0.008500\n",
      "Epoch: 4310/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 4311/25000, Training Loss: 0.000151, lr : 0.008500\n",
      "Epoch: 4312/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 4313/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 4314/25000, Training Loss: 0.000208, lr : 0.008500\n",
      "Epoch: 4315/25000, Training Loss: 0.000131, lr : 0.008500\n",
      "Epoch: 4316/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 4317/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 4318/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 4319/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 4320/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 4321/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 4322/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 4323/25000, Training Loss: 0.000161, lr : 0.008500\n",
      "Epoch: 4324/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 4325/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 4326/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 4327/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 4328/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 4329/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 4330/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 4331/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 4332/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 4333/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 4334/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4335/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 4336/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4337/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 4338/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 4339/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 4340/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 4341/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 4342/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 4343/25000, Training Loss: 0.000192, lr : 0.008500\n",
      "Epoch: 4344/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 4345/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 4346/25000, Training Loss: 0.000209, lr : 0.008500\n",
      "Epoch: 4347/25000, Training Loss: 0.000201, lr : 0.008500\n",
      "Epoch: 4348/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 4349/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4350/25000, Training Loss: 0.000158, lr : 0.008500\n",
      "Epoch: 4351/25000, Training Loss: 0.000205, lr : 0.008500\n",
      "Epoch: 4352/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 4353/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 4354/25000, Training Loss: 0.000213, lr : 0.008500\n",
      "Epoch: 4355/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 4356/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 4357/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 4358/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 4359/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 4360/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 4361/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 4362/25000, Training Loss: 0.000178, lr : 0.008500\n",
      "Epoch: 4363/25000, Training Loss: 0.000167, lr : 0.008500\n",
      "Epoch: 4364/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 4365/25000, Training Loss: 0.000234, lr : 0.008500\n",
      "Epoch: 4366/25000, Training Loss: 0.000214, lr : 0.008500\n",
      "Epoch: 4367/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 4368/25000, Training Loss: 0.000319, lr : 0.008500\n",
      "Epoch: 4369/25000, Training Loss: 0.000247, lr : 0.008500\n",
      "Epoch: 4370/25000, Training Loss: 0.000178, lr : 0.008500\n",
      "Epoch: 4371/25000, Training Loss: 0.000371, lr : 0.008500\n",
      "Epoch: 4372/25000, Training Loss: 0.000193, lr : 0.008500\n",
      "Epoch: 4373/25000, Training Loss: 0.000223, lr : 0.008500\n",
      "Epoch: 4374/25000, Training Loss: 0.000242, lr : 0.008500\n",
      "Epoch: 4375/25000, Training Loss: 0.000196, lr : 0.008500\n",
      "Epoch: 4376/25000, Training Loss: 0.000498, lr : 0.008500\n",
      "Epoch: 4377/25000, Training Loss: 0.000279, lr : 0.008500\n",
      "Epoch: 4378/25000, Training Loss: 0.000305, lr : 0.008500\n",
      "Epoch: 4379/25000, Training Loss: 0.000306, lr : 0.008500\n",
      "Epoch: 4380/25000, Training Loss: 0.000302, lr : 0.008500\n",
      "Epoch: 4381/25000, Training Loss: 0.000382, lr : 0.008500\n",
      "Epoch: 4382/25000, Training Loss: 0.000202, lr : 0.008500\n",
      "Epoch: 4383/25000, Training Loss: 0.000382, lr : 0.008500\n",
      "Epoch: 4384/25000, Training Loss: 0.000257, lr : 0.008500\n",
      "Epoch: 4385/25000, Training Loss: 0.000385, lr : 0.008500\n",
      "Epoch: 4386/25000, Training Loss: 0.000385, lr : 0.008500\n",
      "Epoch: 4387/25000, Training Loss: 0.000311, lr : 0.008500\n",
      "Epoch: 4388/25000, Training Loss: 0.000342, lr : 0.008500\n",
      "Epoch: 4389/25000, Training Loss: 0.000309, lr : 0.008500\n",
      "Epoch: 4390/25000, Training Loss: 0.000237, lr : 0.008500\n",
      "Epoch: 4391/25000, Training Loss: 0.000264, lr : 0.008500\n",
      "Epoch: 4392/25000, Training Loss: 0.000229, lr : 0.008500\n",
      "Epoch: 4393/25000, Training Loss: 0.000293, lr : 0.008500\n",
      "Epoch: 4394/25000, Training Loss: 0.000184, lr : 0.008500\n",
      "Epoch: 4395/25000, Training Loss: 0.000163, lr : 0.008500\n",
      "Epoch: 4396/25000, Training Loss: 0.000228, lr : 0.008500\n",
      "Epoch: 4397/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 4398/25000, Training Loss: 0.000177, lr : 0.008500\n",
      "Epoch: 4399/25000, Training Loss: 0.000131, lr : 0.008500\n",
      "Epoch: 4400/25000, Training Loss: 0.000209, lr : 0.008500\n",
      "Epoch: 4401/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 4402/25000, Training Loss: 0.000133, lr : 0.008500\n",
      "Epoch: 4403/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 4404/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 4405/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 4406/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 4407/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 4408/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 4409/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 4410/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4411/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 4412/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 4413/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 4414/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 4415/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 4416/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 4417/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 4418/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 4419/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 4420/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 4421/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 4422/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 4423/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 4424/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 4425/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 4426/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 4427/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 4428/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 4429/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4430/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 4431/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 4432/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 4433/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 4434/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 4435/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 4436/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 4437/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 4438/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 4439/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 4440/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 4441/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 4442/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4443/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 4444/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 4445/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 4446/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 4447/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 4448/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 4449/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 4450/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 4451/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 4452/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 4453/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 4454/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 4455/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 4456/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 4457/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 4458/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 4459/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 4460/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 4461/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 4462/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 4463/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 4464/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 4465/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 4466/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 4467/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 4468/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 4469/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 4470/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4471/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 4472/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 4473/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 4474/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 4475/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 4476/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 4477/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 4478/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 4479/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 4480/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 4481/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 4482/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 4483/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 4484/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 4485/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 4486/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 4487/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 4488/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 4489/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 4490/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 4491/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 4492/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 4493/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 4494/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 4495/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 4496/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 4497/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 4498/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 4499/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 4500/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 4501/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 4502/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 4503/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 4504/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 4505/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 4506/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 4507/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 4508/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 4509/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 4510/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 4511/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 4512/25000, Training Loss: 0.000164, lr : 0.008500\n",
      "Epoch: 4513/25000, Training Loss: 0.000166, lr : 0.008500\n",
      "Epoch: 4514/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 4515/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 4516/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 4517/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 4518/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 4519/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 4520/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 4521/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 4522/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 4523/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 4524/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 4525/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 4526/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 4527/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 4528/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 4529/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 4530/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 4531/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 4532/25000, Training Loss: 0.000202, lr : 0.008500\n",
      "Epoch: 4533/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 4534/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 4535/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 4536/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 4537/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 4538/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 4539/25000, Training Loss: 0.000159, lr : 0.008500\n",
      "Epoch: 4540/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 4541/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 4542/25000, Training Loss: 0.000182, lr : 0.008500\n",
      "Epoch: 4543/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 4544/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 4545/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 4546/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 4547/25000, Training Loss: 0.000202, lr : 0.008500\n",
      "Epoch: 4548/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 4549/25000, Training Loss: 0.000318, lr : 0.008500\n",
      "Epoch: 4550/25000, Training Loss: 0.000321, lr : 0.008500\n",
      "Epoch: 4551/25000, Training Loss: 0.000276, lr : 0.008500\n",
      "Epoch: 4552/25000, Training Loss: 0.000278, lr : 0.008500\n",
      "Epoch: 4553/25000, Training Loss: 0.000254, lr : 0.008500\n",
      "Epoch: 4554/25000, Training Loss: 0.000210, lr : 0.008500\n",
      "Epoch: 4555/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 4556/25000, Training Loss: 0.000154, lr : 0.008500\n",
      "Epoch: 4557/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 4558/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 4559/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 4560/25000, Training Loss: 0.000133, lr : 0.008500\n",
      "Epoch: 4561/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 4562/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 4563/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 4564/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 4565/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 4566/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 4567/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 4568/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 4569/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 4570/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 4571/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 4572/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 4573/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 4574/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 4575/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 4576/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 4577/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 4578/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 4579/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 4580/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 4581/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 4582/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 4583/25000, Training Loss: 0.000178, lr : 0.008500\n",
      "Epoch: 4584/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 4585/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 4586/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 4587/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 4588/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 4589/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 4590/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 4591/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 4592/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 4593/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 4594/25000, Training Loss: 0.000133, lr : 0.008500\n",
      "Epoch: 4595/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 4596/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 4597/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 4598/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 4599/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 4600/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 4601/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 4602/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 4603/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 4604/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 4605/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 4606/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 4607/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 4608/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 4609/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 4610/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 4611/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 4612/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 4613/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 4614/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 4615/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 4616/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 4617/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 4618/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4619/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 4620/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 4621/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 4622/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 4623/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 4624/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4625/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 4626/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 4627/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 4628/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 4629/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 4630/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 4631/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 4632/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 4633/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 4634/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 4635/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 4636/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 4637/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 4638/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 4639/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 4640/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 4641/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 4642/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 4643/25000, Training Loss: 0.000207, lr : 0.008500\n",
      "Epoch: 4644/25000, Training Loss: 0.000283, lr : 0.008500\n",
      "Epoch: 4645/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 4646/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 4647/25000, Training Loss: 0.000172, lr : 0.008500\n",
      "Epoch: 4648/25000, Training Loss: 0.000151, lr : 0.008500\n",
      "Epoch: 4649/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4650/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 4651/25000, Training Loss: 0.000219, lr : 0.008500\n",
      "Epoch: 4652/25000, Training Loss: 0.000274, lr : 0.008500\n",
      "Epoch: 4653/25000, Training Loss: 0.000196, lr : 0.008500\n",
      "Epoch: 4654/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 4655/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 4656/25000, Training Loss: 0.000163, lr : 0.008500\n",
      "Epoch: 4657/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 4658/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 4659/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 4660/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 4661/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 4662/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 4663/25000, Training Loss: 0.000192, lr : 0.008500\n",
      "Epoch: 4664/25000, Training Loss: 0.000178, lr : 0.008500\n",
      "Epoch: 4665/25000, Training Loss: 0.000164, lr : 0.008500\n",
      "Epoch: 4666/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 4667/25000, Training Loss: 0.000210, lr : 0.008500\n",
      "Epoch: 4668/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 4669/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 4670/25000, Training Loss: 0.000188, lr : 0.008500\n",
      "Epoch: 4671/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 4672/25000, Training Loss: 0.000172, lr : 0.008500\n",
      "Epoch: 4673/25000, Training Loss: 0.000223, lr : 0.008500\n",
      "Epoch: 4674/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 4675/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 4676/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 4677/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 4678/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 4679/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 4680/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 4681/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 4682/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 4683/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 4684/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 4685/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 4686/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 4687/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 4688/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 4689/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 4690/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 4691/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 4692/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 4693/25000, Training Loss: 0.000170, lr : 0.008500\n",
      "Epoch: 4694/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 4695/25000, Training Loss: 0.000245, lr : 0.008500\n",
      "Epoch: 4696/25000, Training Loss: 0.000175, lr : 0.008500\n",
      "Epoch: 4697/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 4698/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 4699/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 4700/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 4701/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 4702/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4703/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 4704/25000, Training Loss: 0.000253, lr : 0.008500\n",
      "Epoch: 4705/25000, Training Loss: 0.000296, lr : 0.008500\n",
      "Epoch: 4706/25000, Training Loss: 0.000229, lr : 0.008500\n",
      "Epoch: 4707/25000, Training Loss: 0.000155, lr : 0.008500\n",
      "Epoch: 4708/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 4709/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 4710/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 4711/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 4712/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 4713/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 4714/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 4715/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 4716/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 4717/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 4718/25000, Training Loss: 0.000270, lr : 0.008500\n",
      "Epoch: 4719/25000, Training Loss: 0.000292, lr : 0.008500\n",
      "Epoch: 4720/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 4721/25000, Training Loss: 0.000306, lr : 0.008500\n",
      "Epoch: 4722/25000, Training Loss: 0.000309, lr : 0.008500\n",
      "Epoch: 4723/25000, Training Loss: 0.000159, lr : 0.008500\n",
      "Epoch: 4724/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 4725/25000, Training Loss: 0.000229, lr : 0.008500\n",
      "Epoch: 4726/25000, Training Loss: 0.000267, lr : 0.008500\n",
      "Epoch: 4727/25000, Training Loss: 0.000192, lr : 0.008500\n",
      "Epoch: 4728/25000, Training Loss: 0.000325, lr : 0.008500\n",
      "Epoch: 4729/25000, Training Loss: 0.000610, lr : 0.008500\n",
      "Epoch: 4730/25000, Training Loss: 0.000274, lr : 0.008500\n",
      "Epoch: 4731/25000, Training Loss: 0.000413, lr : 0.008500\n",
      "Epoch: 4732/25000, Training Loss: 0.000616, lr : 0.008500\n",
      "Epoch: 4733/25000, Training Loss: 0.000309, lr : 0.008500\n",
      "Epoch: 4734/25000, Training Loss: 0.000246, lr : 0.008500\n",
      "Epoch: 4735/25000, Training Loss: 0.000222, lr : 0.008500\n",
      "Epoch: 4736/25000, Training Loss: 0.000244, lr : 0.008500\n",
      "Epoch: 4737/25000, Training Loss: 0.000218, lr : 0.008500\n",
      "Epoch: 4738/25000, Training Loss: 0.000206, lr : 0.008500\n",
      "Epoch: 4739/25000, Training Loss: 0.000242, lr : 0.008500\n",
      "Epoch: 4740/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 4741/25000, Training Loss: 0.000263, lr : 0.008500\n",
      "Epoch: 4742/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 4743/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 4744/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 4745/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4746/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 4747/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 4748/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 4749/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 4750/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 4751/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 4752/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 4753/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 4754/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 4755/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 4756/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 4757/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 4758/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 4759/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 4760/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 4761/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 4762/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 4763/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 4764/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 4765/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 4766/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 4767/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 4768/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 4769/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4770/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4771/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 4772/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 4773/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 4774/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 4775/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 4776/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 4777/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 4778/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4779/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 4780/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4781/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 4782/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 4783/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 4784/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 4785/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 4786/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 4787/25000, Training Loss: 0.000224, lr : 0.008500\n",
      "Epoch: 4788/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 4789/25000, Training Loss: 0.000161, lr : 0.008500\n",
      "Epoch: 4790/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 4791/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 4792/25000, Training Loss: 0.000175, lr : 0.008500\n",
      "Epoch: 4793/25000, Training Loss: 0.000170, lr : 0.008500\n",
      "Epoch: 4794/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 4795/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 4796/25000, Training Loss: 0.000177, lr : 0.008500\n",
      "Epoch: 4797/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 4798/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 4799/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4800/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 4801/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 4802/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 4803/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 4804/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 4805/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 4806/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 4807/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 4808/25000, Training Loss: 0.000206, lr : 0.008500\n",
      "Epoch: 4809/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 4810/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 4811/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 4812/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 4813/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 4814/25000, Training Loss: 0.000215, lr : 0.008500\n",
      "Epoch: 4815/25000, Training Loss: 0.000179, lr : 0.008500\n",
      "Epoch: 4816/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 4817/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 4818/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 4819/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 4820/25000, Training Loss: 0.000211, lr : 0.008500\n",
      "Epoch: 4821/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 4822/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 4823/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 4824/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 4825/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 4826/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 4827/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 4828/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 4829/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 4830/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 4831/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 4832/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 4833/25000, Training Loss: 0.000188, lr : 0.008500\n",
      "Epoch: 4834/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 4835/25000, Training Loss: 0.000167, lr : 0.008500\n",
      "Epoch: 4836/25000, Training Loss: 0.000462, lr : 0.008500\n",
      "Epoch: 4837/25000, Training Loss: 0.000206, lr : 0.008500\n",
      "Epoch: 4838/25000, Training Loss: 0.000460, lr : 0.008500\n",
      "Epoch: 4839/25000, Training Loss: 0.000404, lr : 0.008500\n",
      "Epoch: 4840/25000, Training Loss: 0.000253, lr : 0.008500\n",
      "Epoch: 4841/25000, Training Loss: 0.000453, lr : 0.008500\n",
      "Epoch: 4842/25000, Training Loss: 0.000214, lr : 0.008500\n",
      "Epoch: 4843/25000, Training Loss: 0.000410, lr : 0.008500\n",
      "Epoch: 4844/25000, Training Loss: 0.000226, lr : 0.008500\n",
      "Epoch: 4845/25000, Training Loss: 0.000244, lr : 0.008500\n",
      "Epoch: 4846/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 4847/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 4848/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 4849/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4850/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 4851/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 4852/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4853/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 4854/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 4855/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 4856/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 4857/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 4858/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 4859/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 4860/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 4861/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 4862/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 4863/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 4864/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 4865/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 4866/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 4867/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 4868/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 4869/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 4870/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 4871/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 4872/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 4873/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 4874/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 4875/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 4876/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 4877/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 4878/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 4879/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 4880/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 4881/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 4882/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 4883/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 4884/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 4885/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 4886/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4887/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 4888/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 4889/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 4890/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 4891/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 4892/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 4893/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 4894/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 4895/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4896/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 4897/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 4898/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 4899/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4900/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 4901/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 4902/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 4903/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 4904/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 4905/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 4906/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 4907/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 4908/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 4909/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 4910/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 4911/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 4912/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 4913/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 4914/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 4915/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 4916/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 4917/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 4918/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 4919/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 4920/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 4921/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 4922/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 4923/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 4924/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 4925/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 4926/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 4927/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 4928/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 4929/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 4930/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 4931/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 4932/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 4933/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 4934/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 4935/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 4936/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 4937/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 4938/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 4939/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 4940/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 4941/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 4942/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 4943/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 4944/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 4945/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 4946/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 4947/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 4948/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 4949/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 4950/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 4951/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 4952/25000, Training Loss: 0.000212, lr : 0.008500\n",
      "Epoch: 4953/25000, Training Loss: 0.000236, lr : 0.008500\n",
      "Epoch: 4954/25000, Training Loss: 0.000220, lr : 0.008500\n",
      "Epoch: 4955/25000, Training Loss: 0.000344, lr : 0.008500\n",
      "Epoch: 4956/25000, Training Loss: 0.000309, lr : 0.008500\n",
      "Epoch: 4957/25000, Training Loss: 0.000223, lr : 0.008500\n",
      "Epoch: 4958/25000, Training Loss: 0.000245, lr : 0.008500\n",
      "Epoch: 4959/25000, Training Loss: 0.000192, lr : 0.008500\n",
      "Epoch: 4960/25000, Training Loss: 0.000184, lr : 0.008500\n",
      "Epoch: 4961/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 4962/25000, Training Loss: 0.000196, lr : 0.008500\n",
      "Epoch: 4963/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 4964/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 4965/25000, Training Loss: 0.000184, lr : 0.008500\n",
      "Epoch: 4966/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 4967/25000, Training Loss: 0.000224, lr : 0.008500\n",
      "Epoch: 4968/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 4969/25000, Training Loss: 0.000163, lr : 0.008500\n",
      "Epoch: 4970/25000, Training Loss: 0.000224, lr : 0.008500\n",
      "Epoch: 4971/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 4972/25000, Training Loss: 0.000264, lr : 0.008500\n",
      "Epoch: 4973/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 4974/25000, Training Loss: 0.000155, lr : 0.008500\n",
      "Epoch: 4975/25000, Training Loss: 0.000187, lr : 0.008500\n",
      "Epoch: 4976/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 4977/25000, Training Loss: 0.000175, lr : 0.008500\n",
      "Epoch: 4978/25000, Training Loss: 0.000205, lr : 0.008500\n",
      "Epoch: 4979/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 4980/25000, Training Loss: 0.000155, lr : 0.008500\n",
      "Epoch: 4981/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 4982/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 4983/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 4984/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 4985/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 4986/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 4987/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 4988/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 4989/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 4990/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 4991/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 4992/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 4993/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 4994/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 4995/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 4996/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 4997/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 4998/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 4999/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 5000/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 5001/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 5002/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 5003/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5004/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 5005/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5006/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 5007/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 5008/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 5009/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 5010/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 5011/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 5012/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 5013/25000, Training Loss: 0.000170, lr : 0.008500\n",
      "Epoch: 5014/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 5015/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 5016/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5017/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 5018/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 5019/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 5020/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 5021/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 5022/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 5023/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 5024/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 5025/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 5026/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5027/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 5028/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 5029/25000, Training Loss: 0.000193, lr : 0.008500\n",
      "Epoch: 5030/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 5031/25000, Training Loss: 0.000158, lr : 0.008500\n",
      "Epoch: 5032/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 5033/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 5034/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 5035/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 5036/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 5037/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 5038/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 5039/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 5040/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 5041/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 5042/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 5043/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 5044/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5045/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 5046/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5047/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 5048/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 5049/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 5050/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 5051/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 5052/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5053/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 5054/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 5055/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 5056/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 5057/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5058/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 5059/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 5060/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 5061/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 5062/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 5063/25000, Training Loss: 0.000207, lr : 0.008500\n",
      "Epoch: 5064/25000, Training Loss: 0.000188, lr : 0.008500\n",
      "Epoch: 5065/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 5066/25000, Training Loss: 0.000229, lr : 0.008500\n",
      "Epoch: 5067/25000, Training Loss: 0.000328, lr : 0.008500\n",
      "Epoch: 5068/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 5069/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 5070/25000, Training Loss: 0.000284, lr : 0.008500\n",
      "Epoch: 5071/25000, Training Loss: 0.000284, lr : 0.008500\n",
      "Epoch: 5072/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 5073/25000, Training Loss: 0.000314, lr : 0.008500\n",
      "Epoch: 5074/25000, Training Loss: 0.000292, lr : 0.008500\n",
      "Epoch: 5075/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 5076/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 5077/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 5078/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 5079/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 5080/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 5081/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 5082/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 5083/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 5084/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 5085/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 5086/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 5087/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5088/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5089/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 5090/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 5091/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5092/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 5093/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 5094/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 5095/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5096/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 5097/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 5098/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 5099/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5100/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 5101/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5102/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 5103/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 5104/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 5105/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 5106/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 5107/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 5108/25000, Training Loss: 0.000133, lr : 0.008500\n",
      "Epoch: 5109/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 5110/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 5111/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5112/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 5113/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5114/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 5115/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 5116/25000, Training Loss: 0.000161, lr : 0.008500\n",
      "Epoch: 5117/25000, Training Loss: 0.000230, lr : 0.008500\n",
      "Epoch: 5118/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 5119/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 5120/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 5121/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 5122/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 5123/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 5124/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 5125/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5126/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5127/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 5128/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 5129/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 5130/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 5131/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 5132/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 5133/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 5134/25000, Training Loss: 0.000166, lr : 0.008500\n",
      "Epoch: 5135/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 5136/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 5137/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 5138/25000, Training Loss: 0.000177, lr : 0.008500\n",
      "Epoch: 5139/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 5140/25000, Training Loss: 0.000167, lr : 0.008500\n",
      "Epoch: 5141/25000, Training Loss: 0.000217, lr : 0.008500\n",
      "Epoch: 5142/25000, Training Loss: 0.000250, lr : 0.008500\n",
      "Epoch: 5143/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 5144/25000, Training Loss: 0.000161, lr : 0.008500\n",
      "Epoch: 5145/25000, Training Loss: 0.000133, lr : 0.008500\n",
      "Epoch: 5146/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 5147/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 5148/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 5149/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 5150/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 5151/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 5152/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 5153/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 5154/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 5155/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 5156/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 5157/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 5158/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 5159/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 5160/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5161/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 5162/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5163/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 5164/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 5165/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 5166/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5167/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 5168/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 5169/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 5170/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 5171/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 5172/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 5173/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 5174/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 5175/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 5176/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 5177/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 5178/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 5179/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5180/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 5181/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 5182/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5183/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5184/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5185/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 5186/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 5187/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 5188/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 5189/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 5190/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5191/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 5192/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 5193/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 5194/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 5195/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 5196/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5197/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 5198/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5199/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5200/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 5201/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5202/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 5203/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 5204/25000, Training Loss: 0.000189, lr : 0.008500\n",
      "Epoch: 5205/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 5206/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 5207/25000, Training Loss: 0.000343, lr : 0.008500\n",
      "Epoch: 5208/25000, Training Loss: 0.000293, lr : 0.008500\n",
      "Epoch: 5209/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 5210/25000, Training Loss: 0.000243, lr : 0.008500\n",
      "Epoch: 5211/25000, Training Loss: 0.000194, lr : 0.008500\n",
      "Epoch: 5212/25000, Training Loss: 0.000164, lr : 0.008500\n",
      "Epoch: 5213/25000, Training Loss: 0.000318, lr : 0.008500\n",
      "Epoch: 5214/25000, Training Loss: 0.000346, lr : 0.008500\n",
      "Epoch: 5215/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 5216/25000, Training Loss: 0.000264, lr : 0.008500\n",
      "Epoch: 5217/25000, Training Loss: 0.000542, lr : 0.008500\n",
      "Epoch: 5218/25000, Training Loss: 0.000209, lr : 0.008500\n",
      "Epoch: 5219/25000, Training Loss: 0.000426, lr : 0.008500\n",
      "Epoch: 5220/25000, Training Loss: 0.000350, lr : 0.008500\n",
      "Epoch: 5221/25000, Training Loss: 0.000268, lr : 0.008500\n",
      "Epoch: 5222/25000, Training Loss: 0.000427, lr : 0.008500\n",
      "Epoch: 5223/25000, Training Loss: 0.000435, lr : 0.008500\n",
      "Epoch: 5224/25000, Training Loss: 0.000336, lr : 0.008500\n",
      "Epoch: 5225/25000, Training Loss: 0.000433, lr : 0.008500\n",
      "Epoch: 5226/25000, Training Loss: 0.000292, lr : 0.008500\n",
      "Epoch: 5227/25000, Training Loss: 0.000308, lr : 0.008500\n",
      "Epoch: 5228/25000, Training Loss: 0.000378, lr : 0.008500\n",
      "Epoch: 5229/25000, Training Loss: 0.000263, lr : 0.008500\n",
      "Epoch: 5230/25000, Training Loss: 0.000365, lr : 0.008500\n",
      "Epoch: 5231/25000, Training Loss: 0.000330, lr : 0.008500\n",
      "Epoch: 5232/25000, Training Loss: 0.000325, lr : 0.008500\n",
      "Epoch: 5233/25000, Training Loss: 0.000267, lr : 0.008500\n",
      "Epoch: 5234/25000, Training Loss: 0.000198, lr : 0.008500\n",
      "Epoch: 5235/25000, Training Loss: 0.000259, lr : 0.008500\n",
      "Epoch: 5236/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 5237/25000, Training Loss: 0.000177, lr : 0.008500\n",
      "Epoch: 5238/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 5239/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 5240/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 5241/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 5242/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 5243/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 5244/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 5245/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 5246/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 5247/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 5248/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 5249/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 5250/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 5251/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 5252/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 5253/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 5254/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 5255/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 5256/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 5257/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 5258/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 5259/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 5260/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 5261/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 5262/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 5263/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 5264/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 5265/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 5266/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 5267/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 5268/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 5269/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 5270/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 5271/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 5272/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 5273/25000, Training Loss: 0.000239, lr : 0.008500\n",
      "Epoch: 5274/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 5275/25000, Training Loss: 0.000192, lr : 0.008500\n",
      "Epoch: 5276/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 5277/25000, Training Loss: 0.000170, lr : 0.008500\n",
      "Epoch: 5278/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 5279/25000, Training Loss: 0.000133, lr : 0.008500\n",
      "Epoch: 5280/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 5281/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 5282/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 5283/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 5284/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 5285/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 5286/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 5287/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 5288/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 5289/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 5290/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 5291/25000, Training Loss: 0.000172, lr : 0.008500\n",
      "Epoch: 5292/25000, Training Loss: 0.000228, lr : 0.008500\n",
      "Epoch: 5293/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 5294/25000, Training Loss: 0.000163, lr : 0.008500\n",
      "Epoch: 5295/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 5296/25000, Training Loss: 0.000170, lr : 0.008500\n",
      "Epoch: 5297/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 5298/25000, Training Loss: 0.000178, lr : 0.008500\n",
      "Epoch: 5299/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 5300/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 5301/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 5302/25000, Training Loss: 0.000268, lr : 0.008500\n",
      "Epoch: 5303/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 5304/25000, Training Loss: 0.000195, lr : 0.008500\n",
      "Epoch: 5305/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 5306/25000, Training Loss: 0.000204, lr : 0.008500\n",
      "Epoch: 5307/25000, Training Loss: 0.000195, lr : 0.008500\n",
      "Epoch: 5308/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 5309/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 5310/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 5311/25000, Training Loss: 0.000286, lr : 0.008500\n",
      "Epoch: 5312/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 5313/25000, Training Loss: 0.000328, lr : 0.008500\n",
      "Epoch: 5314/25000, Training Loss: 0.000226, lr : 0.008500\n",
      "Epoch: 5315/25000, Training Loss: 0.000178, lr : 0.008500\n",
      "Epoch: 5316/25000, Training Loss: 0.000155, lr : 0.008500\n",
      "Epoch: 5317/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 5318/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 5319/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 5320/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 5321/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 5322/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 5323/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 5324/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 5325/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 5326/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 5327/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 5328/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 5329/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 5330/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 5331/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 5332/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 5333/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 5334/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 5335/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 5336/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 5337/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5338/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 5339/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 5340/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 5341/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 5342/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 5343/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 5344/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5345/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 5346/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5347/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 5348/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5349/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 5350/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 5351/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 5352/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5353/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5354/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 5355/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 5356/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 5357/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 5358/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 5359/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5360/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 5361/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 5362/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 5363/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 5364/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 5365/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 5366/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 5367/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 5368/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5369/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 5370/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 5371/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5372/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 5373/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 5374/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 5375/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 5376/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 5377/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 5378/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 5379/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 5380/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 5381/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 5382/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5383/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5384/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 5385/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 5386/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 5387/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5388/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 5389/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 5390/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 5391/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5392/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 5393/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 5394/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 5395/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 5396/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 5397/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 5398/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 5399/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 5400/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 5401/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 5402/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 5403/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 5404/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 5405/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5406/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 5407/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 5408/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 5409/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 5410/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 5411/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 5412/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5413/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 5414/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 5415/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5416/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 5417/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 5418/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 5419/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 5420/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 5421/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 5422/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 5423/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 5424/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 5425/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 5426/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 5427/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 5428/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5429/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 5430/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 5431/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 5432/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 5433/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 5434/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 5435/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 5436/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 5437/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5438/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5439/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 5440/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 5441/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 5442/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 5443/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5444/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 5445/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 5446/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5447/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 5448/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 5449/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 5450/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 5451/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 5452/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5453/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 5454/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 5455/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5456/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5457/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 5458/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 5459/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 5460/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 5461/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 5462/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 5463/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 5464/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 5465/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 5466/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 5467/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 5468/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 5469/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 5470/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 5471/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 5472/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5473/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 5474/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 5475/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 5476/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 5477/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 5478/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 5479/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 5480/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 5481/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 5482/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 5483/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 5484/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5485/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 5486/25000, Training Loss: 0.000186, lr : 0.008500\n",
      "Epoch: 5487/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 5488/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 5489/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 5490/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 5491/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 5492/25000, Training Loss: 0.000182, lr : 0.008500\n",
      "Epoch: 5493/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 5494/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 5495/25000, Training Loss: 0.000311, lr : 0.008500\n",
      "Epoch: 5496/25000, Training Loss: 0.000386, lr : 0.008500\n",
      "Epoch: 5497/25000, Training Loss: 0.000221, lr : 0.008500\n",
      "Epoch: 5498/25000, Training Loss: 0.000256, lr : 0.008500\n",
      "Epoch: 5499/25000, Training Loss: 0.000208, lr : 0.008500\n",
      "Epoch: 5500/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 5501/25000, Training Loss: 0.000210, lr : 0.008500\n",
      "Epoch: 5502/25000, Training Loss: 0.000296, lr : 0.008500\n",
      "Epoch: 5503/25000, Training Loss: 0.000192, lr : 0.008500\n",
      "Epoch: 5504/25000, Training Loss: 0.000338, lr : 0.008500\n",
      "Epoch: 5505/25000, Training Loss: 0.000207, lr : 0.008500\n",
      "Epoch: 5506/25000, Training Loss: 0.000188, lr : 0.008500\n",
      "Epoch: 5507/25000, Training Loss: 0.000237, lr : 0.008500\n",
      "Epoch: 5508/25000, Training Loss: 0.000302, lr : 0.008500\n",
      "Epoch: 5509/25000, Training Loss: 0.000133, lr : 0.008500\n",
      "Epoch: 5510/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 5511/25000, Training Loss: 0.000170, lr : 0.008500\n",
      "Epoch: 5512/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 5513/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 5514/25000, Training Loss: 0.000249, lr : 0.008500\n",
      "Epoch: 5515/25000, Training Loss: 0.000188, lr : 0.008500\n",
      "Epoch: 5516/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 5517/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 5518/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 5519/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 5520/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 5521/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 5522/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 5523/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 5524/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5525/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 5526/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 5527/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 5528/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 5529/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5530/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 5531/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 5532/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 5533/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 5534/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5535/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5536/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 5537/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 5538/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 5539/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 5540/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 5541/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 5542/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 5543/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 5544/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 5545/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 5546/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 5547/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 5548/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 5549/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 5550/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 5551/25000, Training Loss: 0.000151, lr : 0.008500\n",
      "Epoch: 5552/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 5553/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 5554/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 5555/25000, Training Loss: 0.000179, lr : 0.008500\n",
      "Epoch: 5556/25000, Training Loss: 0.000161, lr : 0.008500\n",
      "Epoch: 5557/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 5558/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 5559/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 5560/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 5561/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 5562/25000, Training Loss: 0.000155, lr : 0.008500\n",
      "Epoch: 5563/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5564/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 5565/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 5566/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 5567/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 5568/25000, Training Loss: 0.000207, lr : 0.008500\n",
      "Epoch: 5569/25000, Training Loss: 0.000133, lr : 0.008500\n",
      "Epoch: 5570/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 5571/25000, Training Loss: 0.000206, lr : 0.008500\n",
      "Epoch: 5572/25000, Training Loss: 0.000244, lr : 0.008500\n",
      "Epoch: 5573/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 5574/25000, Training Loss: 0.000196, lr : 0.008500\n",
      "Epoch: 5575/25000, Training Loss: 0.000154, lr : 0.008500\n",
      "Epoch: 5576/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 5577/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 5578/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 5579/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 5580/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 5581/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 5582/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 5583/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 5584/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 5585/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 5586/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 5587/25000, Training Loss: 0.000238, lr : 0.008500\n",
      "Epoch: 5588/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 5589/25000, Training Loss: 0.000270, lr : 0.008500\n",
      "Epoch: 5590/25000, Training Loss: 0.000154, lr : 0.008500\n",
      "Epoch: 5591/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 5592/25000, Training Loss: 0.000207, lr : 0.008500\n",
      "Epoch: 5593/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 5594/25000, Training Loss: 0.000269, lr : 0.008500\n",
      "Epoch: 5595/25000, Training Loss: 0.000268, lr : 0.008500\n",
      "Epoch: 5596/25000, Training Loss: 0.000204, lr : 0.008500\n",
      "Epoch: 5597/25000, Training Loss: 0.000290, lr : 0.008500\n",
      "Epoch: 5598/25000, Training Loss: 0.000239, lr : 0.008500\n",
      "Epoch: 5599/25000, Training Loss: 0.000190, lr : 0.008500\n",
      "Epoch: 5600/25000, Training Loss: 0.000278, lr : 0.008500\n",
      "Epoch: 5601/25000, Training Loss: 0.000312, lr : 0.008500\n",
      "Epoch: 5602/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 5603/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 5604/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 5605/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 5606/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 5607/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 5608/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 5609/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 5610/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 5611/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5612/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 5613/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 5614/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5615/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5616/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 5617/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 5618/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5619/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5620/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5621/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 5622/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 5623/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5624/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5625/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 5626/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 5627/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 5628/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5629/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 5630/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 5631/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 5632/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 5633/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 5634/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 5635/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 5636/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 5637/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5638/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 5639/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 5640/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 5641/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 5642/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 5643/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 5644/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5645/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5646/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 5647/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 5648/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 5649/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 5650/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5651/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 5652/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 5653/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 5654/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 5655/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 5656/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 5657/25000, Training Loss: 0.000185, lr : 0.008500\n",
      "Epoch: 5658/25000, Training Loss: 0.000245, lr : 0.008500\n",
      "Epoch: 5659/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 5660/25000, Training Loss: 0.000179, lr : 0.008500\n",
      "Epoch: 5661/25000, Training Loss: 0.000225, lr : 0.008500\n",
      "Epoch: 5662/25000, Training Loss: 0.000164, lr : 0.008500\n",
      "Epoch: 5663/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 5664/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 5665/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 5666/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 5667/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 5668/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 5669/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 5670/25000, Training Loss: 0.000202, lr : 0.008500\n",
      "Epoch: 5671/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 5672/25000, Training Loss: 0.000179, lr : 0.008500\n",
      "Epoch: 5673/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 5674/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 5675/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 5676/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 5677/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 5678/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 5679/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 5680/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5681/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 5682/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 5683/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 5684/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 5685/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 5686/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 5687/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 5688/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 5689/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 5690/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 5691/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5692/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5693/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 5694/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 5695/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 5696/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 5697/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 5698/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5699/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5700/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 5701/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 5702/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 5703/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 5704/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 5705/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 5706/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 5707/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 5708/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 5709/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 5710/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 5711/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 5712/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 5713/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 5714/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 5715/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 5716/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 5717/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 5718/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 5719/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 5720/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 5721/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 5722/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 5723/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5724/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 5725/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 5726/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 5727/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 5728/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 5729/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 5730/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 5731/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5732/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 5733/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 5734/25000, Training Loss: 0.000197, lr : 0.008500\n",
      "Epoch: 5735/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 5736/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5737/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 5738/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 5739/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 5740/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 5741/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 5742/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 5743/25000, Training Loss: 0.000197, lr : 0.008500\n",
      "Epoch: 5744/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 5745/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 5746/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 5747/25000, Training Loss: 0.000198, lr : 0.008500\n",
      "Epoch: 5748/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 5749/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 5750/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 5751/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 5752/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 5753/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 5754/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 5755/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5756/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 5757/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 5758/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 5759/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 5760/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 5761/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5762/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5763/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 5764/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 5765/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 5766/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 5767/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 5768/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 5769/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 5770/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 5771/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 5772/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 5773/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5774/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 5775/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 5776/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 5777/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 5778/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 5779/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 5780/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 5781/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 5782/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 5783/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 5784/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 5785/25000, Training Loss: 0.000193, lr : 0.008500\n",
      "Epoch: 5786/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 5787/25000, Training Loss: 0.000186, lr : 0.008500\n",
      "Epoch: 5788/25000, Training Loss: 0.000202, lr : 0.008500\n",
      "Epoch: 5789/25000, Training Loss: 0.000214, lr : 0.008500\n",
      "Epoch: 5790/25000, Training Loss: 0.000184, lr : 0.008500\n",
      "Epoch: 5791/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 5792/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 5793/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 5794/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 5795/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 5796/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 5797/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 5798/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 5799/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 5800/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 5801/25000, Training Loss: 0.000175, lr : 0.008500\n",
      "Epoch: 5802/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 5803/25000, Training Loss: 0.000250, lr : 0.008500\n",
      "Epoch: 5804/25000, Training Loss: 0.000212, lr : 0.008500\n",
      "Epoch: 5805/25000, Training Loss: 0.000190, lr : 0.008500\n",
      "Epoch: 5806/25000, Training Loss: 0.000237, lr : 0.008500\n",
      "Epoch: 5807/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 5808/25000, Training Loss: 0.000265, lr : 0.008500\n",
      "Epoch: 5809/25000, Training Loss: 0.000270, lr : 0.008500\n",
      "Epoch: 5810/25000, Training Loss: 0.000290, lr : 0.008500\n",
      "Epoch: 5811/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 5812/25000, Training Loss: 0.000261, lr : 0.008500\n",
      "Epoch: 5813/25000, Training Loss: 0.000195, lr : 0.008500\n",
      "Epoch: 5814/25000, Training Loss: 0.000188, lr : 0.008500\n",
      "Epoch: 5815/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 5816/25000, Training Loss: 0.000235, lr : 0.008500\n",
      "Epoch: 5817/25000, Training Loss: 0.000229, lr : 0.008500\n",
      "Epoch: 5818/25000, Training Loss: 0.000166, lr : 0.008500\n",
      "Epoch: 5819/25000, Training Loss: 0.000206, lr : 0.008500\n",
      "Epoch: 5820/25000, Training Loss: 0.000186, lr : 0.008500\n",
      "Epoch: 5821/25000, Training Loss: 0.000198, lr : 0.008500\n",
      "Epoch: 5822/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 5823/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 5824/25000, Training Loss: 0.000131, lr : 0.008500\n",
      "Epoch: 5825/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 5826/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 5827/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 5828/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 5829/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 5830/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 5831/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 5832/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 5833/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 5834/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 5835/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 5836/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 5837/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 5838/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5839/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 5840/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 5841/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 5842/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 5843/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 5844/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5845/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 5846/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 5847/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 5848/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 5849/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 5850/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 5851/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 5852/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 5853/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 5854/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5855/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 5856/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 5857/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 5858/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 5859/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 5860/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 5861/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 5862/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 5863/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 5864/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 5865/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 5866/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 5867/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 5868/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 5869/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 5870/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 5871/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 5872/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 5873/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 5874/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 5875/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 5876/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 5877/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 5878/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 5879/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 5880/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 5881/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 5882/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5883/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 5884/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 5885/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 5886/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 5887/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 5888/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 5889/25000, Training Loss: 0.000185, lr : 0.008500\n",
      "Epoch: 5890/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 5891/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 5892/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 5893/25000, Training Loss: 0.000192, lr : 0.008500\n",
      "Epoch: 5894/25000, Training Loss: 0.000334, lr : 0.008500\n",
      "Epoch: 5895/25000, Training Loss: 0.000249, lr : 0.008500\n",
      "Epoch: 5896/25000, Training Loss: 0.000206, lr : 0.008500\n",
      "Epoch: 5897/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 5898/25000, Training Loss: 0.000164, lr : 0.008500\n",
      "Epoch: 5899/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 5900/25000, Training Loss: 0.000306, lr : 0.008500\n",
      "Epoch: 5901/25000, Training Loss: 0.000245, lr : 0.008500\n",
      "Epoch: 5902/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 5903/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 5904/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 5905/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 5906/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 5907/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 5908/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5909/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 5910/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 5911/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 5912/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 5913/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 5914/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 5915/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 5916/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 5917/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 5918/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 5919/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 5920/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 5921/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 5922/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 5923/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 5924/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 5925/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 5926/25000, Training Loss: 0.000170, lr : 0.008500\n",
      "Epoch: 5927/25000, Training Loss: 0.000212, lr : 0.008500\n",
      "Epoch: 5928/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 5929/25000, Training Loss: 0.000197, lr : 0.008500\n",
      "Epoch: 5930/25000, Training Loss: 0.000541, lr : 0.008500\n",
      "Epoch: 5931/25000, Training Loss: 0.000300, lr : 0.008500\n",
      "Epoch: 5932/25000, Training Loss: 0.000236, lr : 0.008500\n",
      "Epoch: 5933/25000, Training Loss: 0.000247, lr : 0.008500\n",
      "Epoch: 5934/25000, Training Loss: 0.000184, lr : 0.008500\n",
      "Epoch: 5935/25000, Training Loss: 0.000255, lr : 0.008500\n",
      "Epoch: 5936/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 5937/25000, Training Loss: 0.000172, lr : 0.008500\n",
      "Epoch: 5938/25000, Training Loss: 0.000199, lr : 0.008500\n",
      "Epoch: 5939/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 5940/25000, Training Loss: 0.000163, lr : 0.008500\n",
      "Epoch: 5941/25000, Training Loss: 0.000187, lr : 0.008500\n",
      "Epoch: 5942/25000, Training Loss: 0.000158, lr : 0.008500\n",
      "Epoch: 5943/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 5944/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 5945/25000, Training Loss: 0.000151, lr : 0.008500\n",
      "Epoch: 5946/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 5947/25000, Training Loss: 0.000131, lr : 0.008500\n",
      "Epoch: 5948/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 5949/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 5950/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 5951/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 5952/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 5953/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 5954/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 5955/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 5956/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 5957/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 5958/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 5959/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 5960/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 5961/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 5962/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 5963/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 5964/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 5965/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 5966/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 5967/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 5968/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 5969/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 5970/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 5971/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 5972/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 5973/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 5974/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 5975/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 5976/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 5977/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 5978/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 5979/25000, Training Loss: 0.000133, lr : 0.008500\n",
      "Epoch: 5980/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 5981/25000, Training Loss: 0.000279, lr : 0.008500\n",
      "Epoch: 5982/25000, Training Loss: 0.000453, lr : 0.008500\n",
      "Epoch: 5983/25000, Training Loss: 0.000269, lr : 0.008500\n",
      "Epoch: 5984/25000, Training Loss: 0.000287, lr : 0.008500\n",
      "Epoch: 5985/25000, Training Loss: 0.000382, lr : 0.008500\n",
      "Epoch: 5986/25000, Training Loss: 0.000224, lr : 0.008500\n",
      "Epoch: 5987/25000, Training Loss: 0.000226, lr : 0.008500\n",
      "Epoch: 5988/25000, Training Loss: 0.000198, lr : 0.008500\n",
      "Epoch: 5989/25000, Training Loss: 0.000154, lr : 0.008500\n",
      "Epoch: 5990/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 5991/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 5992/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 5993/25000, Training Loss: 0.000206, lr : 0.008500\n",
      "Epoch: 5994/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 5995/25000, Training Loss: 0.000133, lr : 0.008500\n",
      "Epoch: 5996/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 5997/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 5998/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 5999/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 6000/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 6001/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 6002/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6003/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 6004/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 6005/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 6006/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6007/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 6008/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 6009/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 6010/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6011/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6012/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 6013/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 6014/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 6015/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 6016/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 6017/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 6018/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6019/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 6020/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 6021/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 6022/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 6023/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 6024/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 6025/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6026/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 6027/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 6028/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 6029/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6030/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 6031/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 6032/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 6033/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 6034/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 6035/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 6036/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 6037/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 6038/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 6039/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 6040/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 6041/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6042/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 6043/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 6044/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 6045/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 6046/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6047/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 6048/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 6049/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6050/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6051/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 6052/25000, Training Loss: 0.000158, lr : 0.008500\n",
      "Epoch: 6053/25000, Training Loss: 0.000163, lr : 0.008500\n",
      "Epoch: 6054/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 6055/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 6056/25000, Training Loss: 0.000163, lr : 0.008500\n",
      "Epoch: 6057/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 6058/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 6059/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 6060/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 6061/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 6062/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6063/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 6064/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6065/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6066/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6067/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 6068/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 6069/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 6070/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 6071/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 6072/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 6073/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6074/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6075/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 6076/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 6077/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 6078/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6079/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 6080/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 6081/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 6082/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 6083/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6084/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6085/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 6086/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6087/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 6088/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 6089/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 6090/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 6091/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6092/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 6093/25000, Training Loss: 0.000178, lr : 0.008500\n",
      "Epoch: 6094/25000, Training Loss: 0.000237, lr : 0.008500\n",
      "Epoch: 6095/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 6096/25000, Training Loss: 0.000430, lr : 0.008500\n",
      "Epoch: 6097/25000, Training Loss: 0.000356, lr : 0.008500\n",
      "Epoch: 6098/25000, Training Loss: 0.000210, lr : 0.008500\n",
      "Epoch: 6099/25000, Training Loss: 0.000237, lr : 0.008500\n",
      "Epoch: 6100/25000, Training Loss: 0.000480, lr : 0.008500\n",
      "Epoch: 6101/25000, Training Loss: 0.000278, lr : 0.008500\n",
      "Epoch: 6102/25000, Training Loss: 0.000172, lr : 0.008500\n",
      "Epoch: 6103/25000, Training Loss: 0.000232, lr : 0.008500\n",
      "Epoch: 6104/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 6105/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 6106/25000, Training Loss: 0.000196, lr : 0.008500\n",
      "Epoch: 6107/25000, Training Loss: 0.000237, lr : 0.008500\n",
      "Epoch: 6108/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 6109/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 6110/25000, Training Loss: 0.000131, lr : 0.008500\n",
      "Epoch: 6111/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 6112/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 6113/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 6114/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 6115/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 6116/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 6117/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 6118/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 6119/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 6120/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 6121/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 6122/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6123/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 6124/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 6125/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 6126/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 6127/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 6128/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6129/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 6130/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 6131/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 6132/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 6133/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 6134/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 6135/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 6136/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 6137/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 6138/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 6139/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 6140/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 6141/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 6142/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 6143/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 6144/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 6145/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 6146/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 6147/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 6148/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 6149/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 6150/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 6151/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 6152/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 6153/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 6154/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 6155/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 6156/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 6157/25000, Training Loss: 0.000168, lr : 0.008500\n",
      "Epoch: 6158/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 6159/25000, Training Loss: 0.000264, lr : 0.008500\n",
      "Epoch: 6160/25000, Training Loss: 0.000235, lr : 0.008500\n",
      "Epoch: 6161/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 6162/25000, Training Loss: 0.000199, lr : 0.008500\n",
      "Epoch: 6163/25000, Training Loss: 0.000195, lr : 0.008500\n",
      "Epoch: 6164/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 6165/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 6166/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 6167/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 6168/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 6169/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 6170/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 6171/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 6172/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 6173/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 6174/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 6175/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 6176/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 6177/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 6178/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 6179/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 6180/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 6181/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 6182/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 6183/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 6184/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 6185/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 6186/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 6187/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 6188/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6189/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 6190/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 6191/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 6192/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 6193/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 6194/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 6195/25000, Training Loss: 0.000193, lr : 0.008500\n",
      "Epoch: 6196/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 6197/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 6198/25000, Training Loss: 0.000170, lr : 0.008500\n",
      "Epoch: 6199/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 6200/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 6201/25000, Training Loss: 0.000317, lr : 0.008500\n",
      "Epoch: 6202/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 6203/25000, Training Loss: 0.000213, lr : 0.008500\n",
      "Epoch: 6204/25000, Training Loss: 0.000219, lr : 0.008500\n",
      "Epoch: 6205/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 6206/25000, Training Loss: 0.000170, lr : 0.008500\n",
      "Epoch: 6207/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 6208/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 6209/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 6210/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 6211/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 6212/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 6213/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 6214/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 6215/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 6216/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 6217/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 6218/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 6219/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 6220/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 6221/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 6222/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 6223/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 6224/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 6225/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 6226/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 6227/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 6228/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 6229/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 6230/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 6231/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 6232/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 6233/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6234/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6235/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 6236/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 6237/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6238/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6239/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 6240/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 6241/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 6242/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 6243/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6244/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6245/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6246/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 6247/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 6248/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 6249/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 6250/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 6251/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 6252/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 6253/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 6254/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 6255/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6256/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6257/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 6258/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 6259/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 6260/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 6261/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 6262/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6263/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 6264/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6265/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 6266/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 6267/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 6268/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 6269/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 6270/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6271/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 6272/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 6273/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 6274/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6275/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6276/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 6277/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6278/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 6279/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 6280/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6281/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 6282/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 6283/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 6284/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 6285/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 6286/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 6287/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 6288/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 6289/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 6290/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 6291/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 6292/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 6293/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 6294/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 6295/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 6296/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 6297/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 6298/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 6299/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 6300/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 6301/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6302/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6303/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 6304/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 6305/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 6306/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 6307/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 6308/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 6309/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 6310/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 6311/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 6312/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 6313/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 6314/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 6315/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 6316/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 6317/25000, Training Loss: 0.000220, lr : 0.008500\n",
      "Epoch: 6318/25000, Training Loss: 0.000216, lr : 0.008500\n",
      "Epoch: 6319/25000, Training Loss: 0.000201, lr : 0.008500\n",
      "Epoch: 6320/25000, Training Loss: 0.000234, lr : 0.008500\n",
      "Epoch: 6321/25000, Training Loss: 0.000220, lr : 0.008500\n",
      "Epoch: 6322/25000, Training Loss: 0.000227, lr : 0.008500\n",
      "Epoch: 6323/25000, Training Loss: 0.000338, lr : 0.008500\n",
      "Epoch: 6324/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 6325/25000, Training Loss: 0.000187, lr : 0.008500\n",
      "Epoch: 6326/25000, Training Loss: 0.000187, lr : 0.008500\n",
      "Epoch: 6327/25000, Training Loss: 0.000198, lr : 0.008500\n",
      "Epoch: 6328/25000, Training Loss: 0.000201, lr : 0.008500\n",
      "Epoch: 6329/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 6330/25000, Training Loss: 0.000163, lr : 0.008500\n",
      "Epoch: 6331/25000, Training Loss: 0.000168, lr : 0.008500\n",
      "Epoch: 6332/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 6333/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 6334/25000, Training Loss: 0.000133, lr : 0.008500\n",
      "Epoch: 6335/25000, Training Loss: 0.000172, lr : 0.008500\n",
      "Epoch: 6336/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 6337/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 6338/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 6339/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 6340/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 6341/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 6342/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 6343/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 6344/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 6345/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 6346/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 6347/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 6348/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 6349/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 6350/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 6351/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 6352/25000, Training Loss: 0.000210, lr : 0.008500\n",
      "Epoch: 6353/25000, Training Loss: 0.000155, lr : 0.008500\n",
      "Epoch: 6354/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 6355/25000, Training Loss: 0.000307, lr : 0.008500\n",
      "Epoch: 6356/25000, Training Loss: 0.000246, lr : 0.008500\n",
      "Epoch: 6357/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 6358/25000, Training Loss: 0.000159, lr : 0.008500\n",
      "Epoch: 6359/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 6360/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 6361/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 6362/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 6363/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 6364/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 6365/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 6366/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 6367/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 6368/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6369/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 6370/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 6371/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 6372/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6373/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 6374/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 6375/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 6376/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 6377/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6378/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6379/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 6380/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 6381/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 6382/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 6383/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 6384/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 6385/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6386/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 6387/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 6388/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 6389/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6390/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6391/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 6392/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 6393/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 6394/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 6395/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 6396/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 6397/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6398/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 6399/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 6400/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 6401/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 6402/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 6403/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6404/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 6405/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 6406/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 6407/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 6408/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 6409/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 6410/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 6411/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6412/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 6413/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 6414/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 6415/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 6416/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 6417/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 6418/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 6419/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6420/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 6421/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 6422/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 6423/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 6424/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6425/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6426/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 6427/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 6428/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 6429/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 6430/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6431/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 6432/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 6433/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 6434/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 6435/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 6436/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 6437/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6438/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 6439/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 6440/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 6441/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 6442/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 6443/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 6444/25000, Training Loss: 0.000164, lr : 0.008500\n",
      "Epoch: 6445/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 6446/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6447/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6448/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 6449/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 6450/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6451/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 6452/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 6453/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 6454/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 6455/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 6456/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 6457/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 6458/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 6459/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 6460/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 6461/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 6462/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 6463/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 6464/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 6465/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 6466/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 6467/25000, Training Loss: 0.000166, lr : 0.008500\n",
      "Epoch: 6468/25000, Training Loss: 0.000194, lr : 0.008500\n",
      "Epoch: 6469/25000, Training Loss: 0.000187, lr : 0.008500\n",
      "Epoch: 6470/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 6471/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 6472/25000, Training Loss: 0.000301, lr : 0.008500\n",
      "Epoch: 6473/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 6474/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 6475/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 6476/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 6477/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 6478/25000, Training Loss: 0.000206, lr : 0.008500\n",
      "Epoch: 6479/25000, Training Loss: 0.000211, lr : 0.008500\n",
      "Epoch: 6480/25000, Training Loss: 0.000205, lr : 0.008500\n",
      "Epoch: 6481/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 6482/25000, Training Loss: 0.000175, lr : 0.008500\n",
      "Epoch: 6483/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 6484/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 6485/25000, Training Loss: 0.000252, lr : 0.008500\n",
      "Epoch: 6486/25000, Training Loss: 0.000179, lr : 0.008500\n",
      "Epoch: 6487/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 6488/25000, Training Loss: 0.000232, lr : 0.008500\n",
      "Epoch: 6489/25000, Training Loss: 0.000270, lr : 0.008500\n",
      "Epoch: 6490/25000, Training Loss: 0.000186, lr : 0.008500\n",
      "Epoch: 6491/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 6492/25000, Training Loss: 0.000248, lr : 0.008500\n",
      "Epoch: 6493/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 6494/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 6495/25000, Training Loss: 0.000208, lr : 0.008500\n",
      "Epoch: 6496/25000, Training Loss: 0.000159, lr : 0.008500\n",
      "Epoch: 6497/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 6498/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 6499/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 6500/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 6501/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 6502/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 6503/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 6504/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 6505/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 6506/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 6507/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 6508/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6509/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 6510/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 6511/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 6512/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 6513/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6514/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 6515/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 6516/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 6517/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 6518/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 6519/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 6520/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 6521/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 6522/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 6523/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 6524/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 6525/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 6526/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 6527/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 6528/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 6529/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 6530/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 6531/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6532/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 6533/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 6534/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 6535/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 6536/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 6537/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 6538/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 6539/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 6540/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6541/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6542/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6543/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 6544/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6545/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 6546/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 6547/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 6548/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6549/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 6550/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 6551/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 6552/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 6553/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 6554/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 6555/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 6556/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 6557/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 6558/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 6559/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 6560/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 6561/25000, Training Loss: 0.000185, lr : 0.008500\n",
      "Epoch: 6562/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 6563/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 6564/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 6565/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 6566/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 6567/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 6568/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6569/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 6570/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6571/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 6572/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 6573/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6574/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 6575/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 6576/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 6577/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 6578/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 6579/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 6580/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6581/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 6582/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 6583/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 6584/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 6585/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 6586/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6587/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 6588/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 6589/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6590/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 6591/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 6592/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 6593/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 6594/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 6595/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 6596/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 6597/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 6598/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 6599/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 6600/25000, Training Loss: 0.000240, lr : 0.008500\n",
      "Epoch: 6601/25000, Training Loss: 0.000167, lr : 0.008500\n",
      "Epoch: 6602/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 6603/25000, Training Loss: 0.000230, lr : 0.008500\n",
      "Epoch: 6604/25000, Training Loss: 0.000188, lr : 0.008500\n",
      "Epoch: 6605/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 6606/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 6607/25000, Training Loss: 0.000197, lr : 0.008500\n",
      "Epoch: 6608/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 6609/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 6610/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 6611/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 6612/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 6613/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 6614/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 6615/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 6616/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 6617/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 6618/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 6619/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 6620/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 6621/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 6622/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6623/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 6624/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 6625/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6626/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6627/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6628/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 6629/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 6630/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 6631/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 6632/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 6633/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 6634/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 6635/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 6636/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6637/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 6638/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 6639/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6640/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 6641/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 6642/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 6643/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 6644/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 6645/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 6646/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 6647/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 6648/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 6649/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 6650/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6651/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 6652/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 6653/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 6654/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 6655/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 6656/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 6657/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 6658/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 6659/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 6660/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6661/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 6662/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 6663/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 6664/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 6665/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 6666/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 6667/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 6668/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 6669/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 6670/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 6671/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 6672/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 6673/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 6674/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 6675/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 6676/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 6677/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 6678/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 6679/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 6680/25000, Training Loss: 0.000262, lr : 0.008500\n",
      "Epoch: 6681/25000, Training Loss: 0.000392, lr : 0.008500\n",
      "Epoch: 6682/25000, Training Loss: 0.000486, lr : 0.008500\n",
      "Epoch: 6683/25000, Training Loss: 0.000461, lr : 0.008500\n",
      "Epoch: 6684/25000, Training Loss: 0.000237, lr : 0.008500\n",
      "Epoch: 6685/25000, Training Loss: 0.000375, lr : 0.008500\n",
      "Epoch: 6686/25000, Training Loss: 0.000689, lr : 0.008500\n",
      "Epoch: 6687/25000, Training Loss: 0.000438, lr : 0.008500\n",
      "Epoch: 6688/25000, Training Loss: 0.000348, lr : 0.008500\n",
      "Epoch: 6689/25000, Training Loss: 0.000619, lr : 0.008500\n",
      "Epoch: 6690/25000, Training Loss: 0.000340, lr : 0.008500\n",
      "Epoch: 6691/25000, Training Loss: 0.000234, lr : 0.008500\n",
      "Epoch: 6692/25000, Training Loss: 0.000252, lr : 0.008500\n",
      "Epoch: 6693/25000, Training Loss: 0.000220, lr : 0.008500\n",
      "Epoch: 6694/25000, Training Loss: 0.000256, lr : 0.008500\n",
      "Epoch: 6695/25000, Training Loss: 0.000234, lr : 0.008500\n",
      "Epoch: 6696/25000, Training Loss: 0.000227, lr : 0.008500\n",
      "Epoch: 6697/25000, Training Loss: 0.000212, lr : 0.008500\n",
      "Epoch: 6698/25000, Training Loss: 0.000189, lr : 0.008500\n",
      "Epoch: 6699/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 6700/25000, Training Loss: 0.000167, lr : 0.008500\n",
      "Epoch: 6701/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 6702/25000, Training Loss: 0.000277, lr : 0.008500\n",
      "Epoch: 6703/25000, Training Loss: 0.000251, lr : 0.008500\n",
      "Epoch: 6704/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 6705/25000, Training Loss: 0.000199, lr : 0.008500\n",
      "Epoch: 6706/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 6707/25000, Training Loss: 0.000281, lr : 0.008500\n",
      "Epoch: 6708/25000, Training Loss: 0.000172, lr : 0.008500\n",
      "Epoch: 6709/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 6710/25000, Training Loss: 0.000175, lr : 0.008500\n",
      "Epoch: 6711/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 6712/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 6713/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 6714/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 6715/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 6716/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 6717/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 6718/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6719/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 6720/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 6721/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 6722/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6723/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 6724/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 6725/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 6726/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 6727/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 6728/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 6729/25000, Training Loss: 0.000133, lr : 0.008500\n",
      "Epoch: 6730/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6731/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 6732/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 6733/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 6734/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 6735/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 6736/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 6737/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 6738/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 6739/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 6740/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 6741/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 6742/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 6743/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6744/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 6745/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 6746/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 6747/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 6748/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 6749/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 6750/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 6751/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 6752/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6753/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 6754/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6755/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6756/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 6757/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 6758/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 6759/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 6760/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 6761/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 6762/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 6763/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6764/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 6765/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 6766/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 6767/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 6768/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 6769/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6770/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 6771/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 6772/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 6773/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6774/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 6775/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6776/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 6777/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 6778/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 6779/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 6780/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 6781/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 6782/25000, Training Loss: 0.000230, lr : 0.008500\n",
      "Epoch: 6783/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 6784/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 6785/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 6786/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 6787/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 6788/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 6789/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 6790/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 6791/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 6792/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6793/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 6794/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6795/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 6796/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 6797/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 6798/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6799/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 6800/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6801/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 6802/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 6803/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 6804/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 6805/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6806/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6807/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 6808/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 6809/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6810/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 6811/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6812/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 6813/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 6814/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 6815/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 6816/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6817/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 6818/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 6819/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 6820/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 6821/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 6822/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 6823/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6824/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 6825/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 6826/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 6827/25000, Training Loss: 0.000024, lr : 0.008500\n",
      "Epoch: 6828/25000, Training Loss: 0.000020, lr : 0.008500\n",
      "Epoch: 6829/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 6830/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 6831/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 6832/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 6833/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 6834/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 6835/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 6836/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 6837/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 6838/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 6839/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 6840/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 6841/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 6842/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6843/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 6844/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 6845/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6846/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 6847/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6848/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 6849/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 6850/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 6851/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6852/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 6853/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 6854/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 6855/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 6856/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 6857/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 6858/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 6859/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 6860/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 6861/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 6862/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6863/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 6864/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 6865/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 6866/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 6867/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6868/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 6869/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6870/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 6871/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 6872/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 6873/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6874/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 6875/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 6876/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 6877/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 6878/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 6879/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 6880/25000, Training Loss: 0.000191, lr : 0.008500\n",
      "Epoch: 6881/25000, Training Loss: 0.000164, lr : 0.008500\n",
      "Epoch: 6882/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 6883/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 6884/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 6885/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 6886/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 6887/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 6888/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 6889/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 6890/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 6891/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 6892/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 6893/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 6894/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 6895/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 6896/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 6897/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 6898/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 6899/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6900/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 6901/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 6902/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 6903/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6904/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 6905/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 6906/25000, Training Loss: 0.000234, lr : 0.008500\n",
      "Epoch: 6907/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 6908/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 6909/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6910/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 6911/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 6912/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 6913/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 6914/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 6915/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 6916/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 6917/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 6918/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 6919/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 6920/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 6921/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 6922/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 6923/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 6924/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 6925/25000, Training Loss: 0.000166, lr : 0.008500\n",
      "Epoch: 6926/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 6927/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 6928/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 6929/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 6930/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 6931/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 6932/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 6933/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 6934/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 6935/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 6936/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 6937/25000, Training Loss: 0.000296, lr : 0.008500\n",
      "Epoch: 6938/25000, Training Loss: 0.000258, lr : 0.008500\n",
      "Epoch: 6939/25000, Training Loss: 0.000253, lr : 0.008500\n",
      "Epoch: 6940/25000, Training Loss: 0.000172, lr : 0.008500\n",
      "Epoch: 6941/25000, Training Loss: 0.000198, lr : 0.008500\n",
      "Epoch: 6942/25000, Training Loss: 0.000321, lr : 0.008500\n",
      "Epoch: 6943/25000, Training Loss: 0.000159, lr : 0.008500\n",
      "Epoch: 6944/25000, Training Loss: 0.000272, lr : 0.008500\n",
      "Epoch: 6945/25000, Training Loss: 0.000342, lr : 0.008500\n",
      "Epoch: 6946/25000, Training Loss: 0.000191, lr : 0.008500\n",
      "Epoch: 6947/25000, Training Loss: 0.000409, lr : 0.008500\n",
      "Epoch: 6948/25000, Training Loss: 0.000434, lr : 0.008500\n",
      "Epoch: 6949/25000, Training Loss: 0.000216, lr : 0.008500\n",
      "Epoch: 6950/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 6951/25000, Training Loss: 0.000284, lr : 0.008500\n",
      "Epoch: 6952/25000, Training Loss: 0.000208, lr : 0.008500\n",
      "Epoch: 6953/25000, Training Loss: 0.000237, lr : 0.008500\n",
      "Epoch: 6954/25000, Training Loss: 0.000462, lr : 0.008500\n",
      "Epoch: 6955/25000, Training Loss: 0.000224, lr : 0.008500\n",
      "Epoch: 6956/25000, Training Loss: 0.000221, lr : 0.008500\n",
      "Epoch: 6957/25000, Training Loss: 0.000222, lr : 0.008500\n",
      "Epoch: 6958/25000, Training Loss: 0.000159, lr : 0.008500\n",
      "Epoch: 6959/25000, Training Loss: 0.000214, lr : 0.008500\n",
      "Epoch: 6960/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 6961/25000, Training Loss: 0.000155, lr : 0.008500\n",
      "Epoch: 6962/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 6963/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 6964/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 6965/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 6966/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 6967/25000, Training Loss: 0.000249, lr : 0.008500\n",
      "Epoch: 6968/25000, Training Loss: 0.000248, lr : 0.008500\n",
      "Epoch: 6969/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 6970/25000, Training Loss: 0.000226, lr : 0.008500\n",
      "Epoch: 6971/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 6972/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 6973/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 6974/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 6975/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 6976/25000, Training Loss: 0.000168, lr : 0.008500\n",
      "Epoch: 6977/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 6978/25000, Training Loss: 0.000166, lr : 0.008500\n",
      "Epoch: 6979/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 6980/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 6981/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 6982/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 6983/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 6984/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 6985/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 6986/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 6987/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 6988/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 6989/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 6990/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 6991/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 6992/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 6993/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 6994/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 6995/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 6996/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 6997/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 6998/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 6999/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7000/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7001/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 7002/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7003/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7004/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 7005/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7006/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7007/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7008/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7009/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7010/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7011/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 7012/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7013/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7014/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 7015/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7016/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7017/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7018/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7019/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7020/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7021/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7022/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7023/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7024/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7025/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 7026/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 7027/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7028/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 7029/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 7030/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7031/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 7032/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 7033/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 7034/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 7035/25000, Training Loss: 0.000114, lr : 0.008500\n",
      "Epoch: 7036/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 7037/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 7038/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 7039/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 7040/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 7041/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 7042/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 7043/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 7044/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 7045/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 7046/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 7047/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 7048/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 7049/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7050/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 7051/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 7052/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 7053/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 7054/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 7055/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 7056/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 7057/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 7058/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 7059/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 7060/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 7061/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 7062/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 7063/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 7064/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 7065/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 7066/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 7067/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 7068/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 7069/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 7070/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 7071/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 7072/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 7073/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 7074/25000, Training Loss: 0.000216, lr : 0.008500\n",
      "Epoch: 7075/25000, Training Loss: 0.000334, lr : 0.008500\n",
      "Epoch: 7076/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 7077/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 7078/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 7079/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 7080/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 7081/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 7082/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 7083/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 7084/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 7085/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 7086/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 7087/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 7088/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 7089/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 7090/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 7091/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 7092/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 7093/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7094/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 7095/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7096/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 7097/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 7098/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7099/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 7100/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7101/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7102/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 7103/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 7104/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 7105/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 7106/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 7107/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7108/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7109/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 7110/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7111/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7112/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 7113/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 7114/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7115/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 7116/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7117/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7118/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 7119/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7120/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7121/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7122/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 7123/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7124/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7125/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7126/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 7127/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 7128/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7129/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7130/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7131/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7132/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 7133/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 7134/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 7135/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7136/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7137/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7138/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7139/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 7140/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 7141/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 7142/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 7143/25000, Training Loss: 0.000194, lr : 0.008500\n",
      "Epoch: 7144/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 7145/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 7146/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 7147/25000, Training Loss: 0.000166, lr : 0.008500\n",
      "Epoch: 7148/25000, Training Loss: 0.000379, lr : 0.008500\n",
      "Epoch: 7149/25000, Training Loss: 0.000323, lr : 0.008500\n",
      "Epoch: 7150/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 7151/25000, Training Loss: 0.000250, lr : 0.008500\n",
      "Epoch: 7152/25000, Training Loss: 0.000374, lr : 0.008500\n",
      "Epoch: 7153/25000, Training Loss: 0.000224, lr : 0.008500\n",
      "Epoch: 7154/25000, Training Loss: 0.000267, lr : 0.008500\n",
      "Epoch: 7155/25000, Training Loss: 0.000325, lr : 0.008500\n",
      "Epoch: 7156/25000, Training Loss: 0.000267, lr : 0.008500\n",
      "Epoch: 7157/25000, Training Loss: 0.000151, lr : 0.008500\n",
      "Epoch: 7158/25000, Training Loss: 0.000252, lr : 0.008500\n",
      "Epoch: 7159/25000, Training Loss: 0.000184, lr : 0.008500\n",
      "Epoch: 7160/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 7161/25000, Training Loss: 0.000351, lr : 0.008500\n",
      "Epoch: 7162/25000, Training Loss: 0.000195, lr : 0.008500\n",
      "Epoch: 7163/25000, Training Loss: 0.000244, lr : 0.008500\n",
      "Epoch: 7164/25000, Training Loss: 0.000289, lr : 0.008500\n",
      "Epoch: 7165/25000, Training Loss: 0.000197, lr : 0.008500\n",
      "Epoch: 7166/25000, Training Loss: 0.000326, lr : 0.008500\n",
      "Epoch: 7167/25000, Training Loss: 0.000459, lr : 0.008500\n",
      "Epoch: 7168/25000, Training Loss: 0.000249, lr : 0.008500\n",
      "Epoch: 7169/25000, Training Loss: 0.000244, lr : 0.008500\n",
      "Epoch: 7170/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 7171/25000, Training Loss: 0.000251, lr : 0.008500\n",
      "Epoch: 7172/25000, Training Loss: 0.000216, lr : 0.008500\n",
      "Epoch: 7173/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 7174/25000, Training Loss: 0.000223, lr : 0.008500\n",
      "Epoch: 7175/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 7176/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 7177/25000, Training Loss: 0.000151, lr : 0.008500\n",
      "Epoch: 7178/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 7179/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 7180/25000, Training Loss: 0.000172, lr : 0.008500\n",
      "Epoch: 7181/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 7182/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 7183/25000, Training Loss: 0.000192, lr : 0.008500\n",
      "Epoch: 7184/25000, Training Loss: 0.000186, lr : 0.008500\n",
      "Epoch: 7185/25000, Training Loss: 0.000256, lr : 0.008500\n",
      "Epoch: 7186/25000, Training Loss: 0.000228, lr : 0.008500\n",
      "Epoch: 7187/25000, Training Loss: 0.000233, lr : 0.008500\n",
      "Epoch: 7188/25000, Training Loss: 0.000234, lr : 0.008500\n",
      "Epoch: 7189/25000, Training Loss: 0.000258, lr : 0.008500\n",
      "Epoch: 7190/25000, Training Loss: 0.000277, lr : 0.008500\n",
      "Epoch: 7191/25000, Training Loss: 0.000248, lr : 0.008500\n",
      "Epoch: 7192/25000, Training Loss: 0.000201, lr : 0.008500\n",
      "Epoch: 7193/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 7194/25000, Training Loss: 0.000247, lr : 0.008500\n",
      "Epoch: 7195/25000, Training Loss: 0.000213, lr : 0.008500\n",
      "Epoch: 7196/25000, Training Loss: 0.000203, lr : 0.008500\n",
      "Epoch: 7197/25000, Training Loss: 0.000202, lr : 0.008500\n",
      "Epoch: 7198/25000, Training Loss: 0.000154, lr : 0.008500\n",
      "Epoch: 7199/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 7200/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 7201/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 7202/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 7203/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 7204/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 7205/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 7206/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 7207/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 7208/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 7209/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7210/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 7211/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 7212/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 7213/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 7214/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 7215/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 7216/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 7217/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 7218/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 7219/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 7220/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 7221/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 7222/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 7223/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 7224/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 7225/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 7226/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 7227/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7228/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 7229/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7230/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7231/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 7232/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7233/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 7234/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7235/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 7236/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 7237/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7238/25000, Training Loss: 0.000024, lr : 0.008500\n",
      "Epoch: 7239/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 7240/25000, Training Loss: 0.000022, lr : 0.008500\n",
      "Epoch: 7241/25000, Training Loss: 0.000020, lr : 0.008500\n",
      "Epoch: 7242/25000, Training Loss: 0.000021, lr : 0.008500\n",
      "Epoch: 7243/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7244/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 7245/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7246/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 7247/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7248/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7249/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7250/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 7251/25000, Training Loss: 0.000024, lr : 0.008500\n",
      "Epoch: 7252/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 7253/25000, Training Loss: 0.000023, lr : 0.008500\n",
      "Epoch: 7254/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7255/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7256/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7257/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7258/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 7259/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7260/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 7261/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7262/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7263/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7264/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 7265/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7266/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 7267/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7268/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 7269/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 7270/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 7271/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 7272/25000, Training Loss: 0.000023, lr : 0.008500\n",
      "Epoch: 7273/25000, Training Loss: 0.000021, lr : 0.008500\n",
      "Epoch: 7274/25000, Training Loss: 0.000023, lr : 0.008500\n",
      "Epoch: 7275/25000, Training Loss: 0.000023, lr : 0.008500\n",
      "Epoch: 7276/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 7277/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7278/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7279/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 7280/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7281/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 7282/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7283/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7284/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 7285/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7286/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7287/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7288/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 7289/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7290/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7291/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7292/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 7293/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7294/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7295/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7296/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 7297/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 7298/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7299/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7300/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 7301/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 7302/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 7303/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 7304/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 7305/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 7306/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7307/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 7308/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 7309/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 7310/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 7311/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7312/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7313/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7314/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7315/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7316/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7317/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7318/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7319/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 7320/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7321/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7322/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 7323/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 7324/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 7325/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7326/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 7327/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7328/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7329/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7330/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7331/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7332/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 7333/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 7334/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7335/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 7336/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 7337/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 7338/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 7339/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 7340/25000, Training Loss: 0.000257, lr : 0.008500\n",
      "Epoch: 7341/25000, Training Loss: 0.000420, lr : 0.008500\n",
      "Epoch: 7342/25000, Training Loss: 0.000421, lr : 0.008500\n",
      "Epoch: 7343/25000, Training Loss: 0.000196, lr : 0.008500\n",
      "Epoch: 7344/25000, Training Loss: 0.000306, lr : 0.008500\n",
      "Epoch: 7345/25000, Training Loss: 0.000738, lr : 0.008500\n",
      "Epoch: 7346/25000, Training Loss: 0.000485, lr : 0.008500\n",
      "Epoch: 7347/25000, Training Loss: 0.000293, lr : 0.008500\n",
      "Epoch: 7348/25000, Training Loss: 0.000418, lr : 0.008500\n",
      "Epoch: 7349/25000, Training Loss: 0.000294, lr : 0.008500\n",
      "Epoch: 7350/25000, Training Loss: 0.000241, lr : 0.008500\n",
      "Epoch: 7351/25000, Training Loss: 0.000302, lr : 0.008500\n",
      "Epoch: 7352/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 7353/25000, Training Loss: 0.000217, lr : 0.008500\n",
      "Epoch: 7354/25000, Training Loss: 0.000213, lr : 0.008500\n",
      "Epoch: 7355/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 7356/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 7357/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 7358/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 7359/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 7360/25000, Training Loss: 0.000154, lr : 0.008500\n",
      "Epoch: 7361/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 7362/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 7363/25000, Training Loss: 0.000137, lr : 0.008500\n",
      "Epoch: 7364/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 7365/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 7366/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 7367/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 7368/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 7369/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7370/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 7371/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 7372/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 7373/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 7374/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 7375/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 7376/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 7377/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 7378/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 7379/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7380/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 7381/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 7382/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 7383/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 7384/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 7385/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7386/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7387/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 7388/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7389/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7390/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7391/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 7392/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7393/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7394/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7395/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7396/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 7397/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 7398/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7399/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7400/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7401/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7402/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 7403/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7404/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7405/25000, Training Loss: 0.000022, lr : 0.008500\n",
      "Epoch: 7406/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 7407/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 7408/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7409/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 7410/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 7411/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 7412/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 7413/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7414/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 7415/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7416/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7417/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7418/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 7419/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 7420/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7421/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7422/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7423/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 7424/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7425/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7426/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 7427/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 7428/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 7429/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7430/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 7431/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 7432/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7433/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7434/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 7435/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 7436/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 7437/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 7438/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7439/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 7440/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7441/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7442/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7443/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7444/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 7445/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7446/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 7447/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 7448/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 7449/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 7450/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 7451/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7452/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7453/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 7454/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 7455/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 7456/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7457/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7458/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7459/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7460/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 7461/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 7462/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 7463/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7464/25000, Training Loss: 0.000161, lr : 0.008500\n",
      "Epoch: 7465/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 7466/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 7467/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 7468/25000, Training Loss: 0.000228, lr : 0.008500\n",
      "Epoch: 7469/25000, Training Loss: 0.000217, lr : 0.008500\n",
      "Epoch: 7470/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 7471/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 7472/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 7473/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 7474/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 7475/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 7476/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 7477/25000, Training Loss: 0.000128, lr : 0.008500\n",
      "Epoch: 7478/25000, Training Loss: 0.000204, lr : 0.008500\n",
      "Epoch: 7479/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 7480/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 7481/25000, Training Loss: 0.000250, lr : 0.008500\n",
      "Epoch: 7482/25000, Training Loss: 0.000240, lr : 0.008500\n",
      "Epoch: 7483/25000, Training Loss: 0.000155, lr : 0.008500\n",
      "Epoch: 7484/25000, Training Loss: 0.000356, lr : 0.008500\n",
      "Epoch: 7485/25000, Training Loss: 0.000293, lr : 0.008500\n",
      "Epoch: 7486/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 7487/25000, Training Loss: 0.000225, lr : 0.008500\n",
      "Epoch: 7488/25000, Training Loss: 0.000194, lr : 0.008500\n",
      "Epoch: 7489/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 7490/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 7491/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 7492/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 7493/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 7494/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 7495/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 7496/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 7497/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 7498/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 7499/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 7500/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 7501/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 7502/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 7503/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 7504/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 7505/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 7506/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 7507/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 7508/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7509/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7510/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 7511/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 7512/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 7513/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 7514/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 7515/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 7516/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 7517/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 7518/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 7519/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 7520/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 7521/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 7522/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 7523/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 7524/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 7525/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 7526/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 7527/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 7528/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 7529/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 7530/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 7531/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 7532/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 7533/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 7534/25000, Training Loss: 0.000131, lr : 0.008500\n",
      "Epoch: 7535/25000, Training Loss: 0.000160, lr : 0.008500\n",
      "Epoch: 7536/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 7537/25000, Training Loss: 0.000214, lr : 0.008500\n",
      "Epoch: 7538/25000, Training Loss: 0.000250, lr : 0.008500\n",
      "Epoch: 7539/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 7540/25000, Training Loss: 0.000260, lr : 0.008500\n",
      "Epoch: 7541/25000, Training Loss: 0.000240, lr : 0.008500\n",
      "Epoch: 7542/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 7543/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 7544/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 7545/25000, Training Loss: 0.000231, lr : 0.008500\n",
      "Epoch: 7546/25000, Training Loss: 0.000203, lr : 0.008500\n",
      "Epoch: 7547/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 7548/25000, Training Loss: 0.000168, lr : 0.008500\n",
      "Epoch: 7549/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 7550/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 7551/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 7552/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 7553/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 7554/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 7555/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 7556/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 7557/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 7558/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 7559/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7560/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 7561/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7562/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7563/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7564/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7565/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 7566/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7567/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7568/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7569/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7570/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7571/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 7572/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 7573/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7574/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7575/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7576/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7577/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 7578/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 7579/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 7580/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7581/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 7582/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7583/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7584/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 7585/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 7586/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 7587/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 7588/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 7589/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 7590/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 7591/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 7592/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 7593/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 7594/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 7595/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 7596/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 7597/25000, Training Loss: 0.000120, lr : 0.008500\n",
      "Epoch: 7598/25000, Training Loss: 0.000177, lr : 0.008500\n",
      "Epoch: 7599/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 7600/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 7601/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 7602/25000, Training Loss: 0.000079, lr : 0.008500\n",
      "Epoch: 7603/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 7604/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 7605/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 7606/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7607/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 7608/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 7609/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 7610/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 7611/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7612/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 7613/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7614/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7615/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 7616/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 7617/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 7618/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 7619/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7620/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 7621/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 7622/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 7623/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 7624/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7625/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 7626/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 7627/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 7628/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 7629/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 7630/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 7631/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 7632/25000, Training Loss: 0.000210, lr : 0.008500\n",
      "Epoch: 7633/25000, Training Loss: 0.000252, lr : 0.008500\n",
      "Epoch: 7634/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 7635/25000, Training Loss: 0.000155, lr : 0.008500\n",
      "Epoch: 7636/25000, Training Loss: 0.000234, lr : 0.008500\n",
      "Epoch: 7637/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 7638/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 7639/25000, Training Loss: 0.000158, lr : 0.008500\n",
      "Epoch: 7640/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 7641/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 7642/25000, Training Loss: 0.000173, lr : 0.008500\n",
      "Epoch: 7643/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 7644/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 7645/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 7646/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 7647/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 7648/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 7649/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 7650/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7651/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 7652/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 7653/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7654/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 7655/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7656/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7657/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 7658/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7659/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 7660/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 7661/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 7662/25000, Training Loss: 0.000175, lr : 0.008500\n",
      "Epoch: 7663/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 7664/25000, Training Loss: 0.000195, lr : 0.008500\n",
      "Epoch: 7665/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 7666/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 7667/25000, Training Loss: 0.000171, lr : 0.008500\n",
      "Epoch: 7668/25000, Training Loss: 0.000195, lr : 0.008500\n",
      "Epoch: 7669/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 7670/25000, Training Loss: 0.000196, lr : 0.008500\n",
      "Epoch: 7671/25000, Training Loss: 0.000364, lr : 0.008500\n",
      "Epoch: 7672/25000, Training Loss: 0.000279, lr : 0.008500\n",
      "Epoch: 7673/25000, Training Loss: 0.000351, lr : 0.008500\n",
      "Epoch: 7674/25000, Training Loss: 0.000316, lr : 0.008500\n",
      "Epoch: 7675/25000, Training Loss: 0.000203, lr : 0.008500\n",
      "Epoch: 7676/25000, Training Loss: 0.000387, lr : 0.008500\n",
      "Epoch: 7677/25000, Training Loss: 0.000250, lr : 0.008500\n",
      "Epoch: 7678/25000, Training Loss: 0.000349, lr : 0.008500\n",
      "Epoch: 7679/25000, Training Loss: 0.000650, lr : 0.008500\n",
      "Epoch: 7680/25000, Training Loss: 0.000454, lr : 0.008500\n",
      "Epoch: 7681/25000, Training Loss: 0.000333, lr : 0.008500\n",
      "Epoch: 7682/25000, Training Loss: 0.000341, lr : 0.008500\n",
      "Epoch: 7683/25000, Training Loss: 0.000371, lr : 0.008500\n",
      "Epoch: 7684/25000, Training Loss: 0.000276, lr : 0.008500\n",
      "Epoch: 7685/25000, Training Loss: 0.000247, lr : 0.008500\n",
      "Epoch: 7686/25000, Training Loss: 0.000278, lr : 0.008500\n",
      "Epoch: 7687/25000, Training Loss: 0.000208, lr : 0.008500\n",
      "Epoch: 7688/25000, Training Loss: 0.000265, lr : 0.008500\n",
      "Epoch: 7689/25000, Training Loss: 0.000198, lr : 0.008500\n",
      "Epoch: 7690/25000, Training Loss: 0.000174, lr : 0.008500\n",
      "Epoch: 7691/25000, Training Loss: 0.000164, lr : 0.008500\n",
      "Epoch: 7692/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 7693/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 7694/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 7695/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 7696/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 7697/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 7698/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 7699/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 7700/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 7701/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 7702/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 7703/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 7704/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 7705/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 7706/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 7707/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 7708/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 7709/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 7710/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 7711/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 7712/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 7713/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 7714/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7715/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 7716/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 7717/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7718/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 7719/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7720/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 7721/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 7722/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7723/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7724/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7725/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 7726/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7727/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7728/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7729/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 7730/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7731/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7732/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7733/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7734/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 7735/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 7736/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 7737/25000, Training Loss: 0.000022, lr : 0.008500\n",
      "Epoch: 7738/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 7739/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7740/25000, Training Loss: 0.000023, lr : 0.008500\n",
      "Epoch: 7741/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 7742/25000, Training Loss: 0.000023, lr : 0.008500\n",
      "Epoch: 7743/25000, Training Loss: 0.000022, lr : 0.008500\n",
      "Epoch: 7744/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 7745/25000, Training Loss: 0.000023, lr : 0.008500\n",
      "Epoch: 7746/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 7747/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 7748/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 7749/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7750/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 7751/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 7752/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 7753/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 7754/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7755/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 7756/25000, Training Loss: 0.000088, lr : 0.008500\n",
      "Epoch: 7757/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 7758/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 7759/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 7760/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 7761/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 7762/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 7763/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 7764/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 7765/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 7766/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7767/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 7768/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7769/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 7770/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 7771/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 7772/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 7773/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7774/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 7775/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7776/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7777/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7778/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 7779/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 7780/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 7781/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7782/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 7783/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 7784/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 7785/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7786/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7787/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7788/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7789/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 7790/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 7791/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7792/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7793/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7794/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7795/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7796/25000, Training Loss: 0.000024, lr : 0.008500\n",
      "Epoch: 7797/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7798/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7799/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 7800/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7801/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7802/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 7803/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7804/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 7805/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 7806/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 7807/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7808/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 7809/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 7810/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 7811/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 7812/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7813/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7814/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7815/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7816/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7817/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 7818/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 7819/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 7820/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7821/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 7822/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7823/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7824/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7825/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7826/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 7827/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 7828/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 7829/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 7830/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 7831/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 7832/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7833/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7834/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7835/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 7836/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 7837/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 7838/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 7839/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 7840/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 7841/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 7842/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 7843/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7844/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 7845/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7846/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 7847/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 7848/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7849/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7850/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 7851/25000, Training Loss: 0.000134, lr : 0.008500\n",
      "Epoch: 7852/25000, Training Loss: 0.000238, lr : 0.008500\n",
      "Epoch: 7853/25000, Training Loss: 0.000275, lr : 0.008500\n",
      "Epoch: 7854/25000, Training Loss: 0.000256, lr : 0.008500\n",
      "Epoch: 7855/25000, Training Loss: 0.000184, lr : 0.008500\n",
      "Epoch: 7856/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 7857/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 7858/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 7859/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 7860/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 7861/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 7862/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 7863/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 7864/25000, Training Loss: 0.000111, lr : 0.008500\n",
      "Epoch: 7865/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 7866/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 7867/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 7868/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 7869/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 7870/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 7871/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 7872/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 7873/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 7874/25000, Training Loss: 0.000142, lr : 0.008500\n",
      "Epoch: 7875/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 7876/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 7877/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 7878/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 7879/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 7880/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 7881/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 7882/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 7883/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7884/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 7885/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 7886/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 7887/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 7888/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 7889/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 7890/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 7891/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 7892/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 7893/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 7894/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 7895/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 7896/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 7897/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 7898/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 7899/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 7900/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 7901/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7902/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 7903/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7904/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7905/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 7906/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7907/25000, Training Loss: 0.000024, lr : 0.008500\n",
      "Epoch: 7908/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 7909/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 7910/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 7911/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 7912/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 7913/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 7914/25000, Training Loss: 0.000023, lr : 0.008500\n",
      "Epoch: 7915/25000, Training Loss: 0.000024, lr : 0.008500\n",
      "Epoch: 7916/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 7917/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 7918/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 7919/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 7920/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 7921/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 7922/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 7923/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 7924/25000, Training Loss: 0.000250, lr : 0.008500\n",
      "Epoch: 7925/25000, Training Loss: 0.000158, lr : 0.008500\n",
      "Epoch: 7926/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 7927/25000, Training Loss: 0.000274, lr : 0.008500\n",
      "Epoch: 7928/25000, Training Loss: 0.000243, lr : 0.008500\n",
      "Epoch: 7929/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 7930/25000, Training Loss: 0.000153, lr : 0.008500\n",
      "Epoch: 7931/25000, Training Loss: 0.000205, lr : 0.008500\n",
      "Epoch: 7932/25000, Training Loss: 0.000363, lr : 0.008500\n",
      "Epoch: 7933/25000, Training Loss: 0.000395, lr : 0.008500\n",
      "Epoch: 7934/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 7935/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 7936/25000, Training Loss: 0.000363, lr : 0.008500\n",
      "Epoch: 7937/25000, Training Loss: 0.000227, lr : 0.008500\n",
      "Epoch: 7938/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 7939/25000, Training Loss: 0.000214, lr : 0.008500\n",
      "Epoch: 7940/25000, Training Loss: 0.000133, lr : 0.008500\n",
      "Epoch: 7941/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 7942/25000, Training Loss: 0.000233, lr : 0.008500\n",
      "Epoch: 7943/25000, Training Loss: 0.000163, lr : 0.008500\n",
      "Epoch: 7944/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 7945/25000, Training Loss: 0.000221, lr : 0.008500\n",
      "Epoch: 7946/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 7947/25000, Training Loss: 0.000231, lr : 0.008500\n",
      "Epoch: 7948/25000, Training Loss: 0.000372, lr : 0.008500\n",
      "Epoch: 7949/25000, Training Loss: 0.000148, lr : 0.008500\n",
      "Epoch: 7950/25000, Training Loss: 0.000179, lr : 0.008500\n",
      "Epoch: 7951/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 7952/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 7953/25000, Training Loss: 0.000192, lr : 0.008500\n",
      "Epoch: 7954/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 7955/25000, Training Loss: 0.000176, lr : 0.008500\n",
      "Epoch: 7956/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 7957/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 7958/25000, Training Loss: 0.000151, lr : 0.008500\n",
      "Epoch: 7959/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 7960/25000, Training Loss: 0.000166, lr : 0.008500\n",
      "Epoch: 7961/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 7962/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 7963/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 7964/25000, Training Loss: 0.000091, lr : 0.008500\n",
      "Epoch: 7965/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 7966/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 7967/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 7968/25000, Training Loss: 0.000127, lr : 0.008500\n",
      "Epoch: 7969/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 7970/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 7971/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 7972/25000, Training Loss: 0.000108, lr : 0.008500\n",
      "Epoch: 7973/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 7974/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 7975/25000, Training Loss: 0.000231, lr : 0.008500\n",
      "Epoch: 7976/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 7977/25000, Training Loss: 0.000241, lr : 0.008500\n",
      "Epoch: 7978/25000, Training Loss: 0.000239, lr : 0.008500\n",
      "Epoch: 7979/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 7980/25000, Training Loss: 0.000143, lr : 0.008500\n",
      "Epoch: 7981/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 7982/25000, Training Loss: 0.000188, lr : 0.008500\n",
      "Epoch: 7983/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 7984/25000, Training Loss: 0.000180, lr : 0.008500\n",
      "Epoch: 7985/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 7986/25000, Training Loss: 0.000150, lr : 0.008500\n",
      "Epoch: 7987/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 7988/25000, Training Loss: 0.000126, lr : 0.008500\n",
      "Epoch: 7989/25000, Training Loss: 0.000117, lr : 0.008500\n",
      "Epoch: 7990/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 7991/25000, Training Loss: 0.000138, lr : 0.008500\n",
      "Epoch: 7992/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 7993/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 7994/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 7995/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 7996/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 7997/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 7998/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 7999/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 8000/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 8001/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 8002/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 8003/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 8004/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 8005/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 8006/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 8007/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 8008/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 8009/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 8010/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 8011/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 8012/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 8013/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 8014/25000, Training Loss: 0.000076, lr : 0.008500\n",
      "Epoch: 8015/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 8016/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 8017/25000, Training Loss: 0.000096, lr : 0.008500\n",
      "Epoch: 8018/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 8019/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 8020/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 8021/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 8022/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 8023/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 8024/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 8025/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 8026/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 8027/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 8028/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 8029/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 8030/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 8031/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 8032/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 8033/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 8034/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 8035/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 8036/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 8037/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 8038/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 8039/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 8040/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 8041/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 8042/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 8043/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 8044/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 8045/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 8046/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 8047/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 8048/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 8049/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 8050/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 8051/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 8052/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 8053/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 8054/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 8055/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 8056/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 8057/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 8058/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 8059/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 8060/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 8061/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 8062/25000, Training Loss: 0.000024, lr : 0.008500\n",
      "Epoch: 8063/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 8064/25000, Training Loss: 0.000022, lr : 0.008500\n",
      "Epoch: 8065/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 8066/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 8067/25000, Training Loss: 0.000021, lr : 0.008500\n",
      "Epoch: 8068/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 8069/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 8070/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 8071/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 8072/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 8073/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 8074/25000, Training Loss: 0.000022, lr : 0.008500\n",
      "Epoch: 8075/25000, Training Loss: 0.000022, lr : 0.008500\n",
      "Epoch: 8076/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 8077/25000, Training Loss: 0.000018, lr : 0.008500\n",
      "Epoch: 8078/25000, Training Loss: 0.000019, lr : 0.008500\n",
      "Epoch: 8079/25000, Training Loss: 0.000023, lr : 0.008500\n",
      "Epoch: 8080/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 8081/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 8082/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 8083/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 8084/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 8085/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 8086/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 8087/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 8088/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 8089/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 8090/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 8091/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 8092/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 8093/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 8094/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 8095/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 8096/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 8097/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 8098/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 8099/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 8100/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 8101/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 8102/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 8103/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 8104/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 8105/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 8106/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 8107/25000, Training Loss: 0.000109, lr : 0.008500\n",
      "Epoch: 8108/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 8109/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 8110/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 8111/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 8112/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 8113/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 8114/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 8115/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 8116/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 8117/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 8118/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 8119/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 8120/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 8121/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 8122/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 8123/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 8124/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 8125/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 8126/25000, Training Loss: 0.000043, lr : 0.008500\n",
      "Epoch: 8127/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 8128/25000, Training Loss: 0.000098, lr : 0.008500\n",
      "Epoch: 8129/25000, Training Loss: 0.000065, lr : 0.008500\n",
      "Epoch: 8130/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 8131/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 8132/25000, Training Loss: 0.000132, lr : 0.008500\n",
      "Epoch: 8133/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 8134/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 8135/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 8136/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 8137/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 8138/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 8139/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 8140/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 8141/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 8142/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 8143/25000, Training Loss: 0.000081, lr : 0.008500\n",
      "Epoch: 8144/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 8145/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 8146/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 8147/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 8148/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 8149/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 8150/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 8151/25000, Training Loss: 0.000032, lr : 0.008500\n",
      "Epoch: 8152/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 8153/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 8154/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 8155/25000, Training Loss: 0.000036, lr : 0.008500\n",
      "Epoch: 8156/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 8157/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 8158/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 8159/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 8160/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 8161/25000, Training Loss: 0.000101, lr : 0.008500\n",
      "Epoch: 8162/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 8163/25000, Training Loss: 0.000157, lr : 0.008500\n",
      "Epoch: 8164/25000, Training Loss: 0.000219, lr : 0.008500\n",
      "Epoch: 8165/25000, Training Loss: 0.000207, lr : 0.008500\n",
      "Epoch: 8166/25000, Training Loss: 0.000185, lr : 0.008500\n",
      "Epoch: 8167/25000, Training Loss: 0.000345, lr : 0.008500\n",
      "Epoch: 8168/25000, Training Loss: 0.000667, lr : 0.008500\n",
      "Epoch: 8169/25000, Training Loss: 0.000372, lr : 0.008500\n",
      "Epoch: 8170/25000, Training Loss: 0.000325, lr : 0.008500\n",
      "Epoch: 8171/25000, Training Loss: 0.000245, lr : 0.008500\n",
      "Epoch: 8172/25000, Training Loss: 0.000296, lr : 0.008500\n",
      "Epoch: 8173/25000, Training Loss: 0.000244, lr : 0.008500\n",
      "Epoch: 8174/25000, Training Loss: 0.000320, lr : 0.008500\n",
      "Epoch: 8175/25000, Training Loss: 0.000414, lr : 0.008500\n",
      "Epoch: 8176/25000, Training Loss: 0.000257, lr : 0.008500\n",
      "Epoch: 8177/25000, Training Loss: 0.000266, lr : 0.008500\n",
      "Epoch: 8178/25000, Training Loss: 0.000218, lr : 0.008500\n",
      "Epoch: 8179/25000, Training Loss: 0.000203, lr : 0.008500\n",
      "Epoch: 8180/25000, Training Loss: 0.000237, lr : 0.008500\n",
      "Epoch: 8181/25000, Training Loss: 0.000155, lr : 0.008500\n",
      "Epoch: 8182/25000, Training Loss: 0.000183, lr : 0.008500\n",
      "Epoch: 8183/25000, Training Loss: 0.000164, lr : 0.008500\n",
      "Epoch: 8184/25000, Training Loss: 0.000211, lr : 0.008500\n",
      "Epoch: 8185/25000, Training Loss: 0.000177, lr : 0.008500\n",
      "Epoch: 8186/25000, Training Loss: 0.000145, lr : 0.008500\n",
      "Epoch: 8187/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 8188/25000, Training Loss: 0.000144, lr : 0.008500\n",
      "Epoch: 8189/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 8190/25000, Training Loss: 0.000156, lr : 0.008500\n",
      "Epoch: 8191/25000, Training Loss: 0.000336, lr : 0.008500\n",
      "Epoch: 8192/25000, Training Loss: 0.000187, lr : 0.008500\n",
      "Epoch: 8193/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 8194/25000, Training Loss: 0.000158, lr : 0.008500\n",
      "Epoch: 8195/25000, Training Loss: 0.000167, lr : 0.008500\n",
      "Epoch: 8196/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 8197/25000, Training Loss: 0.000181, lr : 0.008500\n",
      "Epoch: 8198/25000, Training Loss: 0.000257, lr : 0.008500\n",
      "Epoch: 8199/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 8200/25000, Training Loss: 0.000130, lr : 0.008500\n",
      "Epoch: 8201/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 8202/25000, Training Loss: 0.000222, lr : 0.008500\n",
      "Epoch: 8203/25000, Training Loss: 0.000246, lr : 0.008500\n",
      "Epoch: 8204/25000, Training Loss: 0.000149, lr : 0.008500\n",
      "Epoch: 8205/25000, Training Loss: 0.000152, lr : 0.008500\n",
      "Epoch: 8206/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 8207/25000, Training Loss: 0.000115, lr : 0.008500\n",
      "Epoch: 8208/25000, Training Loss: 0.000129, lr : 0.008500\n",
      "Epoch: 8209/25000, Training Loss: 0.000122, lr : 0.008500\n",
      "Epoch: 8210/25000, Training Loss: 0.000106, lr : 0.008500\n",
      "Epoch: 8211/25000, Training Loss: 0.000110, lr : 0.008500\n",
      "Epoch: 8212/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 8213/25000, Training Loss: 0.000083, lr : 0.008500\n",
      "Epoch: 8214/25000, Training Loss: 0.000097, lr : 0.008500\n",
      "Epoch: 8215/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 8216/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 8217/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 8218/25000, Training Loss: 0.000099, lr : 0.008500\n",
      "Epoch: 8219/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 8220/25000, Training Loss: 0.000063, lr : 0.008500\n",
      "Epoch: 8221/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 8222/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 8223/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 8224/25000, Training Loss: 0.000090, lr : 0.008500\n",
      "Epoch: 8225/25000, Training Loss: 0.000125, lr : 0.008500\n",
      "Epoch: 8226/25000, Training Loss: 0.000112, lr : 0.008500\n",
      "Epoch: 8227/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 8228/25000, Training Loss: 0.000169, lr : 0.008500\n",
      "Epoch: 8229/25000, Training Loss: 0.000140, lr : 0.008500\n",
      "Epoch: 8230/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 8231/25000, Training Loss: 0.000200, lr : 0.008500\n",
      "Epoch: 8232/25000, Training Loss: 0.000162, lr : 0.008500\n",
      "Epoch: 8233/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 8234/25000, Training Loss: 0.000188, lr : 0.008500\n",
      "Epoch: 8235/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 8236/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 8237/25000, Training Loss: 0.000136, lr : 0.008500\n",
      "Epoch: 8238/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 8239/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 8240/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 8241/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 8242/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 8243/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 8244/25000, Training Loss: 0.000070, lr : 0.008500\n",
      "Epoch: 8245/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 8246/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 8247/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 8248/25000, Training Loss: 0.000037, lr : 0.008500\n",
      "Epoch: 8249/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 8250/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 8251/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 8252/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 8253/25000, Training Loss: 0.000040, lr : 0.008500\n",
      "Epoch: 8254/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 8255/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 8256/25000, Training Loss: 0.000061, lr : 0.008500\n",
      "Epoch: 8257/25000, Training Loss: 0.000086, lr : 0.008500\n",
      "Epoch: 8258/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 8259/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 8260/25000, Training Loss: 0.000095, lr : 0.008500\n",
      "Epoch: 8261/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 8262/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 8263/25000, Training Loss: 0.000064, lr : 0.008500\n",
      "Epoch: 8264/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 8265/25000, Training Loss: 0.000057, lr : 0.008500\n",
      "Epoch: 8266/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 8267/25000, Training Loss: 0.000042, lr : 0.008500\n",
      "Epoch: 8268/25000, Training Loss: 0.000033, lr : 0.008500\n",
      "Epoch: 8269/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 8270/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 8271/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 8272/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 8273/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 8274/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 8275/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 8276/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 8277/25000, Training Loss: 0.000023, lr : 0.008500\n",
      "Epoch: 8278/25000, Training Loss: 0.000024, lr : 0.008500\n",
      "Epoch: 8279/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 8280/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 8281/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 8282/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 8283/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 8284/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 8285/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 8286/25000, Training Loss: 0.000038, lr : 0.008500\n",
      "Epoch: 8287/25000, Training Loss: 0.000029, lr : 0.008500\n",
      "Epoch: 8288/25000, Training Loss: 0.000027, lr : 0.008500\n",
      "Epoch: 8289/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 8290/25000, Training Loss: 0.000021, lr : 0.008500\n",
      "Epoch: 8291/25000, Training Loss: 0.000021, lr : 0.008500\n",
      "Epoch: 8292/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 8293/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 8294/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 8295/25000, Training Loss: 0.000025, lr : 0.008500\n",
      "Epoch: 8296/25000, Training Loss: 0.000028, lr : 0.008500\n",
      "Epoch: 8297/25000, Training Loss: 0.000035, lr : 0.008500\n",
      "Epoch: 8298/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 8299/25000, Training Loss: 0.000026, lr : 0.008500\n",
      "Epoch: 8300/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 8301/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 8302/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 8303/25000, Training Loss: 0.000039, lr : 0.008500\n",
      "Epoch: 8304/25000, Training Loss: 0.000022, lr : 0.008500\n",
      "Epoch: 8305/25000, Training Loss: 0.000041, lr : 0.008500\n",
      "Epoch: 8306/25000, Training Loss: 0.000105, lr : 0.008500\n",
      "Epoch: 8307/25000, Training Loss: 0.000123, lr : 0.008500\n",
      "Epoch: 8308/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 8309/25000, Training Loss: 0.000030, lr : 0.008500\n",
      "Epoch: 8310/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 8311/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 8312/25000, Training Loss: 0.000034, lr : 0.008500\n",
      "Epoch: 8313/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 8314/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 8315/25000, Training Loss: 0.000048, lr : 0.008500\n",
      "Epoch: 8316/25000, Training Loss: 0.000031, lr : 0.008500\n",
      "Epoch: 8317/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 8318/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 8319/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 8320/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 8321/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 8322/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 8323/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 8324/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 8325/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 8326/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 8327/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 8328/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 8329/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 8330/25000, Training Loss: 0.000049, lr : 0.008500\n",
      "Epoch: 8331/25000, Training Loss: 0.000078, lr : 0.008500\n",
      "Epoch: 8332/25000, Training Loss: 0.000124, lr : 0.008500\n",
      "Epoch: 8333/25000, Training Loss: 0.000082, lr : 0.008500\n",
      "Epoch: 8334/25000, Training Loss: 0.000055, lr : 0.008500\n",
      "Epoch: 8335/25000, Training Loss: 0.000047, lr : 0.008500\n",
      "Epoch: 8336/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 8337/25000, Training Loss: 0.000052, lr : 0.008500\n",
      "Epoch: 8338/25000, Training Loss: 0.000050, lr : 0.008500\n",
      "Epoch: 8339/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 8340/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 8341/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 8342/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 8343/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 8344/25000, Training Loss: 0.000089, lr : 0.008500\n",
      "Epoch: 8345/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 8346/25000, Training Loss: 0.000094, lr : 0.008500\n",
      "Epoch: 8347/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 8348/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 8349/25000, Training Loss: 0.000107, lr : 0.008500\n",
      "Epoch: 8350/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 8351/25000, Training Loss: 0.000080, lr : 0.008500\n",
      "Epoch: 8352/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 8353/25000, Training Loss: 0.000147, lr : 0.008500\n",
      "Epoch: 8354/25000, Training Loss: 0.000116, lr : 0.008500\n",
      "Epoch: 8355/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 8356/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 8357/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 8358/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 8359/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 8360/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 8361/25000, Training Loss: 0.000084, lr : 0.008500\n",
      "Epoch: 8362/25000, Training Loss: 0.000189, lr : 0.008500\n",
      "Epoch: 8363/25000, Training Loss: 0.000165, lr : 0.008500\n",
      "Epoch: 8364/25000, Training Loss: 0.000072, lr : 0.008500\n",
      "Epoch: 8365/25000, Training Loss: 0.000104, lr : 0.008500\n",
      "Epoch: 8366/25000, Training Loss: 0.000121, lr : 0.008500\n",
      "Epoch: 8367/25000, Training Loss: 0.000100, lr : 0.008500\n",
      "Epoch: 8368/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 8369/25000, Training Loss: 0.000135, lr : 0.008500\n",
      "Epoch: 8370/25000, Training Loss: 0.000146, lr : 0.008500\n",
      "Epoch: 8371/25000, Training Loss: 0.000102, lr : 0.008500\n",
      "Epoch: 8372/25000, Training Loss: 0.000060, lr : 0.008500\n",
      "Epoch: 8373/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 8374/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 8375/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 8376/25000, Training Loss: 0.000066, lr : 0.008500\n",
      "Epoch: 8377/25000, Training Loss: 0.000073, lr : 0.008500\n",
      "Epoch: 8378/25000, Training Loss: 0.000051, lr : 0.008500\n",
      "Epoch: 8379/25000, Training Loss: 0.000053, lr : 0.008500\n",
      "Epoch: 8380/25000, Training Loss: 0.000085, lr : 0.008500\n",
      "Epoch: 8381/25000, Training Loss: 0.000113, lr : 0.008500\n",
      "Epoch: 8382/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 8383/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 8384/25000, Training Loss: 0.000118, lr : 0.008500\n",
      "Epoch: 8385/25000, Training Loss: 0.000069, lr : 0.008500\n",
      "Epoch: 8386/25000, Training Loss: 0.000045, lr : 0.008500\n",
      "Epoch: 8387/25000, Training Loss: 0.000062, lr : 0.008500\n",
      "Epoch: 8388/25000, Training Loss: 0.000056, lr : 0.008500\n",
      "Epoch: 8389/25000, Training Loss: 0.000044, lr : 0.008500\n",
      "Epoch: 8390/25000, Training Loss: 0.000058, lr : 0.008500\n",
      "Epoch: 8391/25000, Training Loss: 0.000077, lr : 0.008500\n",
      "Epoch: 8392/25000, Training Loss: 0.000059, lr : 0.008500\n",
      "Epoch: 8393/25000, Training Loss: 0.000046, lr : 0.008500\n",
      "Epoch: 8394/25000, Training Loss: 0.000075, lr : 0.008500\n",
      "Epoch: 8395/25000, Training Loss: 0.000068, lr : 0.008500\n",
      "Epoch: 8396/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 8397/25000, Training Loss: 0.000139, lr : 0.008500\n",
      "Epoch: 8398/25000, Training Loss: 0.000141, lr : 0.008500\n",
      "Epoch: 8399/25000, Training Loss: 0.000093, lr : 0.008500\n",
      "Epoch: 8400/25000, Training Loss: 0.000087, lr : 0.008500\n",
      "Epoch: 8401/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 8402/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 8403/25000, Training Loss: 0.000067, lr : 0.008500\n",
      "Epoch: 8404/25000, Training Loss: 0.000092, lr : 0.008500\n",
      "Epoch: 8405/25000, Training Loss: 0.000071, lr : 0.008500\n",
      "Epoch: 8406/25000, Training Loss: 0.000054, lr : 0.008500\n",
      "Epoch: 8407/25000, Training Loss: 0.000103, lr : 0.008500\n",
      "Epoch: 8408/25000, Training Loss: 0.000119, lr : 0.008500\n",
      "Epoch: 8409/25000, Training Loss: 0.000074, lr : 0.008500\n",
      "Epoch: 8410/25000, Training Loss: 0.000069, lr : 0.007225\n",
      "Epoch: 8411/25000, Training Loss: 0.000057, lr : 0.007225\n",
      "Epoch: 8412/25000, Training Loss: 0.000057, lr : 0.007225\n",
      "Epoch: 8413/25000, Training Loss: 0.000051, lr : 0.007225\n",
      "Epoch: 8414/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 8415/25000, Training Loss: 0.000046, lr : 0.007225\n",
      "Epoch: 8416/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 8417/25000, Training Loss: 0.000039, lr : 0.007225\n",
      "Epoch: 8418/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8419/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 8420/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 8421/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 8422/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 8423/25000, Training Loss: 0.000037, lr : 0.007225\n",
      "Epoch: 8424/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8425/25000, Training Loss: 0.000044, lr : 0.007225\n",
      "Epoch: 8426/25000, Training Loss: 0.000039, lr : 0.007225\n",
      "Epoch: 8427/25000, Training Loss: 0.000037, lr : 0.007225\n",
      "Epoch: 8428/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8429/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8430/25000, Training Loss: 0.000022, lr : 0.007225\n",
      "Epoch: 8431/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8432/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8433/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8434/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 8435/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8436/25000, Training Loss: 0.000019, lr : 0.007225\n",
      "Epoch: 8437/25000, Training Loss: 0.000019, lr : 0.007225\n",
      "Epoch: 8438/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 8439/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8440/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8441/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8442/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8443/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 8444/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8445/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8446/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8447/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8448/25000, Training Loss: 0.000015, lr : 0.007225\n",
      "Epoch: 8449/25000, Training Loss: 0.000015, lr : 0.007225\n",
      "Epoch: 8450/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8451/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 8452/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8453/25000, Training Loss: 0.000015, lr : 0.007225\n",
      "Epoch: 8454/25000, Training Loss: 0.000019, lr : 0.007225\n",
      "Epoch: 8455/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8456/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8457/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8458/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8459/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8460/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8461/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8462/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8463/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8464/25000, Training Loss: 0.000015, lr : 0.007225\n",
      "Epoch: 8465/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8466/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 8467/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8468/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 8469/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8470/25000, Training Loss: 0.000019, lr : 0.007225\n",
      "Epoch: 8471/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8472/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8473/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8474/25000, Training Loss: 0.000013, lr : 0.007225\n",
      "Epoch: 8475/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8476/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8477/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8478/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8479/25000, Training Loss: 0.000014, lr : 0.007225\n",
      "Epoch: 8480/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8481/25000, Training Loss: 0.000014, lr : 0.007225\n",
      "Epoch: 8482/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8483/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8484/25000, Training Loss: 0.000019, lr : 0.007225\n",
      "Epoch: 8485/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8486/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8487/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8488/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8489/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8490/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8491/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8492/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8493/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8494/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8495/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8496/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8497/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8498/25000, Training Loss: 0.000015, lr : 0.007225\n",
      "Epoch: 8499/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8500/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8501/25000, Training Loss: 0.000013, lr : 0.007225\n",
      "Epoch: 8502/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8503/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8504/25000, Training Loss: 0.000013, lr : 0.007225\n",
      "Epoch: 8505/25000, Training Loss: 0.000014, lr : 0.007225\n",
      "Epoch: 8506/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8507/25000, Training Loss: 0.000014, lr : 0.007225\n",
      "Epoch: 8508/25000, Training Loss: 0.000014, lr : 0.007225\n",
      "Epoch: 8509/25000, Training Loss: 0.000014, lr : 0.007225\n",
      "Epoch: 8510/25000, Training Loss: 0.000014, lr : 0.007225\n",
      "Epoch: 8511/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8512/25000, Training Loss: 0.000019, lr : 0.007225\n",
      "Epoch: 8513/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 8514/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8515/25000, Training Loss: 0.000019, lr : 0.007225\n",
      "Epoch: 8516/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 8517/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8518/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 8519/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 8520/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8521/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8522/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8523/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8524/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8525/25000, Training Loss: 0.000022, lr : 0.007225\n",
      "Epoch: 8526/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 8527/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8528/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 8529/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 8530/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8531/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8532/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8533/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 8534/25000, Training Loss: 0.000019, lr : 0.007225\n",
      "Epoch: 8535/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8536/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8537/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8538/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8539/25000, Training Loss: 0.000022, lr : 0.007225\n",
      "Epoch: 8540/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8541/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8542/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8543/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8544/25000, Training Loss: 0.000015, lr : 0.007225\n",
      "Epoch: 8545/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8546/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8547/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8548/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8549/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8550/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8551/25000, Training Loss: 0.000022, lr : 0.007225\n",
      "Epoch: 8552/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8553/25000, Training Loss: 0.000019, lr : 0.007225\n",
      "Epoch: 8554/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 8555/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8556/25000, Training Loss: 0.000019, lr : 0.007225\n",
      "Epoch: 8557/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8558/25000, Training Loss: 0.000022, lr : 0.007225\n",
      "Epoch: 8559/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8560/25000, Training Loss: 0.000040, lr : 0.007225\n",
      "Epoch: 8561/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8562/25000, Training Loss: 0.000046, lr : 0.007225\n",
      "Epoch: 8563/25000, Training Loss: 0.000046, lr : 0.007225\n",
      "Epoch: 8564/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 8565/25000, Training Loss: 0.000053, lr : 0.007225\n",
      "Epoch: 8566/25000, Training Loss: 0.000048, lr : 0.007225\n",
      "Epoch: 8567/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 8568/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 8569/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 8570/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8571/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 8572/25000, Training Loss: 0.000038, lr : 0.007225\n",
      "Epoch: 8573/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 8574/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8575/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8576/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8577/25000, Training Loss: 0.000022, lr : 0.007225\n",
      "Epoch: 8578/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8579/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8580/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8581/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8582/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8583/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8584/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8585/25000, Training Loss: 0.000044, lr : 0.007225\n",
      "Epoch: 8586/25000, Training Loss: 0.000093, lr : 0.007225\n",
      "Epoch: 8587/25000, Training Loss: 0.000084, lr : 0.007225\n",
      "Epoch: 8588/25000, Training Loss: 0.000039, lr : 0.007225\n",
      "Epoch: 8589/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8590/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 8591/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 8592/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 8593/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 8594/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8595/25000, Training Loss: 0.000037, lr : 0.007225\n",
      "Epoch: 8596/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 8597/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8598/25000, Training Loss: 0.000048, lr : 0.007225\n",
      "Epoch: 8599/25000, Training Loss: 0.000054, lr : 0.007225\n",
      "Epoch: 8600/25000, Training Loss: 0.000042, lr : 0.007225\n",
      "Epoch: 8601/25000, Training Loss: 0.000040, lr : 0.007225\n",
      "Epoch: 8602/25000, Training Loss: 0.000117, lr : 0.007225\n",
      "Epoch: 8603/25000, Training Loss: 0.000100, lr : 0.007225\n",
      "Epoch: 8604/25000, Training Loss: 0.000064, lr : 0.007225\n",
      "Epoch: 8605/25000, Training Loss: 0.000140, lr : 0.007225\n",
      "Epoch: 8606/25000, Training Loss: 0.000142, lr : 0.007225\n",
      "Epoch: 8607/25000, Training Loss: 0.000072, lr : 0.007225\n",
      "Epoch: 8608/25000, Training Loss: 0.000070, lr : 0.007225\n",
      "Epoch: 8609/25000, Training Loss: 0.000102, lr : 0.007225\n",
      "Epoch: 8610/25000, Training Loss: 0.000095, lr : 0.007225\n",
      "Epoch: 8611/25000, Training Loss: 0.000052, lr : 0.007225\n",
      "Epoch: 8612/25000, Training Loss: 0.000054, lr : 0.007225\n",
      "Epoch: 8613/25000, Training Loss: 0.000061, lr : 0.007225\n",
      "Epoch: 8614/25000, Training Loss: 0.000053, lr : 0.007225\n",
      "Epoch: 8615/25000, Training Loss: 0.000054, lr : 0.007225\n",
      "Epoch: 8616/25000, Training Loss: 0.000056, lr : 0.007225\n",
      "Epoch: 8617/25000, Training Loss: 0.000042, lr : 0.007225\n",
      "Epoch: 8618/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 8619/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 8620/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 8621/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 8622/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 8623/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8624/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8625/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 8626/25000, Training Loss: 0.000039, lr : 0.007225\n",
      "Epoch: 8627/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8628/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 8629/25000, Training Loss: 0.000051, lr : 0.007225\n",
      "Epoch: 8630/25000, Training Loss: 0.000037, lr : 0.007225\n",
      "Epoch: 8631/25000, Training Loss: 0.000038, lr : 0.007225\n",
      "Epoch: 8632/25000, Training Loss: 0.000048, lr : 0.007225\n",
      "Epoch: 8633/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8634/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 8635/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8636/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 8637/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 8638/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8639/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8640/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8641/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8642/25000, Training Loss: 0.000014, lr : 0.007225\n",
      "Epoch: 8643/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8644/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8645/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8646/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8647/25000, Training Loss: 0.000013, lr : 0.007225\n",
      "Epoch: 8648/25000, Training Loss: 0.000015, lr : 0.007225\n",
      "Epoch: 8649/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8650/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8651/25000, Training Loss: 0.000019, lr : 0.007225\n",
      "Epoch: 8652/25000, Training Loss: 0.000016, lr : 0.007225\n",
      "Epoch: 8653/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8654/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8655/25000, Training Loss: 0.000038, lr : 0.007225\n",
      "Epoch: 8656/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 8657/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8658/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8659/25000, Training Loss: 0.000037, lr : 0.007225\n",
      "Epoch: 8660/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 8661/25000, Training Loss: 0.000073, lr : 0.007225\n",
      "Epoch: 8662/25000, Training Loss: 0.000060, lr : 0.007225\n",
      "Epoch: 8663/25000, Training Loss: 0.000048, lr : 0.007225\n",
      "Epoch: 8664/25000, Training Loss: 0.000052, lr : 0.007225\n",
      "Epoch: 8665/25000, Training Loss: 0.000048, lr : 0.007225\n",
      "Epoch: 8666/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8667/25000, Training Loss: 0.000053, lr : 0.007225\n",
      "Epoch: 8668/25000, Training Loss: 0.000102, lr : 0.007225\n",
      "Epoch: 8669/25000, Training Loss: 0.000073, lr : 0.007225\n",
      "Epoch: 8670/25000, Training Loss: 0.000038, lr : 0.007225\n",
      "Epoch: 8671/25000, Training Loss: 0.000089, lr : 0.007225\n",
      "Epoch: 8672/25000, Training Loss: 0.000092, lr : 0.007225\n",
      "Epoch: 8673/25000, Training Loss: 0.000064, lr : 0.007225\n",
      "Epoch: 8674/25000, Training Loss: 0.000060, lr : 0.007225\n",
      "Epoch: 8675/25000, Training Loss: 0.000102, lr : 0.007225\n",
      "Epoch: 8676/25000, Training Loss: 0.000129, lr : 0.007225\n",
      "Epoch: 8677/25000, Training Loss: 0.000086, lr : 0.007225\n",
      "Epoch: 8678/25000, Training Loss: 0.000081, lr : 0.007225\n",
      "Epoch: 8679/25000, Training Loss: 0.000096, lr : 0.007225\n",
      "Epoch: 8680/25000, Training Loss: 0.000141, lr : 0.007225\n",
      "Epoch: 8681/25000, Training Loss: 0.000101, lr : 0.007225\n",
      "Epoch: 8682/25000, Training Loss: 0.000089, lr : 0.007225\n",
      "Epoch: 8683/25000, Training Loss: 0.000108, lr : 0.007225\n",
      "Epoch: 8684/25000, Training Loss: 0.000116, lr : 0.007225\n",
      "Epoch: 8685/25000, Training Loss: 0.000067, lr : 0.007225\n",
      "Epoch: 8686/25000, Training Loss: 0.000057, lr : 0.007225\n",
      "Epoch: 8687/25000, Training Loss: 0.000072, lr : 0.007225\n",
      "Epoch: 8688/25000, Training Loss: 0.000053, lr : 0.007225\n",
      "Epoch: 8689/25000, Training Loss: 0.000040, lr : 0.007225\n",
      "Epoch: 8690/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 8691/25000, Training Loss: 0.000044, lr : 0.007225\n",
      "Epoch: 8692/25000, Training Loss: 0.000037, lr : 0.007225\n",
      "Epoch: 8693/25000, Training Loss: 0.000040, lr : 0.007225\n",
      "Epoch: 8694/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 8695/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 8696/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 8697/25000, Training Loss: 0.000051, lr : 0.007225\n",
      "Epoch: 8698/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8699/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 8700/25000, Training Loss: 0.000047, lr : 0.007225\n",
      "Epoch: 8701/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 8702/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 8703/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8704/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8705/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8706/25000, Training Loss: 0.000054, lr : 0.007225\n",
      "Epoch: 8707/25000, Training Loss: 0.000061, lr : 0.007225\n",
      "Epoch: 8708/25000, Training Loss: 0.000044, lr : 0.007225\n",
      "Epoch: 8709/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8710/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 8711/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 8712/25000, Training Loss: 0.000055, lr : 0.007225\n",
      "Epoch: 8713/25000, Training Loss: 0.000048, lr : 0.007225\n",
      "Epoch: 8714/25000, Training Loss: 0.000038, lr : 0.007225\n",
      "Epoch: 8715/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 8716/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 8717/25000, Training Loss: 0.000039, lr : 0.007225\n",
      "Epoch: 8718/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 8719/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8720/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8721/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 8722/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 8723/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8724/25000, Training Loss: 0.000037, lr : 0.007225\n",
      "Epoch: 8725/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8726/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8727/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8728/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8729/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 8730/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 8731/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8732/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8733/25000, Training Loss: 0.000022, lr : 0.007225\n",
      "Epoch: 8734/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8735/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 8736/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 8737/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8738/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8739/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8740/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8741/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8742/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8743/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 8744/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 8745/25000, Training Loss: 0.000054, lr : 0.007225\n",
      "Epoch: 8746/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8747/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8748/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8749/25000, Training Loss: 0.000022, lr : 0.007225\n",
      "Epoch: 8750/25000, Training Loss: 0.000022, lr : 0.007225\n",
      "Epoch: 8751/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 8752/25000, Training Loss: 0.000087, lr : 0.007225\n",
      "Epoch: 8753/25000, Training Loss: 0.000157, lr : 0.007225\n",
      "Epoch: 8754/25000, Training Loss: 0.000142, lr : 0.007225\n",
      "Epoch: 8755/25000, Training Loss: 0.000093, lr : 0.007225\n",
      "Epoch: 8756/25000, Training Loss: 0.000051, lr : 0.007225\n",
      "Epoch: 8757/25000, Training Loss: 0.000170, lr : 0.007225\n",
      "Epoch: 8758/25000, Training Loss: 0.000428, lr : 0.007225\n",
      "Epoch: 8759/25000, Training Loss: 0.000661, lr : 0.007225\n",
      "Epoch: 8760/25000, Training Loss: 0.000348, lr : 0.007225\n",
      "Epoch: 8761/25000, Training Loss: 0.000233, lr : 0.007225\n",
      "Epoch: 8762/25000, Training Loss: 0.000426, lr : 0.007225\n",
      "Epoch: 8763/25000, Training Loss: 0.000258, lr : 0.007225\n",
      "Epoch: 8764/25000, Training Loss: 0.000409, lr : 0.007225\n",
      "Epoch: 8765/25000, Training Loss: 0.000283, lr : 0.007225\n",
      "Epoch: 8766/25000, Training Loss: 0.000305, lr : 0.007225\n",
      "Epoch: 8767/25000, Training Loss: 0.000250, lr : 0.007225\n",
      "Epoch: 8768/25000, Training Loss: 0.000220, lr : 0.007225\n",
      "Epoch: 8769/25000, Training Loss: 0.000265, lr : 0.007225\n",
      "Epoch: 8770/25000, Training Loss: 0.000214, lr : 0.007225\n",
      "Epoch: 8771/25000, Training Loss: 0.000138, lr : 0.007225\n",
      "Epoch: 8772/25000, Training Loss: 0.000154, lr : 0.007225\n",
      "Epoch: 8773/25000, Training Loss: 0.000139, lr : 0.007225\n",
      "Epoch: 8774/25000, Training Loss: 0.000128, lr : 0.007225\n",
      "Epoch: 8775/25000, Training Loss: 0.000121, lr : 0.007225\n",
      "Epoch: 8776/25000, Training Loss: 0.000101, lr : 0.007225\n",
      "Epoch: 8777/25000, Training Loss: 0.000109, lr : 0.007225\n",
      "Epoch: 8778/25000, Training Loss: 0.000119, lr : 0.007225\n",
      "Epoch: 8779/25000, Training Loss: 0.000086, lr : 0.007225\n",
      "Epoch: 8780/25000, Training Loss: 0.000110, lr : 0.007225\n",
      "Epoch: 8781/25000, Training Loss: 0.000100, lr : 0.007225\n",
      "Epoch: 8782/25000, Training Loss: 0.000072, lr : 0.007225\n",
      "Epoch: 8783/25000, Training Loss: 0.000092, lr : 0.007225\n",
      "Epoch: 8784/25000, Training Loss: 0.000066, lr : 0.007225\n",
      "Epoch: 8785/25000, Training Loss: 0.000068, lr : 0.007225\n",
      "Epoch: 8786/25000, Training Loss: 0.000055, lr : 0.007225\n",
      "Epoch: 8787/25000, Training Loss: 0.000061, lr : 0.007225\n",
      "Epoch: 8788/25000, Training Loss: 0.000054, lr : 0.007225\n",
      "Epoch: 8789/25000, Training Loss: 0.000065, lr : 0.007225\n",
      "Epoch: 8790/25000, Training Loss: 0.000064, lr : 0.007225\n",
      "Epoch: 8791/25000, Training Loss: 0.000046, lr : 0.007225\n",
      "Epoch: 8792/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 8793/25000, Training Loss: 0.000046, lr : 0.007225\n",
      "Epoch: 8794/25000, Training Loss: 0.000037, lr : 0.007225\n",
      "Epoch: 8795/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8796/25000, Training Loss: 0.000042, lr : 0.007225\n",
      "Epoch: 8797/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 8798/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8799/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 8800/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 8801/25000, Training Loss: 0.000046, lr : 0.007225\n",
      "Epoch: 8802/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 8803/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 8804/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8805/25000, Training Loss: 0.000022, lr : 0.007225\n",
      "Epoch: 8806/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 8807/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8808/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8809/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8810/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8811/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8812/25000, Training Loss: 0.000022, lr : 0.007225\n",
      "Epoch: 8813/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8814/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8815/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 8816/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8817/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 8818/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 8819/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 8820/25000, Training Loss: 0.000018, lr : 0.007225\n",
      "Epoch: 8821/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 8822/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8823/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8824/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 8825/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 8826/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 8827/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 8828/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8829/25000, Training Loss: 0.000017, lr : 0.007225\n",
      "Epoch: 8830/25000, Training Loss: 0.000019, lr : 0.007225\n",
      "Epoch: 8831/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8832/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8833/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8834/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 8835/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 8836/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8837/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8838/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8839/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8840/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8841/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8842/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8843/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8844/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8845/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8846/25000, Training Loss: 0.000022, lr : 0.007225\n",
      "Epoch: 8847/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8848/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 8849/25000, Training Loss: 0.000022, lr : 0.007225\n",
      "Epoch: 8850/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 8851/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 8852/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 8853/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8854/25000, Training Loss: 0.000040, lr : 0.007225\n",
      "Epoch: 8855/25000, Training Loss: 0.000051, lr : 0.007225\n",
      "Epoch: 8856/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8857/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8858/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 8859/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8860/25000, Training Loss: 0.000052, lr : 0.007225\n",
      "Epoch: 8861/25000, Training Loss: 0.000046, lr : 0.007225\n",
      "Epoch: 8862/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 8863/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8864/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8865/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8866/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8867/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8868/25000, Training Loss: 0.000037, lr : 0.007225\n",
      "Epoch: 8869/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8870/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8871/25000, Training Loss: 0.000104, lr : 0.007225\n",
      "Epoch: 8872/25000, Training Loss: 0.000157, lr : 0.007225\n",
      "Epoch: 8873/25000, Training Loss: 0.000062, lr : 0.007225\n",
      "Epoch: 8874/25000, Training Loss: 0.000091, lr : 0.007225\n",
      "Epoch: 8875/25000, Training Loss: 0.000139, lr : 0.007225\n",
      "Epoch: 8876/25000, Training Loss: 0.000085, lr : 0.007225\n",
      "Epoch: 8877/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 8878/25000, Training Loss: 0.000070, lr : 0.007225\n",
      "Epoch: 8879/25000, Training Loss: 0.000108, lr : 0.007225\n",
      "Epoch: 8880/25000, Training Loss: 0.000088, lr : 0.007225\n",
      "Epoch: 8881/25000, Training Loss: 0.000047, lr : 0.007225\n",
      "Epoch: 8882/25000, Training Loss: 0.000060, lr : 0.007225\n",
      "Epoch: 8883/25000, Training Loss: 0.000056, lr : 0.007225\n",
      "Epoch: 8884/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 8885/25000, Training Loss: 0.000047, lr : 0.007225\n",
      "Epoch: 8886/25000, Training Loss: 0.000073, lr : 0.007225\n",
      "Epoch: 8887/25000, Training Loss: 0.000077, lr : 0.007225\n",
      "Epoch: 8888/25000, Training Loss: 0.000048, lr : 0.007225\n",
      "Epoch: 8889/25000, Training Loss: 0.000048, lr : 0.007225\n",
      "Epoch: 8890/25000, Training Loss: 0.000063, lr : 0.007225\n",
      "Epoch: 8891/25000, Training Loss: 0.000050, lr : 0.007225\n",
      "Epoch: 8892/25000, Training Loss: 0.000054, lr : 0.007225\n",
      "Epoch: 8893/25000, Training Loss: 0.000053, lr : 0.007225\n",
      "Epoch: 8894/25000, Training Loss: 0.000079, lr : 0.007225\n",
      "Epoch: 8895/25000, Training Loss: 0.000072, lr : 0.007225\n",
      "Epoch: 8896/25000, Training Loss: 0.000074, lr : 0.007225\n",
      "Epoch: 8897/25000, Training Loss: 0.000047, lr : 0.007225\n",
      "Epoch: 8898/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 8899/25000, Training Loss: 0.000063, lr : 0.007225\n",
      "Epoch: 8900/25000, Training Loss: 0.000086, lr : 0.007225\n",
      "Epoch: 8901/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 8902/25000, Training Loss: 0.000060, lr : 0.007225\n",
      "Epoch: 8903/25000, Training Loss: 0.000170, lr : 0.007225\n",
      "Epoch: 8904/25000, Training Loss: 0.000142, lr : 0.007225\n",
      "Epoch: 8905/25000, Training Loss: 0.000073, lr : 0.007225\n",
      "Epoch: 8906/25000, Training Loss: 0.000154, lr : 0.007225\n",
      "Epoch: 8907/25000, Training Loss: 0.000095, lr : 0.007225\n",
      "Epoch: 8908/25000, Training Loss: 0.000094, lr : 0.007225\n",
      "Epoch: 8909/25000, Training Loss: 0.000056, lr : 0.007225\n",
      "Epoch: 8910/25000, Training Loss: 0.000050, lr : 0.007225\n",
      "Epoch: 8911/25000, Training Loss: 0.000086, lr : 0.007225\n",
      "Epoch: 8912/25000, Training Loss: 0.000056, lr : 0.007225\n",
      "Epoch: 8913/25000, Training Loss: 0.000076, lr : 0.007225\n",
      "Epoch: 8914/25000, Training Loss: 0.000055, lr : 0.007225\n",
      "Epoch: 8915/25000, Training Loss: 0.000059, lr : 0.007225\n",
      "Epoch: 8916/25000, Training Loss: 0.000056, lr : 0.007225\n",
      "Epoch: 8917/25000, Training Loss: 0.000058, lr : 0.007225\n",
      "Epoch: 8918/25000, Training Loss: 0.000055, lr : 0.007225\n",
      "Epoch: 8919/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 8920/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 8921/25000, Training Loss: 0.000047, lr : 0.007225\n",
      "Epoch: 8922/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 8923/25000, Training Loss: 0.000062, lr : 0.007225\n",
      "Epoch: 8924/25000, Training Loss: 0.000080, lr : 0.007225\n",
      "Epoch: 8925/25000, Training Loss: 0.000060, lr : 0.007225\n",
      "Epoch: 8926/25000, Training Loss: 0.000051, lr : 0.007225\n",
      "Epoch: 8927/25000, Training Loss: 0.000042, lr : 0.007225\n",
      "Epoch: 8928/25000, Training Loss: 0.000042, lr : 0.007225\n",
      "Epoch: 8929/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 8930/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 8931/25000, Training Loss: 0.000053, lr : 0.007225\n",
      "Epoch: 8932/25000, Training Loss: 0.000039, lr : 0.007225\n",
      "Epoch: 8933/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 8934/25000, Training Loss: 0.000079, lr : 0.007225\n",
      "Epoch: 8935/25000, Training Loss: 0.000059, lr : 0.007225\n",
      "Epoch: 8936/25000, Training Loss: 0.000070, lr : 0.007225\n",
      "Epoch: 8937/25000, Training Loss: 0.000078, lr : 0.007225\n",
      "Epoch: 8938/25000, Training Loss: 0.000045, lr : 0.007225\n",
      "Epoch: 8939/25000, Training Loss: 0.000057, lr : 0.007225\n",
      "Epoch: 8940/25000, Training Loss: 0.000059, lr : 0.007225\n",
      "Epoch: 8941/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 8942/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 8943/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 8944/25000, Training Loss: 0.000037, lr : 0.007225\n",
      "Epoch: 8945/25000, Training Loss: 0.000042, lr : 0.007225\n",
      "Epoch: 8946/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 8947/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 8948/25000, Training Loss: 0.000048, lr : 0.007225\n",
      "Epoch: 8949/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 8950/25000, Training Loss: 0.000042, lr : 0.007225\n",
      "Epoch: 8951/25000, Training Loss: 0.000060, lr : 0.007225\n",
      "Epoch: 8952/25000, Training Loss: 0.000050, lr : 0.007225\n",
      "Epoch: 8953/25000, Training Loss: 0.000045, lr : 0.007225\n",
      "Epoch: 8954/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 8955/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8956/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 8957/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 8958/25000, Training Loss: 0.000050, lr : 0.007225\n",
      "Epoch: 8959/25000, Training Loss: 0.000037, lr : 0.007225\n",
      "Epoch: 8960/25000, Training Loss: 0.000056, lr : 0.007225\n",
      "Epoch: 8961/25000, Training Loss: 0.000066, lr : 0.007225\n",
      "Epoch: 8962/25000, Training Loss: 0.000085, lr : 0.007225\n",
      "Epoch: 8963/25000, Training Loss: 0.000045, lr : 0.007225\n",
      "Epoch: 8964/25000, Training Loss: 0.000052, lr : 0.007225\n",
      "Epoch: 8965/25000, Training Loss: 0.000044, lr : 0.007225\n",
      "Epoch: 8966/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 8967/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 8968/25000, Training Loss: 0.000044, lr : 0.007225\n",
      "Epoch: 8969/25000, Training Loss: 0.000045, lr : 0.007225\n",
      "Epoch: 8970/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 8971/25000, Training Loss: 0.000044, lr : 0.007225\n",
      "Epoch: 8972/25000, Training Loss: 0.000038, lr : 0.007225\n",
      "Epoch: 8973/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 8974/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 8975/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8976/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 8977/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8978/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 8979/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 8980/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 8981/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 8982/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8983/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8984/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8985/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 8986/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 8987/25000, Training Loss: 0.000046, lr : 0.007225\n",
      "Epoch: 8988/25000, Training Loss: 0.000072, lr : 0.007225\n",
      "Epoch: 8989/25000, Training Loss: 0.000095, lr : 0.007225\n",
      "Epoch: 8990/25000, Training Loss: 0.000072, lr : 0.007225\n",
      "Epoch: 8991/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 8992/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 8993/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8994/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 8995/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 8996/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 8997/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 8998/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 8999/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 9000/25000, Training Loss: 0.000040, lr : 0.007225\n",
      "Epoch: 9001/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 9002/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 9003/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 9004/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 9005/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 9006/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 9007/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 9008/25000, Training Loss: 0.000061, lr : 0.007225\n",
      "Epoch: 9009/25000, Training Loss: 0.000137, lr : 0.007225\n",
      "Epoch: 9010/25000, Training Loss: 0.000171, lr : 0.007225\n",
      "Epoch: 9011/25000, Training Loss: 0.000110, lr : 0.007225\n",
      "Epoch: 9012/25000, Training Loss: 0.000073, lr : 0.007225\n",
      "Epoch: 9013/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 9014/25000, Training Loss: 0.000080, lr : 0.007225\n",
      "Epoch: 9015/25000, Training Loss: 0.000087, lr : 0.007225\n",
      "Epoch: 9016/25000, Training Loss: 0.000059, lr : 0.007225\n",
      "Epoch: 9017/25000, Training Loss: 0.000110, lr : 0.007225\n",
      "Epoch: 9018/25000, Training Loss: 0.000242, lr : 0.007225\n",
      "Epoch: 9019/25000, Training Loss: 0.000232, lr : 0.007225\n",
      "Epoch: 9020/25000, Training Loss: 0.000097, lr : 0.007225\n",
      "Epoch: 9021/25000, Training Loss: 0.000112, lr : 0.007225\n",
      "Epoch: 9022/25000, Training Loss: 0.000133, lr : 0.007225\n",
      "Epoch: 9023/25000, Training Loss: 0.000070, lr : 0.007225\n",
      "Epoch: 9024/25000, Training Loss: 0.000068, lr : 0.007225\n",
      "Epoch: 9025/25000, Training Loss: 0.000070, lr : 0.007225\n",
      "Epoch: 9026/25000, Training Loss: 0.000054, lr : 0.007225\n",
      "Epoch: 9027/25000, Training Loss: 0.000068, lr : 0.007225\n",
      "Epoch: 9028/25000, Training Loss: 0.000056, lr : 0.007225\n",
      "Epoch: 9029/25000, Training Loss: 0.000085, lr : 0.007225\n",
      "Epoch: 9030/25000, Training Loss: 0.000123, lr : 0.007225\n",
      "Epoch: 9031/25000, Training Loss: 0.000108, lr : 0.007225\n",
      "Epoch: 9032/25000, Training Loss: 0.000086, lr : 0.007225\n",
      "Epoch: 9033/25000, Training Loss: 0.000086, lr : 0.007225\n",
      "Epoch: 9034/25000, Training Loss: 0.000077, lr : 0.007225\n",
      "Epoch: 9035/25000, Training Loss: 0.000060, lr : 0.007225\n",
      "Epoch: 9036/25000, Training Loss: 0.000076, lr : 0.007225\n",
      "Epoch: 9037/25000, Training Loss: 0.000060, lr : 0.007225\n",
      "Epoch: 9038/25000, Training Loss: 0.000051, lr : 0.007225\n",
      "Epoch: 9039/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 9040/25000, Training Loss: 0.000045, lr : 0.007225\n",
      "Epoch: 9041/25000, Training Loss: 0.000039, lr : 0.007225\n",
      "Epoch: 9042/25000, Training Loss: 0.000040, lr : 0.007225\n",
      "Epoch: 9043/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 9044/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 9045/25000, Training Loss: 0.000038, lr : 0.007225\n",
      "Epoch: 9046/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 9047/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 9048/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 9049/25000, Training Loss: 0.000072, lr : 0.007225\n",
      "Epoch: 9050/25000, Training Loss: 0.000056, lr : 0.007225\n",
      "Epoch: 9051/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 9052/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 9053/25000, Training Loss: 0.000037, lr : 0.007225\n",
      "Epoch: 9054/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 9055/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 9056/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 9057/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 9058/25000, Training Loss: 0.000029, lr : 0.007225\n",
      "Epoch: 9059/25000, Training Loss: 0.000026, lr : 0.007225\n",
      "Epoch: 9060/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 9061/25000, Training Loss: 0.000058, lr : 0.007225\n",
      "Epoch: 9062/25000, Training Loss: 0.000046, lr : 0.007225\n",
      "Epoch: 9063/25000, Training Loss: 0.000038, lr : 0.007225\n",
      "Epoch: 9064/25000, Training Loss: 0.000042, lr : 0.007225\n",
      "Epoch: 9065/25000, Training Loss: 0.000057, lr : 0.007225\n",
      "Epoch: 9066/25000, Training Loss: 0.000042, lr : 0.007225\n",
      "Epoch: 9067/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 9068/25000, Training Loss: 0.000045, lr : 0.007225\n",
      "Epoch: 9069/25000, Training Loss: 0.000053, lr : 0.007225\n",
      "Epoch: 9070/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 9071/25000, Training Loss: 0.000024, lr : 0.007225\n",
      "Epoch: 9072/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 9073/25000, Training Loss: 0.000027, lr : 0.007225\n",
      "Epoch: 9074/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 9075/25000, Training Loss: 0.000044, lr : 0.007225\n",
      "Epoch: 9076/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 9077/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 9078/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 9079/25000, Training Loss: 0.000021, lr : 0.007225\n",
      "Epoch: 9080/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 9081/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 9082/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 9083/25000, Training Loss: 0.000032, lr : 0.007225\n",
      "Epoch: 9084/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 9085/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 9086/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 9087/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 9088/25000, Training Loss: 0.000039, lr : 0.007225\n",
      "Epoch: 9089/25000, Training Loss: 0.000040, lr : 0.007225\n",
      "Epoch: 9090/25000, Training Loss: 0.000067, lr : 0.007225\n",
      "Epoch: 9091/25000, Training Loss: 0.000136, lr : 0.007225\n",
      "Epoch: 9092/25000, Training Loss: 0.000113, lr : 0.007225\n",
      "Epoch: 9093/25000, Training Loss: 0.000060, lr : 0.007225\n",
      "Epoch: 9094/25000, Training Loss: 0.000047, lr : 0.007225\n",
      "Epoch: 9095/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 9096/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 9097/25000, Training Loss: 0.000074, lr : 0.007225\n",
      "Epoch: 9098/25000, Training Loss: 0.000092, lr : 0.007225\n",
      "Epoch: 9099/25000, Training Loss: 0.000098, lr : 0.007225\n",
      "Epoch: 9100/25000, Training Loss: 0.000061, lr : 0.007225\n",
      "Epoch: 9101/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 9102/25000, Training Loss: 0.000047, lr : 0.007225\n",
      "Epoch: 9103/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 9104/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 9105/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 9106/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 9107/25000, Training Loss: 0.000047, lr : 0.007225\n",
      "Epoch: 9108/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 9109/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 9110/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 9111/25000, Training Loss: 0.000033, lr : 0.007225\n",
      "Epoch: 9112/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 9113/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 9114/25000, Training Loss: 0.000038, lr : 0.007225\n",
      "Epoch: 9115/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 9116/25000, Training Loss: 0.000020, lr : 0.007225\n",
      "Epoch: 9117/25000, Training Loss: 0.000028, lr : 0.007225\n",
      "Epoch: 9118/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 9119/25000, Training Loss: 0.000025, lr : 0.007225\n",
      "Epoch: 9120/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 9121/25000, Training Loss: 0.000078, lr : 0.007225\n",
      "Epoch: 9122/25000, Training Loss: 0.000092, lr : 0.007225\n",
      "Epoch: 9123/25000, Training Loss: 0.000046, lr : 0.007225\n",
      "Epoch: 9124/25000, Training Loss: 0.000064, lr : 0.007225\n",
      "Epoch: 9125/25000, Training Loss: 0.000127, lr : 0.007225\n",
      "Epoch: 9126/25000, Training Loss: 0.000099, lr : 0.007225\n",
      "Epoch: 9127/25000, Training Loss: 0.000063, lr : 0.007225\n",
      "Epoch: 9128/25000, Training Loss: 0.000052, lr : 0.007225\n",
      "Epoch: 9129/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 9130/25000, Training Loss: 0.000038, lr : 0.007225\n",
      "Epoch: 9131/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 9132/25000, Training Loss: 0.000037, lr : 0.007225\n",
      "Epoch: 9133/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 9134/25000, Training Loss: 0.000030, lr : 0.007225\n",
      "Epoch: 9135/25000, Training Loss: 0.000023, lr : 0.007225\n",
      "Epoch: 9136/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 9137/25000, Training Loss: 0.000035, lr : 0.007225\n",
      "Epoch: 9138/25000, Training Loss: 0.000053, lr : 0.007225\n",
      "Epoch: 9139/25000, Training Loss: 0.000077, lr : 0.007225\n",
      "Epoch: 9140/25000, Training Loss: 0.000077, lr : 0.007225\n",
      "Epoch: 9141/25000, Training Loss: 0.000049, lr : 0.007225\n",
      "Epoch: 9142/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 9143/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 9144/25000, Training Loss: 0.000041, lr : 0.007225\n",
      "Epoch: 9145/25000, Training Loss: 0.000031, lr : 0.007225\n",
      "Epoch: 9146/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 9147/25000, Training Loss: 0.000047, lr : 0.007225\n",
      "Epoch: 9148/25000, Training Loss: 0.000043, lr : 0.007225\n",
      "Epoch: 9149/25000, Training Loss: 0.000036, lr : 0.007225\n",
      "Epoch: 9150/25000, Training Loss: 0.000034, lr : 0.007225\n",
      "Epoch: 9151/25000, Training Loss: 0.000045, lr : 0.006141\n",
      "Epoch: 9152/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9153/25000, Training Loss: 0.000031, lr : 0.006141\n",
      "Epoch: 9154/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9155/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9156/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9157/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9158/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9159/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9160/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9161/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9162/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9163/25000, Training Loss: 0.000032, lr : 0.006141\n",
      "Epoch: 9164/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9165/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9166/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9167/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9168/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9169/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9170/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9171/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9172/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9173/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9174/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9175/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9176/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9177/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9178/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9179/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9180/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9181/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9182/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9183/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9184/25000, Training Loss: 0.000033, lr : 0.006141\n",
      "Epoch: 9185/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9186/25000, Training Loss: 0.000026, lr : 0.006141\n",
      "Epoch: 9187/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9188/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9189/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9190/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9191/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9192/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9193/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9194/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9195/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9196/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9197/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9198/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9199/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9200/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9201/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9202/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9203/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9204/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9205/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9206/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9207/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9208/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9209/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9210/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9211/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9212/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9213/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9214/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9215/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9216/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9217/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9218/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9219/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9220/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9221/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9222/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9223/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9224/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9225/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9226/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9227/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9228/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9229/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9230/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9231/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9232/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9233/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9234/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9235/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9236/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9237/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9238/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9239/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9240/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9241/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9242/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9243/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9244/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9245/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9246/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9247/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9248/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9249/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9250/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9251/25000, Training Loss: 0.000031, lr : 0.006141\n",
      "Epoch: 9252/25000, Training Loss: 0.000046, lr : 0.006141\n",
      "Epoch: 9253/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9254/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9255/25000, Training Loss: 0.000026, lr : 0.006141\n",
      "Epoch: 9256/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9257/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9258/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9259/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9260/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9261/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9262/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9263/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9264/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9265/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9266/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9267/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9268/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9269/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9270/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9271/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9272/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9273/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9274/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9275/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9276/25000, Training Loss: 0.000013, lr : 0.006141\n",
      "Epoch: 9277/25000, Training Loss: 0.000012, lr : 0.006141\n",
      "Epoch: 9278/25000, Training Loss: 0.000012, lr : 0.006141\n",
      "Epoch: 9279/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9280/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9281/25000, Training Loss: 0.000012, lr : 0.006141\n",
      "Epoch: 9282/25000, Training Loss: 0.000011, lr : 0.006141\n",
      "Epoch: 9283/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9284/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9285/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9286/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9287/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9288/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9289/25000, Training Loss: 0.000013, lr : 0.006141\n",
      "Epoch: 9290/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9291/25000, Training Loss: 0.000013, lr : 0.006141\n",
      "Epoch: 9292/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9293/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9294/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9295/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9296/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9297/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9298/25000, Training Loss: 0.000055, lr : 0.006141\n",
      "Epoch: 9299/25000, Training Loss: 0.000045, lr : 0.006141\n",
      "Epoch: 9300/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9301/25000, Training Loss: 0.000078, lr : 0.006141\n",
      "Epoch: 9302/25000, Training Loss: 0.000070, lr : 0.006141\n",
      "Epoch: 9303/25000, Training Loss: 0.000043, lr : 0.006141\n",
      "Epoch: 9304/25000, Training Loss: 0.000107, lr : 0.006141\n",
      "Epoch: 9305/25000, Training Loss: 0.000091, lr : 0.006141\n",
      "Epoch: 9306/25000, Training Loss: 0.000035, lr : 0.006141\n",
      "Epoch: 9307/25000, Training Loss: 0.000076, lr : 0.006141\n",
      "Epoch: 9308/25000, Training Loss: 0.000072, lr : 0.006141\n",
      "Epoch: 9309/25000, Training Loss: 0.000055, lr : 0.006141\n",
      "Epoch: 9310/25000, Training Loss: 0.000118, lr : 0.006141\n",
      "Epoch: 9311/25000, Training Loss: 0.000057, lr : 0.006141\n",
      "Epoch: 9312/25000, Training Loss: 0.000079, lr : 0.006141\n",
      "Epoch: 9313/25000, Training Loss: 0.000079, lr : 0.006141\n",
      "Epoch: 9314/25000, Training Loss: 0.000041, lr : 0.006141\n",
      "Epoch: 9315/25000, Training Loss: 0.000058, lr : 0.006141\n",
      "Epoch: 9316/25000, Training Loss: 0.000068, lr : 0.006141\n",
      "Epoch: 9317/25000, Training Loss: 0.000035, lr : 0.006141\n",
      "Epoch: 9318/25000, Training Loss: 0.000059, lr : 0.006141\n",
      "Epoch: 9319/25000, Training Loss: 0.000050, lr : 0.006141\n",
      "Epoch: 9320/25000, Training Loss: 0.000054, lr : 0.006141\n",
      "Epoch: 9321/25000, Training Loss: 0.000109, lr : 0.006141\n",
      "Epoch: 9322/25000, Training Loss: 0.000049, lr : 0.006141\n",
      "Epoch: 9323/25000, Training Loss: 0.000118, lr : 0.006141\n",
      "Epoch: 9324/25000, Training Loss: 0.000119, lr : 0.006141\n",
      "Epoch: 9325/25000, Training Loss: 0.000061, lr : 0.006141\n",
      "Epoch: 9326/25000, Training Loss: 0.000087, lr : 0.006141\n",
      "Epoch: 9327/25000, Training Loss: 0.000055, lr : 0.006141\n",
      "Epoch: 9328/25000, Training Loss: 0.000099, lr : 0.006141\n",
      "Epoch: 9329/25000, Training Loss: 0.000055, lr : 0.006141\n",
      "Epoch: 9330/25000, Training Loss: 0.000086, lr : 0.006141\n",
      "Epoch: 9331/25000, Training Loss: 0.000077, lr : 0.006141\n",
      "Epoch: 9332/25000, Training Loss: 0.000048, lr : 0.006141\n",
      "Epoch: 9333/25000, Training Loss: 0.000080, lr : 0.006141\n",
      "Epoch: 9334/25000, Training Loss: 0.000059, lr : 0.006141\n",
      "Epoch: 9335/25000, Training Loss: 0.000057, lr : 0.006141\n",
      "Epoch: 9336/25000, Training Loss: 0.000055, lr : 0.006141\n",
      "Epoch: 9337/25000, Training Loss: 0.000066, lr : 0.006141\n",
      "Epoch: 9338/25000, Training Loss: 0.000060, lr : 0.006141\n",
      "Epoch: 9339/25000, Training Loss: 0.000070, lr : 0.006141\n",
      "Epoch: 9340/25000, Training Loss: 0.000053, lr : 0.006141\n",
      "Epoch: 9341/25000, Training Loss: 0.000050, lr : 0.006141\n",
      "Epoch: 9342/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9343/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9344/25000, Training Loss: 0.000042, lr : 0.006141\n",
      "Epoch: 9345/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9346/25000, Training Loss: 0.000038, lr : 0.006141\n",
      "Epoch: 9347/25000, Training Loss: 0.000047, lr : 0.006141\n",
      "Epoch: 9348/25000, Training Loss: 0.000038, lr : 0.006141\n",
      "Epoch: 9349/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9350/25000, Training Loss: 0.000034, lr : 0.006141\n",
      "Epoch: 9351/25000, Training Loss: 0.000035, lr : 0.006141\n",
      "Epoch: 9352/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9353/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9354/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9355/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9356/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9357/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9358/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9359/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9360/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9361/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9362/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9363/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9364/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9365/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9366/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9367/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9368/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9369/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9370/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9371/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9372/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9373/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9374/25000, Training Loss: 0.000012, lr : 0.006141\n",
      "Epoch: 9375/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9376/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9377/25000, Training Loss: 0.000011, lr : 0.006141\n",
      "Epoch: 9378/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9379/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9380/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9381/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9382/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9383/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9384/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9385/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9386/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9387/25000, Training Loss: 0.000035, lr : 0.006141\n",
      "Epoch: 9388/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9389/25000, Training Loss: 0.000026, lr : 0.006141\n",
      "Epoch: 9390/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9391/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9392/25000, Training Loss: 0.000032, lr : 0.006141\n",
      "Epoch: 9393/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9394/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9395/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9396/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9397/25000, Training Loss: 0.000031, lr : 0.006141\n",
      "Epoch: 9398/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9399/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9400/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9401/25000, Training Loss: 0.000056, lr : 0.006141\n",
      "Epoch: 9402/25000, Training Loss: 0.000043, lr : 0.006141\n",
      "Epoch: 9403/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9404/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9405/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9406/25000, Training Loss: 0.000026, lr : 0.006141\n",
      "Epoch: 9407/25000, Training Loss: 0.000041, lr : 0.006141\n",
      "Epoch: 9408/25000, Training Loss: 0.000031, lr : 0.006141\n",
      "Epoch: 9409/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9410/25000, Training Loss: 0.000041, lr : 0.006141\n",
      "Epoch: 9411/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9412/25000, Training Loss: 0.000026, lr : 0.006141\n",
      "Epoch: 9413/25000, Training Loss: 0.000071, lr : 0.006141\n",
      "Epoch: 9414/25000, Training Loss: 0.000067, lr : 0.006141\n",
      "Epoch: 9415/25000, Training Loss: 0.000051, lr : 0.006141\n",
      "Epoch: 9416/25000, Training Loss: 0.000049, lr : 0.006141\n",
      "Epoch: 9417/25000, Training Loss: 0.000097, lr : 0.006141\n",
      "Epoch: 9418/25000, Training Loss: 0.000068, lr : 0.006141\n",
      "Epoch: 9419/25000, Training Loss: 0.000043, lr : 0.006141\n",
      "Epoch: 9420/25000, Training Loss: 0.000049, lr : 0.006141\n",
      "Epoch: 9421/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9422/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9423/25000, Training Loss: 0.000034, lr : 0.006141\n",
      "Epoch: 9424/25000, Training Loss: 0.000044, lr : 0.006141\n",
      "Epoch: 9425/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9426/25000, Training Loss: 0.000038, lr : 0.006141\n",
      "Epoch: 9427/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9428/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9429/25000, Training Loss: 0.000047, lr : 0.006141\n",
      "Epoch: 9430/25000, Training Loss: 0.000069, lr : 0.006141\n",
      "Epoch: 9431/25000, Training Loss: 0.000084, lr : 0.006141\n",
      "Epoch: 9432/25000, Training Loss: 0.000046, lr : 0.006141\n",
      "Epoch: 9433/25000, Training Loss: 0.000057, lr : 0.006141\n",
      "Epoch: 9434/25000, Training Loss: 0.000072, lr : 0.006141\n",
      "Epoch: 9435/25000, Training Loss: 0.000034, lr : 0.006141\n",
      "Epoch: 9436/25000, Training Loss: 0.000050, lr : 0.006141\n",
      "Epoch: 9437/25000, Training Loss: 0.000062, lr : 0.006141\n",
      "Epoch: 9438/25000, Training Loss: 0.000047, lr : 0.006141\n",
      "Epoch: 9439/25000, Training Loss: 0.000043, lr : 0.006141\n",
      "Epoch: 9440/25000, Training Loss: 0.000078, lr : 0.006141\n",
      "Epoch: 9441/25000, Training Loss: 0.000087, lr : 0.006141\n",
      "Epoch: 9442/25000, Training Loss: 0.000117, lr : 0.006141\n",
      "Epoch: 9443/25000, Training Loss: 0.000101, lr : 0.006141\n",
      "Epoch: 9444/25000, Training Loss: 0.000143, lr : 0.006141\n",
      "Epoch: 9445/25000, Training Loss: 0.000179, lr : 0.006141\n",
      "Epoch: 9446/25000, Training Loss: 0.000068, lr : 0.006141\n",
      "Epoch: 9447/25000, Training Loss: 0.000091, lr : 0.006141\n",
      "Epoch: 9448/25000, Training Loss: 0.000119, lr : 0.006141\n",
      "Epoch: 9449/25000, Training Loss: 0.000067, lr : 0.006141\n",
      "Epoch: 9450/25000, Training Loss: 0.000083, lr : 0.006141\n",
      "Epoch: 9451/25000, Training Loss: 0.000099, lr : 0.006141\n",
      "Epoch: 9452/25000, Training Loss: 0.000078, lr : 0.006141\n",
      "Epoch: 9453/25000, Training Loss: 0.000060, lr : 0.006141\n",
      "Epoch: 9454/25000, Training Loss: 0.000097, lr : 0.006141\n",
      "Epoch: 9455/25000, Training Loss: 0.000057, lr : 0.006141\n",
      "Epoch: 9456/25000, Training Loss: 0.000075, lr : 0.006141\n",
      "Epoch: 9457/25000, Training Loss: 0.000059, lr : 0.006141\n",
      "Epoch: 9458/25000, Training Loss: 0.000061, lr : 0.006141\n",
      "Epoch: 9459/25000, Training Loss: 0.000119, lr : 0.006141\n",
      "Epoch: 9460/25000, Training Loss: 0.000068, lr : 0.006141\n",
      "Epoch: 9461/25000, Training Loss: 0.000070, lr : 0.006141\n",
      "Epoch: 9462/25000, Training Loss: 0.000053, lr : 0.006141\n",
      "Epoch: 9463/25000, Training Loss: 0.000051, lr : 0.006141\n",
      "Epoch: 9464/25000, Training Loss: 0.000055, lr : 0.006141\n",
      "Epoch: 9465/25000, Training Loss: 0.000049, lr : 0.006141\n",
      "Epoch: 9466/25000, Training Loss: 0.000040, lr : 0.006141\n",
      "Epoch: 9467/25000, Training Loss: 0.000068, lr : 0.006141\n",
      "Epoch: 9468/25000, Training Loss: 0.000059, lr : 0.006141\n",
      "Epoch: 9469/25000, Training Loss: 0.000057, lr : 0.006141\n",
      "Epoch: 9470/25000, Training Loss: 0.000115, lr : 0.006141\n",
      "Epoch: 9471/25000, Training Loss: 0.000050, lr : 0.006141\n",
      "Epoch: 9472/25000, Training Loss: 0.000075, lr : 0.006141\n",
      "Epoch: 9473/25000, Training Loss: 0.000099, lr : 0.006141\n",
      "Epoch: 9474/25000, Training Loss: 0.000066, lr : 0.006141\n",
      "Epoch: 9475/25000, Training Loss: 0.000105, lr : 0.006141\n",
      "Epoch: 9476/25000, Training Loss: 0.000128, lr : 0.006141\n",
      "Epoch: 9477/25000, Training Loss: 0.000042, lr : 0.006141\n",
      "Epoch: 9478/25000, Training Loss: 0.000086, lr : 0.006141\n",
      "Epoch: 9479/25000, Training Loss: 0.000055, lr : 0.006141\n",
      "Epoch: 9480/25000, Training Loss: 0.000047, lr : 0.006141\n",
      "Epoch: 9481/25000, Training Loss: 0.000048, lr : 0.006141\n",
      "Epoch: 9482/25000, Training Loss: 0.000033, lr : 0.006141\n",
      "Epoch: 9483/25000, Training Loss: 0.000037, lr : 0.006141\n",
      "Epoch: 9484/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9485/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9486/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9487/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9488/25000, Training Loss: 0.000026, lr : 0.006141\n",
      "Epoch: 9489/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9490/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9491/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9492/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9493/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9494/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9495/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9496/25000, Training Loss: 0.000035, lr : 0.006141\n",
      "Epoch: 9497/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9498/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9499/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9500/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9501/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9502/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9503/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9504/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9505/25000, Training Loss: 0.000012, lr : 0.006141\n",
      "Epoch: 9506/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9507/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9508/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9509/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9510/25000, Training Loss: 0.000013, lr : 0.006141\n",
      "Epoch: 9511/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9512/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9513/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9514/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9515/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9516/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9517/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9518/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9519/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9520/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9521/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9522/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9523/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9524/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9525/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9526/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9527/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9528/25000, Training Loss: 0.000041, lr : 0.006141\n",
      "Epoch: 9529/25000, Training Loss: 0.000045, lr : 0.006141\n",
      "Epoch: 9530/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9531/25000, Training Loss: 0.000047, lr : 0.006141\n",
      "Epoch: 9532/25000, Training Loss: 0.000047, lr : 0.006141\n",
      "Epoch: 9533/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9534/25000, Training Loss: 0.000032, lr : 0.006141\n",
      "Epoch: 9535/25000, Training Loss: 0.000055, lr : 0.006141\n",
      "Epoch: 9536/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9537/25000, Training Loss: 0.000042, lr : 0.006141\n",
      "Epoch: 9538/25000, Training Loss: 0.000052, lr : 0.006141\n",
      "Epoch: 9539/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9540/25000, Training Loss: 0.000049, lr : 0.006141\n",
      "Epoch: 9541/25000, Training Loss: 0.000057, lr : 0.006141\n",
      "Epoch: 9542/25000, Training Loss: 0.000035, lr : 0.006141\n",
      "Epoch: 9543/25000, Training Loss: 0.000045, lr : 0.006141\n",
      "Epoch: 9544/25000, Training Loss: 0.000044, lr : 0.006141\n",
      "Epoch: 9545/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9546/25000, Training Loss: 0.000033, lr : 0.006141\n",
      "Epoch: 9547/25000, Training Loss: 0.000038, lr : 0.006141\n",
      "Epoch: 9548/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9549/25000, Training Loss: 0.000038, lr : 0.006141\n",
      "Epoch: 9550/25000, Training Loss: 0.000041, lr : 0.006141\n",
      "Epoch: 9551/25000, Training Loss: 0.000035, lr : 0.006141\n",
      "Epoch: 9552/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9553/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9554/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9555/25000, Training Loss: 0.000034, lr : 0.006141\n",
      "Epoch: 9556/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9557/25000, Training Loss: 0.000037, lr : 0.006141\n",
      "Epoch: 9558/25000, Training Loss: 0.000059, lr : 0.006141\n",
      "Epoch: 9559/25000, Training Loss: 0.000058, lr : 0.006141\n",
      "Epoch: 9560/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9561/25000, Training Loss: 0.000064, lr : 0.006141\n",
      "Epoch: 9562/25000, Training Loss: 0.000073, lr : 0.006141\n",
      "Epoch: 9563/25000, Training Loss: 0.000048, lr : 0.006141\n",
      "Epoch: 9564/25000, Training Loss: 0.000033, lr : 0.006141\n",
      "Epoch: 9565/25000, Training Loss: 0.000031, lr : 0.006141\n",
      "Epoch: 9566/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9567/25000, Training Loss: 0.000031, lr : 0.006141\n",
      "Epoch: 9568/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9569/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9570/25000, Training Loss: 0.000061, lr : 0.006141\n",
      "Epoch: 9571/25000, Training Loss: 0.000052, lr : 0.006141\n",
      "Epoch: 9572/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9573/25000, Training Loss: 0.000046, lr : 0.006141\n",
      "Epoch: 9574/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9575/25000, Training Loss: 0.000026, lr : 0.006141\n",
      "Epoch: 9576/25000, Training Loss: 0.000062, lr : 0.006141\n",
      "Epoch: 9577/25000, Training Loss: 0.000071, lr : 0.006141\n",
      "Epoch: 9578/25000, Training Loss: 0.000059, lr : 0.006141\n",
      "Epoch: 9579/25000, Training Loss: 0.000032, lr : 0.006141\n",
      "Epoch: 9580/25000, Training Loss: 0.000037, lr : 0.006141\n",
      "Epoch: 9581/25000, Training Loss: 0.000074, lr : 0.006141\n",
      "Epoch: 9582/25000, Training Loss: 0.000045, lr : 0.006141\n",
      "Epoch: 9583/25000, Training Loss: 0.000055, lr : 0.006141\n",
      "Epoch: 9584/25000, Training Loss: 0.000089, lr : 0.006141\n",
      "Epoch: 9585/25000, Training Loss: 0.000040, lr : 0.006141\n",
      "Epoch: 9586/25000, Training Loss: 0.000035, lr : 0.006141\n",
      "Epoch: 9587/25000, Training Loss: 0.000035, lr : 0.006141\n",
      "Epoch: 9588/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9589/25000, Training Loss: 0.000040, lr : 0.006141\n",
      "Epoch: 9590/25000, Training Loss: 0.000034, lr : 0.006141\n",
      "Epoch: 9591/25000, Training Loss: 0.000050, lr : 0.006141\n",
      "Epoch: 9592/25000, Training Loss: 0.000063, lr : 0.006141\n",
      "Epoch: 9593/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9594/25000, Training Loss: 0.000065, lr : 0.006141\n",
      "Epoch: 9595/25000, Training Loss: 0.000069, lr : 0.006141\n",
      "Epoch: 9596/25000, Training Loss: 0.000040, lr : 0.006141\n",
      "Epoch: 9597/25000, Training Loss: 0.000099, lr : 0.006141\n",
      "Epoch: 9598/25000, Training Loss: 0.000170, lr : 0.006141\n",
      "Epoch: 9599/25000, Training Loss: 0.000113, lr : 0.006141\n",
      "Epoch: 9600/25000, Training Loss: 0.000062, lr : 0.006141\n",
      "Epoch: 9601/25000, Training Loss: 0.000078, lr : 0.006141\n",
      "Epoch: 9602/25000, Training Loss: 0.000075, lr : 0.006141\n",
      "Epoch: 9603/25000, Training Loss: 0.000109, lr : 0.006141\n",
      "Epoch: 9604/25000, Training Loss: 0.000082, lr : 0.006141\n",
      "Epoch: 9605/25000, Training Loss: 0.000087, lr : 0.006141\n",
      "Epoch: 9606/25000, Training Loss: 0.000065, lr : 0.006141\n",
      "Epoch: 9607/25000, Training Loss: 0.000086, lr : 0.006141\n",
      "Epoch: 9608/25000, Training Loss: 0.000066, lr : 0.006141\n",
      "Epoch: 9609/25000, Training Loss: 0.000069, lr : 0.006141\n",
      "Epoch: 9610/25000, Training Loss: 0.000094, lr : 0.006141\n",
      "Epoch: 9611/25000, Training Loss: 0.000071, lr : 0.006141\n",
      "Epoch: 9612/25000, Training Loss: 0.000048, lr : 0.006141\n",
      "Epoch: 9613/25000, Training Loss: 0.000052, lr : 0.006141\n",
      "Epoch: 9614/25000, Training Loss: 0.000040, lr : 0.006141\n",
      "Epoch: 9615/25000, Training Loss: 0.000051, lr : 0.006141\n",
      "Epoch: 9616/25000, Training Loss: 0.000076, lr : 0.006141\n",
      "Epoch: 9617/25000, Training Loss: 0.000047, lr : 0.006141\n",
      "Epoch: 9618/25000, Training Loss: 0.000038, lr : 0.006141\n",
      "Epoch: 9619/25000, Training Loss: 0.000034, lr : 0.006141\n",
      "Epoch: 9620/25000, Training Loss: 0.000038, lr : 0.006141\n",
      "Epoch: 9621/25000, Training Loss: 0.000044, lr : 0.006141\n",
      "Epoch: 9622/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9623/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9624/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9625/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9626/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9627/25000, Training Loss: 0.000035, lr : 0.006141\n",
      "Epoch: 9628/25000, Training Loss: 0.000031, lr : 0.006141\n",
      "Epoch: 9629/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9630/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9631/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9632/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9633/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9634/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9635/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9636/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9637/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9638/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9639/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9640/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9641/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9642/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9643/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9644/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9645/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9646/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9647/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9648/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9649/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9650/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9651/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9652/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9653/25000, Training Loss: 0.000012, lr : 0.006141\n",
      "Epoch: 9654/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9655/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9656/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9657/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9658/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9659/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9660/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9661/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9662/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9663/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9664/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9665/25000, Training Loss: 0.000051, lr : 0.006141\n",
      "Epoch: 9666/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9667/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9668/25000, Training Loss: 0.000044, lr : 0.006141\n",
      "Epoch: 9669/25000, Training Loss: 0.000041, lr : 0.006141\n",
      "Epoch: 9670/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9671/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9672/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9673/25000, Training Loss: 0.000037, lr : 0.006141\n",
      "Epoch: 9674/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9675/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9676/25000, Training Loss: 0.000032, lr : 0.006141\n",
      "Epoch: 9677/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9678/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9679/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9680/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9681/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9682/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9683/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9684/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9685/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9686/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9687/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9688/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9689/25000, Training Loss: 0.000035, lr : 0.006141\n",
      "Epoch: 9690/25000, Training Loss: 0.000060, lr : 0.006141\n",
      "Epoch: 9691/25000, Training Loss: 0.000042, lr : 0.006141\n",
      "Epoch: 9692/25000, Training Loss: 0.000047, lr : 0.006141\n",
      "Epoch: 9693/25000, Training Loss: 0.000041, lr : 0.006141\n",
      "Epoch: 9694/25000, Training Loss: 0.000042, lr : 0.006141\n",
      "Epoch: 9695/25000, Training Loss: 0.000040, lr : 0.006141\n",
      "Epoch: 9696/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9697/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9698/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9699/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9700/25000, Training Loss: 0.000043, lr : 0.006141\n",
      "Epoch: 9701/25000, Training Loss: 0.000033, lr : 0.006141\n",
      "Epoch: 9702/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9703/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9704/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9705/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9706/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9707/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9708/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9709/25000, Training Loss: 0.000031, lr : 0.006141\n",
      "Epoch: 9710/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9711/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9712/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9713/25000, Training Loss: 0.000052, lr : 0.006141\n",
      "Epoch: 9714/25000, Training Loss: 0.000124, lr : 0.006141\n",
      "Epoch: 9715/25000, Training Loss: 0.000196, lr : 0.006141\n",
      "Epoch: 9716/25000, Training Loss: 0.000203, lr : 0.006141\n",
      "Epoch: 9717/25000, Training Loss: 0.000111, lr : 0.006141\n",
      "Epoch: 9718/25000, Training Loss: 0.000147, lr : 0.006141\n",
      "Epoch: 9719/25000, Training Loss: 0.000146, lr : 0.006141\n",
      "Epoch: 9720/25000, Training Loss: 0.000246, lr : 0.006141\n",
      "Epoch: 9721/25000, Training Loss: 0.000141, lr : 0.006141\n",
      "Epoch: 9722/25000, Training Loss: 0.000219, lr : 0.006141\n",
      "Epoch: 9723/25000, Training Loss: 0.000148, lr : 0.006141\n",
      "Epoch: 9724/25000, Training Loss: 0.000201, lr : 0.006141\n",
      "Epoch: 9725/25000, Training Loss: 0.000207, lr : 0.006141\n",
      "Epoch: 9726/25000, Training Loss: 0.000212, lr : 0.006141\n",
      "Epoch: 9727/25000, Training Loss: 0.000226, lr : 0.006141\n",
      "Epoch: 9728/25000, Training Loss: 0.000238, lr : 0.006141\n",
      "Epoch: 9729/25000, Training Loss: 0.000184, lr : 0.006141\n",
      "Epoch: 9730/25000, Training Loss: 0.000200, lr : 0.006141\n",
      "Epoch: 9731/25000, Training Loss: 0.000214, lr : 0.006141\n",
      "Epoch: 9732/25000, Training Loss: 0.000152, lr : 0.006141\n",
      "Epoch: 9733/25000, Training Loss: 0.000219, lr : 0.006141\n",
      "Epoch: 9734/25000, Training Loss: 0.000221, lr : 0.006141\n",
      "Epoch: 9735/25000, Training Loss: 0.000136, lr : 0.006141\n",
      "Epoch: 9736/25000, Training Loss: 0.000162, lr : 0.006141\n",
      "Epoch: 9737/25000, Training Loss: 0.000172, lr : 0.006141\n",
      "Epoch: 9738/25000, Training Loss: 0.000126, lr : 0.006141\n",
      "Epoch: 9739/25000, Training Loss: 0.000188, lr : 0.006141\n",
      "Epoch: 9740/25000, Training Loss: 0.000105, lr : 0.006141\n",
      "Epoch: 9741/25000, Training Loss: 0.000113, lr : 0.006141\n",
      "Epoch: 9742/25000, Training Loss: 0.000137, lr : 0.006141\n",
      "Epoch: 9743/25000, Training Loss: 0.000085, lr : 0.006141\n",
      "Epoch: 9744/25000, Training Loss: 0.000181, lr : 0.006141\n",
      "Epoch: 9745/25000, Training Loss: 0.000124, lr : 0.006141\n",
      "Epoch: 9746/25000, Training Loss: 0.000081, lr : 0.006141\n",
      "Epoch: 9747/25000, Training Loss: 0.000135, lr : 0.006141\n",
      "Epoch: 9748/25000, Training Loss: 0.000070, lr : 0.006141\n",
      "Epoch: 9749/25000, Training Loss: 0.000102, lr : 0.006141\n",
      "Epoch: 9750/25000, Training Loss: 0.000074, lr : 0.006141\n",
      "Epoch: 9751/25000, Training Loss: 0.000079, lr : 0.006141\n",
      "Epoch: 9752/25000, Training Loss: 0.000053, lr : 0.006141\n",
      "Epoch: 9753/25000, Training Loss: 0.000060, lr : 0.006141\n",
      "Epoch: 9754/25000, Training Loss: 0.000053, lr : 0.006141\n",
      "Epoch: 9755/25000, Training Loss: 0.000061, lr : 0.006141\n",
      "Epoch: 9756/25000, Training Loss: 0.000058, lr : 0.006141\n",
      "Epoch: 9757/25000, Training Loss: 0.000112, lr : 0.006141\n",
      "Epoch: 9758/25000, Training Loss: 0.000123, lr : 0.006141\n",
      "Epoch: 9759/25000, Training Loss: 0.000081, lr : 0.006141\n",
      "Epoch: 9760/25000, Training Loss: 0.000117, lr : 0.006141\n",
      "Epoch: 9761/25000, Training Loss: 0.000077, lr : 0.006141\n",
      "Epoch: 9762/25000, Training Loss: 0.000115, lr : 0.006141\n",
      "Epoch: 9763/25000, Training Loss: 0.000067, lr : 0.006141\n",
      "Epoch: 9764/25000, Training Loss: 0.000082, lr : 0.006141\n",
      "Epoch: 9765/25000, Training Loss: 0.000079, lr : 0.006141\n",
      "Epoch: 9766/25000, Training Loss: 0.000053, lr : 0.006141\n",
      "Epoch: 9767/25000, Training Loss: 0.000056, lr : 0.006141\n",
      "Epoch: 9768/25000, Training Loss: 0.000047, lr : 0.006141\n",
      "Epoch: 9769/25000, Training Loss: 0.000061, lr : 0.006141\n",
      "Epoch: 9770/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9771/25000, Training Loss: 0.000053, lr : 0.006141\n",
      "Epoch: 9772/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9773/25000, Training Loss: 0.000038, lr : 0.006141\n",
      "Epoch: 9774/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9775/25000, Training Loss: 0.000027, lr : 0.006141\n",
      "Epoch: 9776/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9777/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9778/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9779/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9780/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9781/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9782/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9783/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9784/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9785/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9786/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9787/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9788/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9789/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9790/25000, Training Loss: 0.000013, lr : 0.006141\n",
      "Epoch: 9791/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9792/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9793/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9794/25000, Training Loss: 0.000013, lr : 0.006141\n",
      "Epoch: 9795/25000, Training Loss: 0.000010, lr : 0.006141\n",
      "Epoch: 9796/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9797/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9798/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9799/25000, Training Loss: 0.000013, lr : 0.006141\n",
      "Epoch: 9800/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9801/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9802/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9803/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9804/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9805/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9806/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9807/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9808/25000, Training Loss: 0.000040, lr : 0.006141\n",
      "Epoch: 9809/25000, Training Loss: 0.000040, lr : 0.006141\n",
      "Epoch: 9810/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9811/25000, Training Loss: 0.000043, lr : 0.006141\n",
      "Epoch: 9812/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9813/25000, Training Loss: 0.000043, lr : 0.006141\n",
      "Epoch: 9814/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9815/25000, Training Loss: 0.000031, lr : 0.006141\n",
      "Epoch: 9816/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9817/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9818/25000, Training Loss: 0.000026, lr : 0.006141\n",
      "Epoch: 9819/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9820/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9821/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9822/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9823/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9824/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9825/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9826/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9827/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9828/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9829/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9830/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9831/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9832/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9833/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9834/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9835/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9836/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9837/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9838/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9839/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9840/25000, Training Loss: 0.000026, lr : 0.006141\n",
      "Epoch: 9841/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9842/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9843/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9844/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9845/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9846/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9847/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9848/25000, Training Loss: 0.000026, lr : 0.006141\n",
      "Epoch: 9849/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9850/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9851/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9852/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9853/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9854/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9855/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9856/25000, Training Loss: 0.000012, lr : 0.006141\n",
      "Epoch: 9857/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9858/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9859/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9860/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9861/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9862/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9863/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9864/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9865/25000, Training Loss: 0.000011, lr : 0.006141\n",
      "Epoch: 9866/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9867/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9868/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9869/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9870/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9871/25000, Training Loss: 0.000012, lr : 0.006141\n",
      "Epoch: 9872/25000, Training Loss: 0.000011, lr : 0.006141\n",
      "Epoch: 9873/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9874/25000, Training Loss: 0.000020, lr : 0.006141\n",
      "Epoch: 9875/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9876/25000, Training Loss: 0.000013, lr : 0.006141\n",
      "Epoch: 9877/25000, Training Loss: 0.000013, lr : 0.006141\n",
      "Epoch: 9878/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9879/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9880/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9881/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9882/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9883/25000, Training Loss: 0.000053, lr : 0.006141\n",
      "Epoch: 9884/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9885/25000, Training Loss: 0.000055, lr : 0.006141\n",
      "Epoch: 9886/25000, Training Loss: 0.000120, lr : 0.006141\n",
      "Epoch: 9887/25000, Training Loss: 0.000108, lr : 0.006141\n",
      "Epoch: 9888/25000, Training Loss: 0.000069, lr : 0.006141\n",
      "Epoch: 9889/25000, Training Loss: 0.000138, lr : 0.006141\n",
      "Epoch: 9890/25000, Training Loss: 0.000135, lr : 0.006141\n",
      "Epoch: 9891/25000, Training Loss: 0.000094, lr : 0.006141\n",
      "Epoch: 9892/25000, Training Loss: 0.000094, lr : 0.006141\n",
      "Epoch: 9893/25000, Training Loss: 0.000079, lr : 0.006141\n",
      "Epoch: 9894/25000, Training Loss: 0.000074, lr : 0.006141\n",
      "Epoch: 9895/25000, Training Loss: 0.000089, lr : 0.006141\n",
      "Epoch: 9896/25000, Training Loss: 0.000098, lr : 0.006141\n",
      "Epoch: 9897/25000, Training Loss: 0.000078, lr : 0.006141\n",
      "Epoch: 9898/25000, Training Loss: 0.000052, lr : 0.006141\n",
      "Epoch: 9899/25000, Training Loss: 0.000061, lr : 0.006141\n",
      "Epoch: 9900/25000, Training Loss: 0.000052, lr : 0.006141\n",
      "Epoch: 9901/25000, Training Loss: 0.000034, lr : 0.006141\n",
      "Epoch: 9902/25000, Training Loss: 0.000057, lr : 0.006141\n",
      "Epoch: 9903/25000, Training Loss: 0.000054, lr : 0.006141\n",
      "Epoch: 9904/25000, Training Loss: 0.000045, lr : 0.006141\n",
      "Epoch: 9905/25000, Training Loss: 0.000038, lr : 0.006141\n",
      "Epoch: 9906/25000, Training Loss: 0.000046, lr : 0.006141\n",
      "Epoch: 9907/25000, Training Loss: 0.000033, lr : 0.006141\n",
      "Epoch: 9908/25000, Training Loss: 0.000033, lr : 0.006141\n",
      "Epoch: 9909/25000, Training Loss: 0.000033, lr : 0.006141\n",
      "Epoch: 9910/25000, Training Loss: 0.000034, lr : 0.006141\n",
      "Epoch: 9911/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9912/25000, Training Loss: 0.000034, lr : 0.006141\n",
      "Epoch: 9913/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9914/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9915/25000, Training Loss: 0.000024, lr : 0.006141\n",
      "Epoch: 9916/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9917/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9918/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9919/25000, Training Loss: 0.000015, lr : 0.006141\n",
      "Epoch: 9920/25000, Training Loss: 0.000014, lr : 0.006141\n",
      "Epoch: 9921/25000, Training Loss: 0.000017, lr : 0.006141\n",
      "Epoch: 9922/25000, Training Loss: 0.000016, lr : 0.006141\n",
      "Epoch: 9923/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9924/25000, Training Loss: 0.000018, lr : 0.006141\n",
      "Epoch: 9925/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9926/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9927/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9928/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9929/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9930/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9931/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9932/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9933/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9934/25000, Training Loss: 0.000021, lr : 0.006141\n",
      "Epoch: 9935/25000, Training Loss: 0.000022, lr : 0.006141\n",
      "Epoch: 9936/25000, Training Loss: 0.000039, lr : 0.006141\n",
      "Epoch: 9937/25000, Training Loss: 0.000053, lr : 0.006141\n",
      "Epoch: 9938/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9939/25000, Training Loss: 0.000055, lr : 0.006141\n",
      "Epoch: 9940/25000, Training Loss: 0.000083, lr : 0.006141\n",
      "Epoch: 9941/25000, Training Loss: 0.000044, lr : 0.006141\n",
      "Epoch: 9942/25000, Training Loss: 0.000046, lr : 0.006141\n",
      "Epoch: 9943/25000, Training Loss: 0.000079, lr : 0.006141\n",
      "Epoch: 9944/25000, Training Loss: 0.000099, lr : 0.006141\n",
      "Epoch: 9945/25000, Training Loss: 0.000057, lr : 0.006141\n",
      "Epoch: 9946/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9947/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9948/25000, Training Loss: 0.000036, lr : 0.006141\n",
      "Epoch: 9949/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9950/25000, Training Loss: 0.000056, lr : 0.006141\n",
      "Epoch: 9951/25000, Training Loss: 0.000077, lr : 0.006141\n",
      "Epoch: 9952/25000, Training Loss: 0.000038, lr : 0.006141\n",
      "Epoch: 9953/25000, Training Loss: 0.000029, lr : 0.006141\n",
      "Epoch: 9954/25000, Training Loss: 0.000040, lr : 0.006141\n",
      "Epoch: 9955/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9956/25000, Training Loss: 0.000042, lr : 0.006141\n",
      "Epoch: 9957/25000, Training Loss: 0.000044, lr : 0.006141\n",
      "Epoch: 9958/25000, Training Loss: 0.000026, lr : 0.006141\n",
      "Epoch: 9959/25000, Training Loss: 0.000028, lr : 0.006141\n",
      "Epoch: 9960/25000, Training Loss: 0.000047, lr : 0.006141\n",
      "Epoch: 9961/25000, Training Loss: 0.000038, lr : 0.006141\n",
      "Epoch: 9962/25000, Training Loss: 0.000023, lr : 0.006141\n",
      "Epoch: 9963/25000, Training Loss: 0.000025, lr : 0.006141\n",
      "Epoch: 9964/25000, Training Loss: 0.000033, lr : 0.006141\n",
      "Epoch: 9965/25000, Training Loss: 0.000048, lr : 0.006141\n",
      "Epoch: 9966/25000, Training Loss: 0.000056, lr : 0.006141\n",
      "Epoch: 9967/25000, Training Loss: 0.000034, lr : 0.006141\n",
      "Epoch: 9968/25000, Training Loss: 0.000034, lr : 0.006141\n",
      "Epoch: 9969/25000, Training Loss: 0.000047, lr : 0.006141\n",
      "Epoch: 9970/25000, Training Loss: 0.000030, lr : 0.006141\n",
      "Epoch: 9971/25000, Training Loss: 0.000019, lr : 0.006141\n",
      "Epoch: 9972/25000, Training Loss: 0.000035, lr : 0.006141\n",
      "Epoch: 9973/25000, Training Loss: 0.000027, lr : 0.005220\n",
      "Epoch: 9974/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 9975/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 9976/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 9977/25000, Training Loss: 0.000022, lr : 0.005220\n",
      "Epoch: 9978/25000, Training Loss: 0.000026, lr : 0.005220\n",
      "Epoch: 9979/25000, Training Loss: 0.000020, lr : 0.005220\n",
      "Epoch: 9980/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 9981/25000, Training Loss: 0.000020, lr : 0.005220\n",
      "Epoch: 9982/25000, Training Loss: 0.000023, lr : 0.005220\n",
      "Epoch: 9983/25000, Training Loss: 0.000024, lr : 0.005220\n",
      "Epoch: 9984/25000, Training Loss: 0.000017, lr : 0.005220\n",
      "Epoch: 9985/25000, Training Loss: 0.000020, lr : 0.005220\n",
      "Epoch: 9986/25000, Training Loss: 0.000020, lr : 0.005220\n",
      "Epoch: 9987/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 9988/25000, Training Loss: 0.000018, lr : 0.005220\n",
      "Epoch: 9989/25000, Training Loss: 0.000018, lr : 0.005220\n",
      "Epoch: 9990/25000, Training Loss: 0.000017, lr : 0.005220\n",
      "Epoch: 9991/25000, Training Loss: 0.000017, lr : 0.005220\n",
      "Epoch: 9992/25000, Training Loss: 0.000017, lr : 0.005220\n",
      "Epoch: 9993/25000, Training Loss: 0.000014, lr : 0.005220\n",
      "Epoch: 9994/25000, Training Loss: 0.000017, lr : 0.005220\n",
      "Epoch: 9995/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 9996/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 9997/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 9998/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 9999/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10000/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10001/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10002/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10003/25000, Training Loss: 0.000014, lr : 0.005220\n",
      "Epoch: 10004/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10005/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10006/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10007/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10008/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10009/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10010/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10011/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10012/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10013/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10014/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10015/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10016/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10017/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10018/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10019/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10020/25000, Training Loss: 0.000017, lr : 0.005220\n",
      "Epoch: 10021/25000, Training Loss: 0.000014, lr : 0.005220\n",
      "Epoch: 10022/25000, Training Loss: 0.000014, lr : 0.005220\n",
      "Epoch: 10023/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10024/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10025/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10026/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10027/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10028/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10029/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10030/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10031/25000, Training Loss: 0.000017, lr : 0.005220\n",
      "Epoch: 10032/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10033/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10034/25000, Training Loss: 0.000014, lr : 0.005220\n",
      "Epoch: 10035/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10036/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10037/25000, Training Loss: 0.000008, lr : 0.005220\n",
      "Epoch: 10038/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10039/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10040/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10041/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10042/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10043/25000, Training Loss: 0.000015, lr : 0.005220\n",
      "Epoch: 10044/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10045/25000, Training Loss: 0.000014, lr : 0.005220\n",
      "Epoch: 10046/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10047/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10048/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10049/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10050/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10051/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10052/25000, Training Loss: 0.000008, lr : 0.005220\n",
      "Epoch: 10053/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10054/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10055/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10056/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10057/25000, Training Loss: 0.000007, lr : 0.005220\n",
      "Epoch: 10058/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10059/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10060/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10061/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10062/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10063/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10064/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10065/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10066/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10067/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10068/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10069/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10070/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10071/25000, Training Loss: 0.000008, lr : 0.005220\n",
      "Epoch: 10072/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10073/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10074/25000, Training Loss: 0.000008, lr : 0.005220\n",
      "Epoch: 10075/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10076/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10077/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10078/25000, Training Loss: 0.000008, lr : 0.005220\n",
      "Epoch: 10079/25000, Training Loss: 0.000008, lr : 0.005220\n",
      "Epoch: 10080/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10081/25000, Training Loss: 0.000014, lr : 0.005220\n",
      "Epoch: 10082/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10083/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10084/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10085/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10086/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10087/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10088/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10089/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10090/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10091/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10092/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10093/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10094/25000, Training Loss: 0.000014, lr : 0.005220\n",
      "Epoch: 10095/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10096/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10097/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10098/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10099/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10100/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10101/25000, Training Loss: 0.000011, lr : 0.005220\n",
      "Epoch: 10102/25000, Training Loss: 0.000008, lr : 0.005220\n",
      "Epoch: 10103/25000, Training Loss: 0.000010, lr : 0.005220\n",
      "Epoch: 10104/25000, Training Loss: 0.000009, lr : 0.005220\n",
      "Epoch: 10105/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10106/25000, Training Loss: 0.000022, lr : 0.005220\n",
      "Epoch: 10107/25000, Training Loss: 0.000015, lr : 0.005220\n",
      "Epoch: 10108/25000, Training Loss: 0.000018, lr : 0.005220\n",
      "Epoch: 10109/25000, Training Loss: 0.000026, lr : 0.005220\n",
      "Epoch: 10110/25000, Training Loss: 0.000016, lr : 0.005220\n",
      "Epoch: 10111/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 10112/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10113/25000, Training Loss: 0.000023, lr : 0.005220\n",
      "Epoch: 10114/25000, Training Loss: 0.000032, lr : 0.005220\n",
      "Epoch: 10115/25000, Training Loss: 0.000016, lr : 0.005220\n",
      "Epoch: 10116/25000, Training Loss: 0.000034, lr : 0.005220\n",
      "Epoch: 10117/25000, Training Loss: 0.000040, lr : 0.005220\n",
      "Epoch: 10118/25000, Training Loss: 0.000018, lr : 0.005220\n",
      "Epoch: 10119/25000, Training Loss: 0.000026, lr : 0.005220\n",
      "Epoch: 10120/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 10121/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 10122/25000, Training Loss: 0.000022, lr : 0.005220\n",
      "Epoch: 10123/25000, Training Loss: 0.000016, lr : 0.005220\n",
      "Epoch: 10124/25000, Training Loss: 0.000022, lr : 0.005220\n",
      "Epoch: 10125/25000, Training Loss: 0.000026, lr : 0.005220\n",
      "Epoch: 10126/25000, Training Loss: 0.000023, lr : 0.005220\n",
      "Epoch: 10127/25000, Training Loss: 0.000030, lr : 0.005220\n",
      "Epoch: 10128/25000, Training Loss: 0.000026, lr : 0.005220\n",
      "Epoch: 10129/25000, Training Loss: 0.000028, lr : 0.005220\n",
      "Epoch: 10130/25000, Training Loss: 0.000021, lr : 0.005220\n",
      "Epoch: 10131/25000, Training Loss: 0.000028, lr : 0.005220\n",
      "Epoch: 10132/25000, Training Loss: 0.000034, lr : 0.005220\n",
      "Epoch: 10133/25000, Training Loss: 0.000020, lr : 0.005220\n",
      "Epoch: 10134/25000, Training Loss: 0.000048, lr : 0.005220\n",
      "Epoch: 10135/25000, Training Loss: 0.000041, lr : 0.005220\n",
      "Epoch: 10136/25000, Training Loss: 0.000031, lr : 0.005220\n",
      "Epoch: 10137/25000, Training Loss: 0.000048, lr : 0.005220\n",
      "Epoch: 10138/25000, Training Loss: 0.000038, lr : 0.005220\n",
      "Epoch: 10139/25000, Training Loss: 0.000033, lr : 0.005220\n",
      "Epoch: 10140/25000, Training Loss: 0.000034, lr : 0.005220\n",
      "Epoch: 10141/25000, Training Loss: 0.000036, lr : 0.005220\n",
      "Epoch: 10142/25000, Training Loss: 0.000039, lr : 0.005220\n",
      "Epoch: 10143/25000, Training Loss: 0.000027, lr : 0.005220\n",
      "Epoch: 10144/25000, Training Loss: 0.000035, lr : 0.005220\n",
      "Epoch: 10145/25000, Training Loss: 0.000029, lr : 0.005220\n",
      "Epoch: 10146/25000, Training Loss: 0.000032, lr : 0.005220\n",
      "Epoch: 10147/25000, Training Loss: 0.000031, lr : 0.005220\n",
      "Epoch: 10148/25000, Training Loss: 0.000023, lr : 0.005220\n",
      "Epoch: 10149/25000, Training Loss: 0.000023, lr : 0.005220\n",
      "Epoch: 10150/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 10151/25000, Training Loss: 0.000018, lr : 0.005220\n",
      "Epoch: 10152/25000, Training Loss: 0.000024, lr : 0.005220\n",
      "Epoch: 10153/25000, Training Loss: 0.000024, lr : 0.005220\n",
      "Epoch: 10154/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 10155/25000, Training Loss: 0.000017, lr : 0.005220\n",
      "Epoch: 10156/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 10157/25000, Training Loss: 0.000027, lr : 0.005220\n",
      "Epoch: 10158/25000, Training Loss: 0.000020, lr : 0.005220\n",
      "Epoch: 10159/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 10160/25000, Training Loss: 0.000031, lr : 0.005220\n",
      "Epoch: 10161/25000, Training Loss: 0.000028, lr : 0.005220\n",
      "Epoch: 10162/25000, Training Loss: 0.000032, lr : 0.005220\n",
      "Epoch: 10163/25000, Training Loss: 0.000035, lr : 0.005220\n",
      "Epoch: 10164/25000, Training Loss: 0.000052, lr : 0.005220\n",
      "Epoch: 10165/25000, Training Loss: 0.000036, lr : 0.005220\n",
      "Epoch: 10166/25000, Training Loss: 0.000039, lr : 0.005220\n",
      "Epoch: 10167/25000, Training Loss: 0.000038, lr : 0.005220\n",
      "Epoch: 10168/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 10169/25000, Training Loss: 0.000027, lr : 0.005220\n",
      "Epoch: 10170/25000, Training Loss: 0.000024, lr : 0.005220\n",
      "Epoch: 10171/25000, Training Loss: 0.000023, lr : 0.005220\n",
      "Epoch: 10172/25000, Training Loss: 0.000026, lr : 0.005220\n",
      "Epoch: 10173/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 10174/25000, Training Loss: 0.000023, lr : 0.005220\n",
      "Epoch: 10175/25000, Training Loss: 0.000016, lr : 0.005220\n",
      "Epoch: 10176/25000, Training Loss: 0.000024, lr : 0.005220\n",
      "Epoch: 10177/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 10178/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 10179/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 10180/25000, Training Loss: 0.000021, lr : 0.005220\n",
      "Epoch: 10181/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 10182/25000, Training Loss: 0.000028, lr : 0.005220\n",
      "Epoch: 10183/25000, Training Loss: 0.000021, lr : 0.005220\n",
      "Epoch: 10184/25000, Training Loss: 0.000021, lr : 0.005220\n",
      "Epoch: 10185/25000, Training Loss: 0.000022, lr : 0.005220\n",
      "Epoch: 10186/25000, Training Loss: 0.000021, lr : 0.005220\n",
      "Epoch: 10187/25000, Training Loss: 0.000018, lr : 0.005220\n",
      "Epoch: 10188/25000, Training Loss: 0.000018, lr : 0.005220\n",
      "Epoch: 10189/25000, Training Loss: 0.000016, lr : 0.005220\n",
      "Epoch: 10190/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 10191/25000, Training Loss: 0.000017, lr : 0.005220\n",
      "Epoch: 10192/25000, Training Loss: 0.000015, lr : 0.005220\n",
      "Epoch: 10193/25000, Training Loss: 0.000016, lr : 0.005220\n",
      "Epoch: 10194/25000, Training Loss: 0.000034, lr : 0.005220\n",
      "Epoch: 10195/25000, Training Loss: 0.000040, lr : 0.005220\n",
      "Epoch: 10196/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 10197/25000, Training Loss: 0.000020, lr : 0.005220\n",
      "Epoch: 10198/25000, Training Loss: 0.000053, lr : 0.005220\n",
      "Epoch: 10199/25000, Training Loss: 0.000046, lr : 0.005220\n",
      "Epoch: 10200/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 10201/25000, Training Loss: 0.000032, lr : 0.005220\n",
      "Epoch: 10202/25000, Training Loss: 0.000057, lr : 0.005220\n",
      "Epoch: 10203/25000, Training Loss: 0.000049, lr : 0.005220\n",
      "Epoch: 10204/25000, Training Loss: 0.000029, lr : 0.005220\n",
      "Epoch: 10205/25000, Training Loss: 0.000064, lr : 0.005220\n",
      "Epoch: 10206/25000, Training Loss: 0.000067, lr : 0.005220\n",
      "Epoch: 10207/25000, Training Loss: 0.000029, lr : 0.005220\n",
      "Epoch: 10208/25000, Training Loss: 0.000049, lr : 0.005220\n",
      "Epoch: 10209/25000, Training Loss: 0.000078, lr : 0.005220\n",
      "Epoch: 10210/25000, Training Loss: 0.000035, lr : 0.005220\n",
      "Epoch: 10211/25000, Training Loss: 0.000028, lr : 0.005220\n",
      "Epoch: 10212/25000, Training Loss: 0.000028, lr : 0.005220\n",
      "Epoch: 10213/25000, Training Loss: 0.000024, lr : 0.005220\n",
      "Epoch: 10214/25000, Training Loss: 0.000021, lr : 0.005220\n",
      "Epoch: 10215/25000, Training Loss: 0.000016, lr : 0.005220\n",
      "Epoch: 10216/25000, Training Loss: 0.000015, lr : 0.005220\n",
      "Epoch: 10217/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10218/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10219/25000, Training Loss: 0.000016, lr : 0.005220\n",
      "Epoch: 10220/25000, Training Loss: 0.000017, lr : 0.005220\n",
      "Epoch: 10221/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10222/25000, Training Loss: 0.000015, lr : 0.005220\n",
      "Epoch: 10223/25000, Training Loss: 0.000022, lr : 0.005220\n",
      "Epoch: 10224/25000, Training Loss: 0.000022, lr : 0.005220\n",
      "Epoch: 10225/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10226/25000, Training Loss: 0.000018, lr : 0.005220\n",
      "Epoch: 10227/25000, Training Loss: 0.000018, lr : 0.005220\n",
      "Epoch: 10228/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10229/25000, Training Loss: 0.000013, lr : 0.005220\n",
      "Epoch: 10230/25000, Training Loss: 0.000015, lr : 0.005220\n",
      "Epoch: 10231/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 10232/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 10233/25000, Training Loss: 0.000015, lr : 0.005220\n",
      "Epoch: 10234/25000, Training Loss: 0.000012, lr : 0.005220\n",
      "Epoch: 10235/25000, Training Loss: 0.000021, lr : 0.005220\n",
      "Epoch: 10236/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 10237/25000, Training Loss: 0.000020, lr : 0.005220\n",
      "Epoch: 10238/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 10239/25000, Training Loss: 0.000037, lr : 0.005220\n",
      "Epoch: 10240/25000, Training Loss: 0.000031, lr : 0.005220\n",
      "Epoch: 10241/25000, Training Loss: 0.000018, lr : 0.005220\n",
      "Epoch: 10242/25000, Training Loss: 0.000022, lr : 0.005220\n",
      "Epoch: 10243/25000, Training Loss: 0.000018, lr : 0.005220\n",
      "Epoch: 10244/25000, Training Loss: 0.000027, lr : 0.005220\n",
      "Epoch: 10245/25000, Training Loss: 0.000030, lr : 0.005220\n",
      "Epoch: 10246/25000, Training Loss: 0.000038, lr : 0.005220\n",
      "Epoch: 10247/25000, Training Loss: 0.000040, lr : 0.005220\n",
      "Epoch: 10248/25000, Training Loss: 0.000030, lr : 0.005220\n",
      "Epoch: 10249/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 10250/25000, Training Loss: 0.000021, lr : 0.005220\n",
      "Epoch: 10251/25000, Training Loss: 0.000028, lr : 0.005220\n",
      "Epoch: 10252/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 10253/25000, Training Loss: 0.000033, lr : 0.005220\n",
      "Epoch: 10254/25000, Training Loss: 0.000022, lr : 0.005220\n",
      "Epoch: 10255/25000, Training Loss: 0.000029, lr : 0.005220\n",
      "Epoch: 10256/25000, Training Loss: 0.000023, lr : 0.005220\n",
      "Epoch: 10257/25000, Training Loss: 0.000024, lr : 0.005220\n",
      "Epoch: 10258/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 10259/25000, Training Loss: 0.000022, lr : 0.005220\n",
      "Epoch: 10260/25000, Training Loss: 0.000021, lr : 0.005220\n",
      "Epoch: 10261/25000, Training Loss: 0.000021, lr : 0.005220\n",
      "Epoch: 10262/25000, Training Loss: 0.000021, lr : 0.005220\n",
      "Epoch: 10263/25000, Training Loss: 0.000018, lr : 0.005220\n",
      "Epoch: 10264/25000, Training Loss: 0.000019, lr : 0.005220\n",
      "Epoch: 10265/25000, Training Loss: 0.000023, lr : 0.005220\n",
      "Epoch: 10266/25000, Training Loss: 0.000030, lr : 0.005220\n",
      "Epoch: 10267/25000, Training Loss: 0.000029, lr : 0.005220\n",
      "Epoch: 10268/25000, Training Loss: 0.000025, lr : 0.005220\n",
      "Epoch: 10269/25000, Training Loss: 0.000023, lr : 0.005220\n",
      "Epoch: 10270/25000, Training Loss: 0.000018, lr : 0.005220\n",
      "Epoch: 10271/25000, Training Loss: 0.000034, lr : 0.005220\n",
      "Epoch: 10272/25000, Training Loss: 0.000059, lr : 0.005220\n",
      "Epoch: 10273/25000, Training Loss: 0.000068, lr : 0.005220\n",
      "Epoch: 10274/25000, Training Loss: 0.000042, lr : 0.004437\n",
      "Epoch: 10275/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 10276/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10277/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10278/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10279/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10280/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10281/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10282/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10283/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10284/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10285/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10286/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10287/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10288/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10289/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10290/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10291/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10292/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10293/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10294/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10295/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10296/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 10297/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10298/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10299/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10300/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10301/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10302/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10303/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10304/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10305/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10306/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10307/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10308/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10309/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10310/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10311/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10312/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10313/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10314/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10315/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10316/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10317/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10318/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10319/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10320/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10321/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10322/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10323/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 10324/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10325/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10326/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 10327/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10328/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10329/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10330/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10331/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10332/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10333/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10334/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10335/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10336/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10337/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10338/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 10339/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10340/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10341/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10342/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10343/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10344/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10345/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10346/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10347/25000, Training Loss: 0.000037, lr : 0.004437\n",
      "Epoch: 10348/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10349/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10350/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10351/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 10352/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 10353/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10354/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 10355/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10356/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10357/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10358/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10359/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10360/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10361/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10362/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10363/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10364/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10365/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10366/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10367/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10368/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10369/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10370/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10371/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10372/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10373/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10374/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10375/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10376/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10377/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10378/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10379/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10380/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10381/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10382/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10383/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10384/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10385/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10386/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10387/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10388/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10389/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10390/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10391/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10392/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10393/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10394/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10395/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10396/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10397/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10398/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10399/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10400/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10401/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10402/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10403/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 10404/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 10405/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10406/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10407/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10408/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10409/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10410/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10411/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10412/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10413/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10414/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10415/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10416/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10417/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10418/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10419/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10420/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10421/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10422/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10423/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10424/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10425/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10426/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10427/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10428/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10429/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10430/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10431/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10432/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10433/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10434/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10435/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10436/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10437/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10438/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10439/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10440/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10441/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 10442/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10443/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10444/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10445/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10446/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 10447/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10448/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10449/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10450/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10451/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10452/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10453/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10454/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10455/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10456/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10457/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10458/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10459/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10460/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10461/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10462/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10463/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10464/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10465/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10466/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10467/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10468/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10469/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10470/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10471/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10472/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10473/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10474/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10475/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10476/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10477/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10478/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10479/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10480/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10481/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10482/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10483/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10484/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10485/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10486/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10487/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 10488/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 10489/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 10490/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 10491/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10492/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10493/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10494/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10495/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 10496/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 10497/25000, Training Loss: 0.000051, lr : 0.004437\n",
      "Epoch: 10498/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 10499/25000, Training Loss: 0.000041, lr : 0.004437\n",
      "Epoch: 10500/25000, Training Loss: 0.000047, lr : 0.004437\n",
      "Epoch: 10501/25000, Training Loss: 0.000043, lr : 0.004437\n",
      "Epoch: 10502/25000, Training Loss: 0.000044, lr : 0.004437\n",
      "Epoch: 10503/25000, Training Loss: 0.000094, lr : 0.004437\n",
      "Epoch: 10504/25000, Training Loss: 0.000072, lr : 0.004437\n",
      "Epoch: 10505/25000, Training Loss: 0.000038, lr : 0.004437\n",
      "Epoch: 10506/25000, Training Loss: 0.000074, lr : 0.004437\n",
      "Epoch: 10507/25000, Training Loss: 0.000058, lr : 0.004437\n",
      "Epoch: 10508/25000, Training Loss: 0.000058, lr : 0.004437\n",
      "Epoch: 10509/25000, Training Loss: 0.000105, lr : 0.004437\n",
      "Epoch: 10510/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 10511/25000, Training Loss: 0.000085, lr : 0.004437\n",
      "Epoch: 10512/25000, Training Loss: 0.000141, lr : 0.004437\n",
      "Epoch: 10513/25000, Training Loss: 0.000069, lr : 0.004437\n",
      "Epoch: 10514/25000, Training Loss: 0.000140, lr : 0.004437\n",
      "Epoch: 10515/25000, Training Loss: 0.000129, lr : 0.004437\n",
      "Epoch: 10516/25000, Training Loss: 0.000048, lr : 0.004437\n",
      "Epoch: 10517/25000, Training Loss: 0.000075, lr : 0.004437\n",
      "Epoch: 10518/25000, Training Loss: 0.000038, lr : 0.004437\n",
      "Epoch: 10519/25000, Training Loss: 0.000053, lr : 0.004437\n",
      "Epoch: 10520/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 10521/25000, Training Loss: 0.000044, lr : 0.004437\n",
      "Epoch: 10522/25000, Training Loss: 0.000041, lr : 0.004437\n",
      "Epoch: 10523/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 10524/25000, Training Loss: 0.000042, lr : 0.004437\n",
      "Epoch: 10525/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10526/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 10527/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10528/25000, Training Loss: 0.000041, lr : 0.004437\n",
      "Epoch: 10529/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 10530/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 10531/25000, Training Loss: 0.000042, lr : 0.004437\n",
      "Epoch: 10532/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 10533/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10534/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 10535/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10536/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10537/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10538/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10539/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10540/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10541/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10542/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10543/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10544/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10545/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10546/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10547/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10548/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10549/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10550/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10551/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10552/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10553/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10554/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10555/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10556/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10557/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10558/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10559/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10560/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10561/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10562/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10563/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10564/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10565/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10566/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10567/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10568/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10569/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10570/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10571/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10572/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10573/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10574/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10575/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10576/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10577/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10578/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 10579/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10580/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10581/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10582/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10583/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10584/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10585/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10586/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10587/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10588/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10589/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10590/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10591/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10592/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10593/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10594/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10595/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10596/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10597/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10598/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10599/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10600/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10601/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10602/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10603/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10604/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10605/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10606/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10607/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10608/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10609/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10610/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10611/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10612/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10613/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10614/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10615/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10616/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10617/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10618/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10619/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10620/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10621/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10622/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10623/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10624/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10625/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10626/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10627/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10628/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10629/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10630/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10631/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 10632/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10633/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10634/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10635/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10636/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10637/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10638/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10639/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10640/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10641/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10642/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10643/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10644/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10645/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10646/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10647/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10648/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10649/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10650/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10651/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10652/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10653/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10654/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10655/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10656/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10657/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10658/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10659/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10660/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10661/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10662/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10663/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10664/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10665/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10666/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10667/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10668/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10669/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10670/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 10671/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 10672/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10673/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 10674/25000, Training Loss: 0.000057, lr : 0.004437\n",
      "Epoch: 10675/25000, Training Loss: 0.000059, lr : 0.004437\n",
      "Epoch: 10676/25000, Training Loss: 0.000036, lr : 0.004437\n",
      "Epoch: 10677/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10678/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 10679/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 10680/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 10681/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10682/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 10683/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 10684/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10685/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 10686/25000, Training Loss: 0.000061, lr : 0.004437\n",
      "Epoch: 10687/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10688/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10689/25000, Training Loss: 0.000062, lr : 0.004437\n",
      "Epoch: 10690/25000, Training Loss: 0.000068, lr : 0.004437\n",
      "Epoch: 10691/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10692/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10693/25000, Training Loss: 0.000037, lr : 0.004437\n",
      "Epoch: 10694/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 10695/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10696/25000, Training Loss: 0.000043, lr : 0.004437\n",
      "Epoch: 10697/25000, Training Loss: 0.000053, lr : 0.004437\n",
      "Epoch: 10698/25000, Training Loss: 0.000037, lr : 0.004437\n",
      "Epoch: 10699/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10700/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10701/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 10702/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10703/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 10704/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 10705/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10706/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 10707/25000, Training Loss: 0.000051, lr : 0.004437\n",
      "Epoch: 10708/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10709/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10710/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 10711/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 10712/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10713/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10714/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 10715/25000, Training Loss: 0.000038, lr : 0.004437\n",
      "Epoch: 10716/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10717/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10718/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10719/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10720/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10721/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10722/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10723/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10724/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10725/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10726/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10727/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10728/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10729/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10730/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10731/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10732/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10733/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10734/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10735/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10736/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10737/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 10738/25000, Training Loss: 0.000049, lr : 0.004437\n",
      "Epoch: 10739/25000, Training Loss: 0.000048, lr : 0.004437\n",
      "Epoch: 10740/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10741/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10742/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10743/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 10744/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10745/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10746/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10747/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10748/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10749/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10750/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10751/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10752/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10753/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10754/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10755/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10756/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10757/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 10758/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10759/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10760/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10761/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 10762/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 10763/25000, Training Loss: 0.000036, lr : 0.004437\n",
      "Epoch: 10764/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10765/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10766/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10767/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10768/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10769/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10770/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 10771/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 10772/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 10773/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10774/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10775/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 10776/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10777/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 10778/25000, Training Loss: 0.000044, lr : 0.004437\n",
      "Epoch: 10779/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10780/25000, Training Loss: 0.000055, lr : 0.004437\n",
      "Epoch: 10781/25000, Training Loss: 0.000056, lr : 0.004437\n",
      "Epoch: 10782/25000, Training Loss: 0.000047, lr : 0.004437\n",
      "Epoch: 10783/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 10784/25000, Training Loss: 0.000042, lr : 0.004437\n",
      "Epoch: 10785/25000, Training Loss: 0.000044, lr : 0.004437\n",
      "Epoch: 10786/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10787/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 10788/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 10789/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10790/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10791/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 10792/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10793/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10794/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10795/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10796/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10797/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 10798/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10799/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10800/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10801/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10802/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 10803/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 10804/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10805/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 10806/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 10807/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 10808/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10809/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10810/25000, Training Loss: 0.000042, lr : 0.004437\n",
      "Epoch: 10811/25000, Training Loss: 0.000041, lr : 0.004437\n",
      "Epoch: 10812/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10813/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10814/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 10815/25000, Training Loss: 0.000038, lr : 0.004437\n",
      "Epoch: 10816/25000, Training Loss: 0.000045, lr : 0.004437\n",
      "Epoch: 10817/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 10818/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10819/25000, Training Loss: 0.000036, lr : 0.004437\n",
      "Epoch: 10820/25000, Training Loss: 0.000043, lr : 0.004437\n",
      "Epoch: 10821/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10822/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10823/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10824/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10825/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10826/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10827/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10828/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10829/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10830/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10831/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10832/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10833/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10834/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10835/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10836/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10837/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10838/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10839/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10840/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10841/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10842/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10843/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 10844/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 10845/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10846/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10847/25000, Training Loss: 0.000036, lr : 0.004437\n",
      "Epoch: 10848/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 10849/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10850/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10851/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10852/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10853/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10854/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10855/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10856/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10857/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10858/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10859/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10860/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10861/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10862/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10863/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10864/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10865/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10866/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10867/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10868/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10869/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10870/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10871/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10872/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10873/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10874/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10875/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10876/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10877/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10878/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10879/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10880/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10881/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10882/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10883/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10884/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 10885/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10886/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10887/25000, Training Loss: 0.000052, lr : 0.004437\n",
      "Epoch: 10888/25000, Training Loss: 0.000058, lr : 0.004437\n",
      "Epoch: 10889/25000, Training Loss: 0.000042, lr : 0.004437\n",
      "Epoch: 10890/25000, Training Loss: 0.000038, lr : 0.004437\n",
      "Epoch: 10891/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 10892/25000, Training Loss: 0.000048, lr : 0.004437\n",
      "Epoch: 10893/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 10894/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10895/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10896/25000, Training Loss: 0.000044, lr : 0.004437\n",
      "Epoch: 10897/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 10898/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10899/25000, Training Loss: 0.000046, lr : 0.004437\n",
      "Epoch: 10900/25000, Training Loss: 0.000066, lr : 0.004437\n",
      "Epoch: 10901/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 10902/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10903/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 10904/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 10905/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10906/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 10907/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 10908/25000, Training Loss: 0.000045, lr : 0.004437\n",
      "Epoch: 10909/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 10910/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10911/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10912/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10913/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10914/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10915/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10916/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10917/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10918/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10919/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10920/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10921/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10922/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10923/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10924/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10925/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10926/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10927/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10928/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10929/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10930/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 10931/25000, Training Loss: 0.000044, lr : 0.004437\n",
      "Epoch: 10932/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 10933/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 10934/25000, Training Loss: 0.000076, lr : 0.004437\n",
      "Epoch: 10935/25000, Training Loss: 0.000098, lr : 0.004437\n",
      "Epoch: 10936/25000, Training Loss: 0.000050, lr : 0.004437\n",
      "Epoch: 10937/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10938/25000, Training Loss: 0.000070, lr : 0.004437\n",
      "Epoch: 10939/25000, Training Loss: 0.000046, lr : 0.004437\n",
      "Epoch: 10940/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 10941/25000, Training Loss: 0.000038, lr : 0.004437\n",
      "Epoch: 10942/25000, Training Loss: 0.000037, lr : 0.004437\n",
      "Epoch: 10943/25000, Training Loss: 0.000044, lr : 0.004437\n",
      "Epoch: 10944/25000, Training Loss: 0.000047, lr : 0.004437\n",
      "Epoch: 10945/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 10946/25000, Training Loss: 0.000041, lr : 0.004437\n",
      "Epoch: 10947/25000, Training Loss: 0.000062, lr : 0.004437\n",
      "Epoch: 10948/25000, Training Loss: 0.000067, lr : 0.004437\n",
      "Epoch: 10949/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 10950/25000, Training Loss: 0.000045, lr : 0.004437\n",
      "Epoch: 10951/25000, Training Loss: 0.000043, lr : 0.004437\n",
      "Epoch: 10952/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 10953/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 10954/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 10955/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10956/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10957/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 10958/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 10959/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10960/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 10961/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10962/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10963/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10964/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10965/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10966/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10967/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10968/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 10969/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10970/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10971/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10972/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10973/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 10974/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10975/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10976/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10977/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 10978/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10979/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10980/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 10981/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10982/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10983/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10984/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10985/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10986/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10987/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10988/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10989/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10990/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 10991/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10992/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 10993/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 10994/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 10995/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 10996/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10997/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 10998/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 10999/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11000/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11001/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 11002/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11003/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11004/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11005/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11006/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11007/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11008/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11009/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11010/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11011/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11012/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11013/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11014/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11015/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11016/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11017/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11018/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11019/25000, Training Loss: 0.000041, lr : 0.004437\n",
      "Epoch: 11020/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11021/25000, Training Loss: 0.000036, lr : 0.004437\n",
      "Epoch: 11022/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11023/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11024/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11025/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11026/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11027/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11028/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11029/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11030/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11031/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11032/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11033/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11034/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11035/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11036/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11037/25000, Training Loss: 0.000042, lr : 0.004437\n",
      "Epoch: 11038/25000, Training Loss: 0.000044, lr : 0.004437\n",
      "Epoch: 11039/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11040/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11041/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11042/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11043/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11044/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11045/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11046/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11047/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11048/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11049/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11050/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11051/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11052/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11053/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11054/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11055/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11056/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11057/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 11058/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11059/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11060/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11061/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11062/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11063/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11064/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11065/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11066/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11067/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11068/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11069/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11070/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11071/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11072/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11073/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11074/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11075/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11076/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11077/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11078/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11079/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11080/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11081/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11082/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11083/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11084/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11085/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 11086/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11087/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11088/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11089/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11090/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11091/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11092/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11093/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 11094/25000, Training Loss: 0.000041, lr : 0.004437\n",
      "Epoch: 11095/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11096/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11097/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11098/25000, Training Loss: 0.000042, lr : 0.004437\n",
      "Epoch: 11099/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 11100/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11101/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 11102/25000, Training Loss: 0.000077, lr : 0.004437\n",
      "Epoch: 11103/25000, Training Loss: 0.000106, lr : 0.004437\n",
      "Epoch: 11104/25000, Training Loss: 0.000056, lr : 0.004437\n",
      "Epoch: 11105/25000, Training Loss: 0.000051, lr : 0.004437\n",
      "Epoch: 11106/25000, Training Loss: 0.000089, lr : 0.004437\n",
      "Epoch: 11107/25000, Training Loss: 0.000082, lr : 0.004437\n",
      "Epoch: 11108/25000, Training Loss: 0.000046, lr : 0.004437\n",
      "Epoch: 11109/25000, Training Loss: 0.000062, lr : 0.004437\n",
      "Epoch: 11110/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 11111/25000, Training Loss: 0.000046, lr : 0.004437\n",
      "Epoch: 11112/25000, Training Loss: 0.000049, lr : 0.004437\n",
      "Epoch: 11113/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 11114/25000, Training Loss: 0.000052, lr : 0.004437\n",
      "Epoch: 11115/25000, Training Loss: 0.000041, lr : 0.004437\n",
      "Epoch: 11116/25000, Training Loss: 0.000057, lr : 0.004437\n",
      "Epoch: 11117/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11118/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 11119/25000, Training Loss: 0.000037, lr : 0.004437\n",
      "Epoch: 11120/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 11121/25000, Training Loss: 0.000069, lr : 0.004437\n",
      "Epoch: 11122/25000, Training Loss: 0.000075, lr : 0.004437\n",
      "Epoch: 11123/25000, Training Loss: 0.000077, lr : 0.004437\n",
      "Epoch: 11124/25000, Training Loss: 0.000087, lr : 0.004437\n",
      "Epoch: 11125/25000, Training Loss: 0.000098, lr : 0.004437\n",
      "Epoch: 11126/25000, Training Loss: 0.000054, lr : 0.004437\n",
      "Epoch: 11127/25000, Training Loss: 0.000088, lr : 0.004437\n",
      "Epoch: 11128/25000, Training Loss: 0.000067, lr : 0.004437\n",
      "Epoch: 11129/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 11130/25000, Training Loss: 0.000046, lr : 0.004437\n",
      "Epoch: 11131/25000, Training Loss: 0.000043, lr : 0.004437\n",
      "Epoch: 11132/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 11133/25000, Training Loss: 0.000044, lr : 0.004437\n",
      "Epoch: 11134/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11135/25000, Training Loss: 0.000045, lr : 0.004437\n",
      "Epoch: 11136/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11137/25000, Training Loss: 0.000041, lr : 0.004437\n",
      "Epoch: 11138/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 11139/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11140/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11141/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11142/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11143/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11144/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11145/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11146/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11147/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11148/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11149/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11150/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11151/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11152/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11153/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11154/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11155/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11156/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11157/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11158/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11159/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11160/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11161/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11162/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11163/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 11164/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11165/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11166/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11167/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11168/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11169/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 11170/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11171/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11172/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11173/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11174/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11175/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11176/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11177/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11178/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11179/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11180/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11181/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11182/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11183/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11184/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11185/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11186/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11187/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11188/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11189/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11190/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11191/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11192/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11193/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11194/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11195/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11196/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11197/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11198/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11199/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11200/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11201/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11202/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11203/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11204/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11205/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11206/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11207/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11208/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11209/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11210/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11211/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11212/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11213/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11214/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11215/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11216/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11217/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11218/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11219/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11220/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11221/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11222/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11223/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11224/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 11225/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11226/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11227/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11228/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11229/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11230/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11231/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11232/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11233/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11234/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11235/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11236/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11237/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11238/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11239/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11240/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11241/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11242/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11243/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11244/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11245/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11246/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11247/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11248/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11249/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11250/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11251/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11252/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11253/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11254/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11255/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11256/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11257/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11258/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11259/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11260/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11261/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11262/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11263/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11264/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11265/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11266/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11267/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11268/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11269/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11270/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11271/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11272/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11273/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11274/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11275/25000, Training Loss: 0.000037, lr : 0.004437\n",
      "Epoch: 11276/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11277/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11278/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11279/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11280/25000, Training Loss: 0.000042, lr : 0.004437\n",
      "Epoch: 11281/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11282/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11283/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11284/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11285/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11286/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11287/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11288/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11289/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11290/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11291/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11292/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11293/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11294/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11295/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11296/25000, Training Loss: 0.000056, lr : 0.004437\n",
      "Epoch: 11297/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 11298/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11299/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11300/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11301/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11302/25000, Training Loss: 0.000043, lr : 0.004437\n",
      "Epoch: 11303/25000, Training Loss: 0.000044, lr : 0.004437\n",
      "Epoch: 11304/25000, Training Loss: 0.000038, lr : 0.004437\n",
      "Epoch: 11305/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11306/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 11307/25000, Training Loss: 0.000051, lr : 0.004437\n",
      "Epoch: 11308/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 11309/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 11310/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 11311/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11312/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11313/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11314/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11315/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 11316/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 11317/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11318/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11319/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11320/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11321/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11322/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 11323/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11324/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11325/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11326/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11327/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11328/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11329/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11330/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11331/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11332/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11333/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11334/25000, Training Loss: 0.000049, lr : 0.004437\n",
      "Epoch: 11335/25000, Training Loss: 0.000047, lr : 0.004437\n",
      "Epoch: 11336/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11337/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 11338/25000, Training Loss: 0.000041, lr : 0.004437\n",
      "Epoch: 11339/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11340/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 11341/25000, Training Loss: 0.000066, lr : 0.004437\n",
      "Epoch: 11342/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11343/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 11344/25000, Training Loss: 0.000050, lr : 0.004437\n",
      "Epoch: 11345/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 11346/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11347/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11348/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11349/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11350/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11351/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11352/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11353/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11354/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11355/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11356/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11357/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11358/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11359/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11360/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11361/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11362/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11363/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11364/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11365/25000, Training Loss: 0.000007, lr : 0.004437\n",
      "Epoch: 11366/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11367/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11368/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11369/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11370/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11371/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11372/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11373/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11374/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11375/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11376/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11377/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11378/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11379/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11380/25000, Training Loss: 0.000050, lr : 0.004437\n",
      "Epoch: 11381/25000, Training Loss: 0.000081, lr : 0.004437\n",
      "Epoch: 11382/25000, Training Loss: 0.000068, lr : 0.004437\n",
      "Epoch: 11383/25000, Training Loss: 0.000041, lr : 0.004437\n",
      "Epoch: 11384/25000, Training Loss: 0.000099, lr : 0.004437\n",
      "Epoch: 11385/25000, Training Loss: 0.000103, lr : 0.004437\n",
      "Epoch: 11386/25000, Training Loss: 0.000053, lr : 0.004437\n",
      "Epoch: 11387/25000, Training Loss: 0.000086, lr : 0.004437\n",
      "Epoch: 11388/25000, Training Loss: 0.000066, lr : 0.004437\n",
      "Epoch: 11389/25000, Training Loss: 0.000042, lr : 0.004437\n",
      "Epoch: 11390/25000, Training Loss: 0.000041, lr : 0.004437\n",
      "Epoch: 11391/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 11392/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11393/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 11394/25000, Training Loss: 0.000037, lr : 0.004437\n",
      "Epoch: 11395/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11396/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11397/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 11398/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11399/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 11400/25000, Training Loss: 0.000042, lr : 0.004437\n",
      "Epoch: 11401/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11402/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11403/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11404/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11405/25000, Training Loss: 0.000052, lr : 0.004437\n",
      "Epoch: 11406/25000, Training Loss: 0.000058, lr : 0.004437\n",
      "Epoch: 11407/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 11408/25000, Training Loss: 0.000036, lr : 0.004437\n",
      "Epoch: 11409/25000, Training Loss: 0.000050, lr : 0.004437\n",
      "Epoch: 11410/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 11411/25000, Training Loss: 0.000036, lr : 0.004437\n",
      "Epoch: 11412/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11413/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 11414/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 11415/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 11416/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11417/25000, Training Loss: 0.000038, lr : 0.004437\n",
      "Epoch: 11418/25000, Training Loss: 0.000043, lr : 0.004437\n",
      "Epoch: 11419/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 11420/25000, Training Loss: 0.000056, lr : 0.004437\n",
      "Epoch: 11421/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 11422/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 11423/25000, Training Loss: 0.000062, lr : 0.004437\n",
      "Epoch: 11424/25000, Training Loss: 0.000042, lr : 0.004437\n",
      "Epoch: 11425/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 11426/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11427/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11428/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11429/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 11430/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 11431/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 11432/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11433/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11434/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11435/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11436/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11437/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11438/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11439/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11440/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11441/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11442/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11443/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11444/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11445/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11446/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11447/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11448/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11449/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11450/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11451/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11452/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11453/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11454/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11455/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11456/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11457/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11458/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11459/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11460/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11461/25000, Training Loss: 0.000007, lr : 0.004437\n",
      "Epoch: 11462/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11463/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11464/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11465/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11466/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11467/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11468/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11469/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11470/25000, Training Loss: 0.000036, lr : 0.004437\n",
      "Epoch: 11471/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11472/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11473/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11474/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11475/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11476/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11477/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11478/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11479/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11480/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11481/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11482/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11483/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11484/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11485/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11486/25000, Training Loss: 0.000037, lr : 0.004437\n",
      "Epoch: 11487/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11488/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11489/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11490/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11491/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11492/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11493/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11494/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11495/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11496/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11497/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11498/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11499/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 11500/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11501/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11502/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11503/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11504/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11505/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11506/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11507/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11508/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11509/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11510/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11511/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11512/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11513/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11514/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11515/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11516/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11517/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11518/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11519/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11520/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11521/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11522/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11523/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11524/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11525/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11526/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11527/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11528/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11529/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11530/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11531/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 11532/25000, Training Loss: 0.000036, lr : 0.004437\n",
      "Epoch: 11533/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11534/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11535/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11536/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11537/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11538/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11539/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11540/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11541/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11542/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11543/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11544/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11545/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11546/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11547/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11548/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11549/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11550/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11551/25000, Training Loss: 0.000055, lr : 0.004437\n",
      "Epoch: 11552/25000, Training Loss: 0.000055, lr : 0.004437\n",
      "Epoch: 11553/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 11554/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11555/25000, Training Loss: 0.000047, lr : 0.004437\n",
      "Epoch: 11556/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11557/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11558/25000, Training Loss: 0.000049, lr : 0.004437\n",
      "Epoch: 11559/25000, Training Loss: 0.000036, lr : 0.004437\n",
      "Epoch: 11560/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11561/25000, Training Loss: 0.000053, lr : 0.004437\n",
      "Epoch: 11562/25000, Training Loss: 0.000093, lr : 0.004437\n",
      "Epoch: 11563/25000, Training Loss: 0.000051, lr : 0.004437\n",
      "Epoch: 11564/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 11565/25000, Training Loss: 0.000051, lr : 0.004437\n",
      "Epoch: 11566/25000, Training Loss: 0.000046, lr : 0.004437\n",
      "Epoch: 11567/25000, Training Loss: 0.000050, lr : 0.004437\n",
      "Epoch: 11568/25000, Training Loss: 0.000036, lr : 0.004437\n",
      "Epoch: 11569/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11570/25000, Training Loss: 0.000039, lr : 0.004437\n",
      "Epoch: 11571/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11572/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11573/25000, Training Loss: 0.000047, lr : 0.004437\n",
      "Epoch: 11574/25000, Training Loss: 0.000047, lr : 0.004437\n",
      "Epoch: 11575/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11576/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 11577/25000, Training Loss: 0.000045, lr : 0.004437\n",
      "Epoch: 11578/25000, Training Loss: 0.000038, lr : 0.004437\n",
      "Epoch: 11579/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11580/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 11581/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11582/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11583/25000, Training Loss: 0.000031, lr : 0.004437\n",
      "Epoch: 11584/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 11585/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11586/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11587/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11588/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11589/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11590/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11591/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11592/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11593/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11594/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11595/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11596/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11597/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11598/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11599/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11600/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11601/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11602/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11603/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11604/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11605/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11606/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11607/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11608/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11609/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11610/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 11611/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11612/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11613/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11614/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11615/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11616/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11617/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11618/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11619/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11620/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11621/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11622/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11623/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11624/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11625/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11626/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11627/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11628/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11629/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11630/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11631/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11632/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11633/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11634/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11635/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11636/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11637/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11638/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11639/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11640/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11641/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11642/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11643/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11644/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11645/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11646/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11647/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11648/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11649/25000, Training Loss: 0.000030, lr : 0.004437\n",
      "Epoch: 11650/25000, Training Loss: 0.000050, lr : 0.004437\n",
      "Epoch: 11651/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 11652/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11653/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11654/25000, Training Loss: 0.000058, lr : 0.004437\n",
      "Epoch: 11655/25000, Training Loss: 0.000043, lr : 0.004437\n",
      "Epoch: 11656/25000, Training Loss: 0.000063, lr : 0.004437\n",
      "Epoch: 11657/25000, Training Loss: 0.000232, lr : 0.004437\n",
      "Epoch: 11658/25000, Training Loss: 0.000337, lr : 0.004437\n",
      "Epoch: 11659/25000, Training Loss: 0.000170, lr : 0.004437\n",
      "Epoch: 11660/25000, Training Loss: 0.000187, lr : 0.004437\n",
      "Epoch: 11661/25000, Training Loss: 0.000149, lr : 0.004437\n",
      "Epoch: 11662/25000, Training Loss: 0.000105, lr : 0.004437\n",
      "Epoch: 11663/25000, Training Loss: 0.000167, lr : 0.004437\n",
      "Epoch: 11664/25000, Training Loss: 0.000096, lr : 0.004437\n",
      "Epoch: 11665/25000, Training Loss: 0.000112, lr : 0.004437\n",
      "Epoch: 11666/25000, Training Loss: 0.000078, lr : 0.004437\n",
      "Epoch: 11667/25000, Training Loss: 0.000108, lr : 0.004437\n",
      "Epoch: 11668/25000, Training Loss: 0.000091, lr : 0.004437\n",
      "Epoch: 11669/25000, Training Loss: 0.000105, lr : 0.004437\n",
      "Epoch: 11670/25000, Training Loss: 0.000078, lr : 0.004437\n",
      "Epoch: 11671/25000, Training Loss: 0.000072, lr : 0.004437\n",
      "Epoch: 11672/25000, Training Loss: 0.000063, lr : 0.004437\n",
      "Epoch: 11673/25000, Training Loss: 0.000051, lr : 0.004437\n",
      "Epoch: 11674/25000, Training Loss: 0.000067, lr : 0.004437\n",
      "Epoch: 11675/25000, Training Loss: 0.000051, lr : 0.004437\n",
      "Epoch: 11676/25000, Training Loss: 0.000036, lr : 0.004437\n",
      "Epoch: 11677/25000, Training Loss: 0.000033, lr : 0.004437\n",
      "Epoch: 11678/25000, Training Loss: 0.000032, lr : 0.004437\n",
      "Epoch: 11679/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 11680/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11681/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11682/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11683/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11684/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11685/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11686/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11687/25000, Training Loss: 0.000023, lr : 0.004437\n",
      "Epoch: 11688/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11689/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11690/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11691/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11692/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11693/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11694/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11695/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11696/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11697/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11698/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11699/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11700/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11701/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11702/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11703/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11704/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11705/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11706/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11707/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11708/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11709/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11710/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11711/25000, Training Loss: 0.000017, lr : 0.004437\n",
      "Epoch: 11712/25000, Training Loss: 0.000019, lr : 0.004437\n",
      "Epoch: 11713/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11714/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11715/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11716/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11717/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11718/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11719/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11720/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11721/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11722/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11723/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11724/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11725/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11726/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11727/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11728/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11729/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11730/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11731/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11732/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11733/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11734/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11735/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11736/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11737/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11738/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11739/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11740/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11741/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11742/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11743/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11744/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11745/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11746/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11747/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11748/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11749/25000, Training Loss: 0.000007, lr : 0.004437\n",
      "Epoch: 11750/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11751/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11752/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11753/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11754/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11755/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11756/25000, Training Loss: 0.000008, lr : 0.004437\n",
      "Epoch: 11757/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11758/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11759/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11760/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11761/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11762/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11763/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11764/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11765/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11766/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11767/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11768/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11769/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11770/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11771/25000, Training Loss: 0.000012, lr : 0.004437\n",
      "Epoch: 11772/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11773/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11774/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11775/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11776/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11777/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11778/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11779/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11780/25000, Training Loss: 0.000015, lr : 0.004437\n",
      "Epoch: 11781/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11782/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11783/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11784/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11785/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11786/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11787/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11788/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11789/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11790/25000, Training Loss: 0.000013, lr : 0.004437\n",
      "Epoch: 11791/25000, Training Loss: 0.000009, lr : 0.004437\n",
      "Epoch: 11792/25000, Training Loss: 0.000016, lr : 0.004437\n",
      "Epoch: 11793/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11794/25000, Training Loss: 0.000011, lr : 0.004437\n",
      "Epoch: 11795/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11796/25000, Training Loss: 0.000042, lr : 0.004437\n",
      "Epoch: 11797/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11798/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11799/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11800/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11801/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11802/25000, Training Loss: 0.000037, lr : 0.004437\n",
      "Epoch: 11803/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11804/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11805/25000, Training Loss: 0.000040, lr : 0.004437\n",
      "Epoch: 11806/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11807/25000, Training Loss: 0.000020, lr : 0.004437\n",
      "Epoch: 11808/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11809/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11810/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11811/25000, Training Loss: 0.000035, lr : 0.004437\n",
      "Epoch: 11812/25000, Training Loss: 0.000069, lr : 0.004437\n",
      "Epoch: 11813/25000, Training Loss: 0.000053, lr : 0.004437\n",
      "Epoch: 11814/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11815/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11816/25000, Training Loss: 0.000043, lr : 0.004437\n",
      "Epoch: 11817/25000, Training Loss: 0.000029, lr : 0.004437\n",
      "Epoch: 11818/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11819/25000, Training Loss: 0.000026, lr : 0.004437\n",
      "Epoch: 11820/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11821/25000, Training Loss: 0.000021, lr : 0.004437\n",
      "Epoch: 11822/25000, Training Loss: 0.000024, lr : 0.004437\n",
      "Epoch: 11823/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11824/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11825/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11826/25000, Training Loss: 0.000022, lr : 0.004437\n",
      "Epoch: 11827/25000, Training Loss: 0.000027, lr : 0.004437\n",
      "Epoch: 11828/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11829/25000, Training Loss: 0.000028, lr : 0.004437\n",
      "Epoch: 11830/25000, Training Loss: 0.000034, lr : 0.004437\n",
      "Epoch: 11831/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11832/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11833/25000, Training Loss: 0.000025, lr : 0.004437\n",
      "Epoch: 11834/25000, Training Loss: 0.000018, lr : 0.004437\n",
      "Epoch: 11835/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11836/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11837/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11838/25000, Training Loss: 0.000014, lr : 0.004437\n",
      "Epoch: 11839/25000, Training Loss: 0.000010, lr : 0.004437\n",
      "Epoch: 11840/25000, Training Loss: 0.000015, lr : 0.003771\n",
      "Epoch: 11841/25000, Training Loss: 0.000012, lr : 0.003771\n",
      "Epoch: 11842/25000, Training Loss: 0.000017, lr : 0.003771\n",
      "Epoch: 11843/25000, Training Loss: 0.000013, lr : 0.003771\n",
      "Epoch: 11844/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11845/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11846/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11847/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11848/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 11849/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11850/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11851/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11852/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11853/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11854/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11855/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11856/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11857/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11858/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11859/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11860/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11861/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11862/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11863/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11864/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11865/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11866/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11867/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11868/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11869/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11870/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11871/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11872/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11873/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11874/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11875/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11876/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11877/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11878/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11879/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11880/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11881/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11882/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11883/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11884/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11885/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11886/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11887/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11888/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11889/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11890/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11891/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11892/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11893/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11894/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11895/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11896/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11897/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11898/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11899/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11900/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11901/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11902/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11903/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11904/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11905/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11906/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11907/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11908/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11909/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11910/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11911/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11912/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11913/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11914/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11915/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11916/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11917/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11918/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11919/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11920/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11921/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11922/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11923/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11924/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11925/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11926/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11927/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11928/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11929/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11930/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11931/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11932/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11933/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11934/25000, Training Loss: 0.000005, lr : 0.003771\n",
      "Epoch: 11935/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11936/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11937/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11938/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11939/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 11940/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11941/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11942/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11943/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11944/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11945/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11946/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11947/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11948/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11949/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11950/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11951/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11952/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11953/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 11954/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11955/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11956/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11957/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11958/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11959/25000, Training Loss: 0.000014, lr : 0.003771\n",
      "Epoch: 11960/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11961/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11962/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 11963/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11964/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11965/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11966/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11967/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11968/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11969/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11970/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11971/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11972/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11973/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11974/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11975/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11976/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11977/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11978/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11979/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11980/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11981/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11982/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11983/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11984/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11985/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11986/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11987/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 11988/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 11989/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11990/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11991/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11992/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11993/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11994/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11995/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 11996/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 11997/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11998/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 11999/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 12000/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 12001/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12002/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12003/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12004/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 12005/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 12006/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 12007/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 12008/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12009/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12010/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12011/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 12012/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12013/25000, Training Loss: 0.000012, lr : 0.003771\n",
      "Epoch: 12014/25000, Training Loss: 0.000017, lr : 0.003771\n",
      "Epoch: 12015/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 12016/25000, Training Loss: 0.000017, lr : 0.003771\n",
      "Epoch: 12017/25000, Training Loss: 0.000021, lr : 0.003771\n",
      "Epoch: 12018/25000, Training Loss: 0.000013, lr : 0.003771\n",
      "Epoch: 12019/25000, Training Loss: 0.000018, lr : 0.003771\n",
      "Epoch: 12020/25000, Training Loss: 0.000038, lr : 0.003771\n",
      "Epoch: 12021/25000, Training Loss: 0.000028, lr : 0.003771\n",
      "Epoch: 12022/25000, Training Loss: 0.000023, lr : 0.003771\n",
      "Epoch: 12023/25000, Training Loss: 0.000028, lr : 0.003771\n",
      "Epoch: 12024/25000, Training Loss: 0.000020, lr : 0.003771\n",
      "Epoch: 12025/25000, Training Loss: 0.000019, lr : 0.003771\n",
      "Epoch: 12026/25000, Training Loss: 0.000024, lr : 0.003771\n",
      "Epoch: 12027/25000, Training Loss: 0.000018, lr : 0.003771\n",
      "Epoch: 12028/25000, Training Loss: 0.000016, lr : 0.003771\n",
      "Epoch: 12029/25000, Training Loss: 0.000022, lr : 0.003771\n",
      "Epoch: 12030/25000, Training Loss: 0.000018, lr : 0.003771\n",
      "Epoch: 12031/25000, Training Loss: 0.000017, lr : 0.003771\n",
      "Epoch: 12032/25000, Training Loss: 0.000022, lr : 0.003771\n",
      "Epoch: 12033/25000, Training Loss: 0.000019, lr : 0.003771\n",
      "Epoch: 12034/25000, Training Loss: 0.000014, lr : 0.003771\n",
      "Epoch: 12035/25000, Training Loss: 0.000017, lr : 0.003771\n",
      "Epoch: 12036/25000, Training Loss: 0.000015, lr : 0.003771\n",
      "Epoch: 12037/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12038/25000, Training Loss: 0.000012, lr : 0.003771\n",
      "Epoch: 12039/25000, Training Loss: 0.000012, lr : 0.003771\n",
      "Epoch: 12040/25000, Training Loss: 0.000012, lr : 0.003771\n",
      "Epoch: 12041/25000, Training Loss: 0.000021, lr : 0.003771\n",
      "Epoch: 12042/25000, Training Loss: 0.000014, lr : 0.003771\n",
      "Epoch: 12043/25000, Training Loss: 0.000014, lr : 0.003771\n",
      "Epoch: 12044/25000, Training Loss: 0.000016, lr : 0.003771\n",
      "Epoch: 12045/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 12046/25000, Training Loss: 0.000016, lr : 0.003771\n",
      "Epoch: 12047/25000, Training Loss: 0.000014, lr : 0.003771\n",
      "Epoch: 12048/25000, Training Loss: 0.000013, lr : 0.003771\n",
      "Epoch: 12049/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12050/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 12051/25000, Training Loss: 0.000012, lr : 0.003771\n",
      "Epoch: 12052/25000, Training Loss: 0.000015, lr : 0.003771\n",
      "Epoch: 12053/25000, Training Loss: 0.000018, lr : 0.003771\n",
      "Epoch: 12054/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 12055/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12056/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12057/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12058/25000, Training Loss: 0.000015, lr : 0.003771\n",
      "Epoch: 12059/25000, Training Loss: 0.000023, lr : 0.003771\n",
      "Epoch: 12060/25000, Training Loss: 0.000022, lr : 0.003771\n",
      "Epoch: 12061/25000, Training Loss: 0.000013, lr : 0.003771\n",
      "Epoch: 12062/25000, Training Loss: 0.000014, lr : 0.003771\n",
      "Epoch: 12063/25000, Training Loss: 0.000023, lr : 0.003771\n",
      "Epoch: 12064/25000, Training Loss: 0.000016, lr : 0.003771\n",
      "Epoch: 12065/25000, Training Loss: 0.000013, lr : 0.003771\n",
      "Epoch: 12066/25000, Training Loss: 0.000015, lr : 0.003771\n",
      "Epoch: 12067/25000, Training Loss: 0.000012, lr : 0.003771\n",
      "Epoch: 12068/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12069/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12070/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12071/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 12072/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12073/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 12074/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 12075/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12076/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 12077/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 12078/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 12079/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12080/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 12081/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12082/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12083/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12084/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 12085/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12086/25000, Training Loss: 0.000013, lr : 0.003771\n",
      "Epoch: 12087/25000, Training Loss: 0.000016, lr : 0.003771\n",
      "Epoch: 12088/25000, Training Loss: 0.000018, lr : 0.003771\n",
      "Epoch: 12089/25000, Training Loss: 0.000020, lr : 0.003771\n",
      "Epoch: 12090/25000, Training Loss: 0.000015, lr : 0.003771\n",
      "Epoch: 12091/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12092/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 12093/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12094/25000, Training Loss: 0.000014, lr : 0.003771\n",
      "Epoch: 12095/25000, Training Loss: 0.000013, lr : 0.003771\n",
      "Epoch: 12096/25000, Training Loss: 0.000016, lr : 0.003771\n",
      "Epoch: 12097/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12098/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12099/25000, Training Loss: 0.000016, lr : 0.003771\n",
      "Epoch: 12100/25000, Training Loss: 0.000022, lr : 0.003771\n",
      "Epoch: 12101/25000, Training Loss: 0.000015, lr : 0.003771\n",
      "Epoch: 12102/25000, Training Loss: 0.000014, lr : 0.003771\n",
      "Epoch: 12103/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 12104/25000, Training Loss: 0.000013, lr : 0.003771\n",
      "Epoch: 12105/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 12106/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12107/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 12108/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12109/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12110/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 12111/25000, Training Loss: 0.000013, lr : 0.003771\n",
      "Epoch: 12112/25000, Training Loss: 0.000010, lr : 0.003771\n",
      "Epoch: 12113/25000, Training Loss: 0.000008, lr : 0.003771\n",
      "Epoch: 12114/25000, Training Loss: 0.000014, lr : 0.003771\n",
      "Epoch: 12115/25000, Training Loss: 0.000020, lr : 0.003771\n",
      "Epoch: 12116/25000, Training Loss: 0.000018, lr : 0.003771\n",
      "Epoch: 12117/25000, Training Loss: 0.000012, lr : 0.003771\n",
      "Epoch: 12118/25000, Training Loss: 0.000012, lr : 0.003771\n",
      "Epoch: 12119/25000, Training Loss: 0.000027, lr : 0.003771\n",
      "Epoch: 12120/25000, Training Loss: 0.000022, lr : 0.003771\n",
      "Epoch: 12121/25000, Training Loss: 0.000014, lr : 0.003771\n",
      "Epoch: 12122/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 12123/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 12124/25000, Training Loss: 0.000011, lr : 0.003771\n",
      "Epoch: 12125/25000, Training Loss: 0.000012, lr : 0.003771\n",
      "Epoch: 12126/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 12127/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 12128/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 12129/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 12130/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 12131/25000, Training Loss: 0.000006, lr : 0.003771\n",
      "Epoch: 12132/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12133/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12134/25000, Training Loss: 0.000007, lr : 0.003771\n",
      "Epoch: 12135/25000, Training Loss: 0.000009, lr : 0.003771\n",
      "Epoch: 12136/25000, Training Loss: 0.000014, lr : 0.003771\n",
      "Epoch: 12137/25000, Training Loss: 0.000021, lr : 0.003771\n",
      "Epoch: 12138/25000, Training Loss: 0.000026, lr : 0.003771\n",
      "Epoch: 12139/25000, Training Loss: 0.000026, lr : 0.003771\n",
      "Epoch: 12140/25000, Training Loss: 0.000015, lr : 0.003771\n",
      "Epoch: 12141/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12142/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12143/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12144/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12145/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12146/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12147/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12148/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12149/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12150/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12151/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12152/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12153/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12154/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12155/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12156/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12157/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12158/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12159/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12160/25000, Training Loss: 0.000005, lr : 0.003206\n",
      "Epoch: 12161/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12162/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12163/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12164/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12165/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12166/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12167/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12168/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12169/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12170/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12171/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12172/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12173/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12174/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12175/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12176/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12177/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12178/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12179/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12180/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12181/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12182/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12183/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12184/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12185/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12186/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12187/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12188/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12189/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12190/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12191/25000, Training Loss: 0.000005, lr : 0.003206\n",
      "Epoch: 12192/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12193/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12194/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12195/25000, Training Loss: 0.000005, lr : 0.003206\n",
      "Epoch: 12196/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12197/25000, Training Loss: 0.000005, lr : 0.003206\n",
      "Epoch: 12198/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12199/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12200/25000, Training Loss: 0.000005, lr : 0.003206\n",
      "Epoch: 12201/25000, Training Loss: 0.000005, lr : 0.003206\n",
      "Epoch: 12202/25000, Training Loss: 0.000005, lr : 0.003206\n",
      "Epoch: 12203/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12204/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12205/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12206/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12207/25000, Training Loss: 0.000005, lr : 0.003206\n",
      "Epoch: 12208/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12209/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12210/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12211/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12212/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12213/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12214/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12215/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12216/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12217/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12218/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12219/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12220/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12221/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12222/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12223/25000, Training Loss: 0.000020, lr : 0.003206\n",
      "Epoch: 12224/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12225/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12226/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12227/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12228/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12229/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12230/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12231/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12232/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12233/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12234/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12235/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12236/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12237/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12238/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12239/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12240/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12241/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12242/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12243/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12244/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12245/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12246/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12247/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12248/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12249/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12250/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12251/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12252/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12253/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12254/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12255/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12256/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12257/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12258/25000, Training Loss: 0.000023, lr : 0.003206\n",
      "Epoch: 12259/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12260/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12261/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12262/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12263/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12264/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12265/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12266/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12267/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12268/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12269/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12270/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12271/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12272/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12273/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12274/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12275/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12276/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12277/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12278/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12279/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12280/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12281/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12282/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12283/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12284/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12285/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12286/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12287/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12288/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12289/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12290/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12291/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12292/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12293/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12294/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12295/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12296/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12297/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12298/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12299/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12300/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12301/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12302/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12303/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12304/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12305/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12306/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12307/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12308/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12309/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12310/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12311/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12312/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12313/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12314/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12315/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12316/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12317/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12318/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12319/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12320/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12321/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12322/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12323/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12324/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12325/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12326/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12327/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12328/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12329/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12330/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12331/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12332/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12333/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12334/25000, Training Loss: 0.000026, lr : 0.003206\n",
      "Epoch: 12335/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12336/25000, Training Loss: 0.000023, lr : 0.003206\n",
      "Epoch: 12337/25000, Training Loss: 0.000026, lr : 0.003206\n",
      "Epoch: 12338/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12339/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12340/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12341/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12342/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12343/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12344/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12345/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12346/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12347/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12348/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12349/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12350/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12351/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12352/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12353/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12354/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12355/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12356/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12357/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12358/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12359/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12360/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12361/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12362/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12363/25000, Training Loss: 0.000025, lr : 0.003206\n",
      "Epoch: 12364/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12365/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12366/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12367/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12368/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12369/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12370/25000, Training Loss: 0.000027, lr : 0.003206\n",
      "Epoch: 12371/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12372/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12373/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12374/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12375/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12376/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12377/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12378/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12379/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12380/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12381/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12382/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12383/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12384/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12385/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12386/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12387/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12388/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12389/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12390/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12391/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12392/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12393/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12394/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12395/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12396/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12397/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12398/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12399/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12400/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12401/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12402/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12403/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12404/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12405/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12406/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12407/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12408/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12409/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12410/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12411/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12412/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12413/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12414/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12415/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12416/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12417/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12418/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12419/25000, Training Loss: 0.000027, lr : 0.003206\n",
      "Epoch: 12420/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12421/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12422/25000, Training Loss: 0.000023, lr : 0.003206\n",
      "Epoch: 12423/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12424/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12425/25000, Training Loss: 0.000037, lr : 0.003206\n",
      "Epoch: 12426/25000, Training Loss: 0.000043, lr : 0.003206\n",
      "Epoch: 12427/25000, Training Loss: 0.000020, lr : 0.003206\n",
      "Epoch: 12428/25000, Training Loss: 0.000023, lr : 0.003206\n",
      "Epoch: 12429/25000, Training Loss: 0.000030, lr : 0.003206\n",
      "Epoch: 12430/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12431/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12432/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12433/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12434/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12435/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12436/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12437/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12438/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12439/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12440/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12441/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12442/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12443/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12444/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12445/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12446/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12447/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12448/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12449/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12450/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12451/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12452/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12453/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12454/25000, Training Loss: 0.000022, lr : 0.003206\n",
      "Epoch: 12455/25000, Training Loss: 0.000026, lr : 0.003206\n",
      "Epoch: 12456/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12457/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12458/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12459/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12460/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12461/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12462/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12463/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12464/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12465/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12466/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12467/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12468/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12469/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12470/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12471/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12472/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12473/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12474/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12475/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12476/25000, Training Loss: 0.000025, lr : 0.003206\n",
      "Epoch: 12477/25000, Training Loss: 0.000024, lr : 0.003206\n",
      "Epoch: 12478/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12479/25000, Training Loss: 0.000027, lr : 0.003206\n",
      "Epoch: 12480/25000, Training Loss: 0.000024, lr : 0.003206\n",
      "Epoch: 12481/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12482/25000, Training Loss: 0.000022, lr : 0.003206\n",
      "Epoch: 12483/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12484/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12485/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12486/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12487/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12488/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12489/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12490/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12491/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12492/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12493/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12494/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12495/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12496/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12497/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12498/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12499/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12500/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12501/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12502/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12503/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12504/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12505/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12506/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12507/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12508/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12509/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12510/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12511/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12512/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12513/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12514/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12515/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12516/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12517/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12518/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12519/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12520/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12521/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12522/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12523/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12524/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12525/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12526/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12527/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12528/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12529/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12530/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12531/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12532/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12533/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12534/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12535/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12536/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12537/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12538/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12539/25000, Training Loss: 0.000028, lr : 0.003206\n",
      "Epoch: 12540/25000, Training Loss: 0.000032, lr : 0.003206\n",
      "Epoch: 12541/25000, Training Loss: 0.000021, lr : 0.003206\n",
      "Epoch: 12542/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12543/25000, Training Loss: 0.000021, lr : 0.003206\n",
      "Epoch: 12544/25000, Training Loss: 0.000028, lr : 0.003206\n",
      "Epoch: 12545/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12546/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12547/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12548/25000, Training Loss: 0.000021, lr : 0.003206\n",
      "Epoch: 12549/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12550/25000, Training Loss: 0.000023, lr : 0.003206\n",
      "Epoch: 12551/25000, Training Loss: 0.000028, lr : 0.003206\n",
      "Epoch: 12552/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12553/25000, Training Loss: 0.000026, lr : 0.003206\n",
      "Epoch: 12554/25000, Training Loss: 0.000020, lr : 0.003206\n",
      "Epoch: 12555/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12556/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12557/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12558/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12559/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12560/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12561/25000, Training Loss: 0.000022, lr : 0.003206\n",
      "Epoch: 12562/25000, Training Loss: 0.000027, lr : 0.003206\n",
      "Epoch: 12563/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12564/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12565/25000, Training Loss: 0.000020, lr : 0.003206\n",
      "Epoch: 12566/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12567/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12568/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12569/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12570/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12571/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12572/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12573/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12574/25000, Training Loss: 0.000031, lr : 0.003206\n",
      "Epoch: 12575/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12576/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12577/25000, Training Loss: 0.000024, lr : 0.003206\n",
      "Epoch: 12578/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12579/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12580/25000, Training Loss: 0.000020, lr : 0.003206\n",
      "Epoch: 12581/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12582/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12583/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12584/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12585/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12586/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12587/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12588/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12589/25000, Training Loss: 0.000020, lr : 0.003206\n",
      "Epoch: 12590/25000, Training Loss: 0.000022, lr : 0.003206\n",
      "Epoch: 12591/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12592/25000, Training Loss: 0.000027, lr : 0.003206\n",
      "Epoch: 12593/25000, Training Loss: 0.000034, lr : 0.003206\n",
      "Epoch: 12594/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12595/25000, Training Loss: 0.000023, lr : 0.003206\n",
      "Epoch: 12596/25000, Training Loss: 0.000035, lr : 0.003206\n",
      "Epoch: 12597/25000, Training Loss: 0.000028, lr : 0.003206\n",
      "Epoch: 12598/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12599/25000, Training Loss: 0.000035, lr : 0.003206\n",
      "Epoch: 12600/25000, Training Loss: 0.000020, lr : 0.003206\n",
      "Epoch: 12601/25000, Training Loss: 0.000024, lr : 0.003206\n",
      "Epoch: 12602/25000, Training Loss: 0.000052, lr : 0.003206\n",
      "Epoch: 12603/25000, Training Loss: 0.000022, lr : 0.003206\n",
      "Epoch: 12604/25000, Training Loss: 0.000033, lr : 0.003206\n",
      "Epoch: 12605/25000, Training Loss: 0.000034, lr : 0.003206\n",
      "Epoch: 12606/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12607/25000, Training Loss: 0.000028, lr : 0.003206\n",
      "Epoch: 12608/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12609/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12610/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12611/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12612/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12613/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12614/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12615/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12616/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12617/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12618/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12619/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12620/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12621/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12622/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12623/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12624/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12625/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12626/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12627/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12628/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12629/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12630/25000, Training Loss: 0.000021, lr : 0.003206\n",
      "Epoch: 12631/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12632/25000, Training Loss: 0.000021, lr : 0.003206\n",
      "Epoch: 12633/25000, Training Loss: 0.000022, lr : 0.003206\n",
      "Epoch: 12634/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12635/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12636/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12637/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12638/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12639/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12640/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12641/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12642/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12643/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12644/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12645/25000, Training Loss: 0.000028, lr : 0.003206\n",
      "Epoch: 12646/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12647/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12648/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12649/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12650/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12651/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12652/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12653/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12654/25000, Training Loss: 0.000022, lr : 0.003206\n",
      "Epoch: 12655/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12656/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12657/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12658/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12659/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12660/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12661/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12662/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12663/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12664/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12665/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12666/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12667/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12668/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12669/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12670/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12671/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12672/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12673/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12674/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12675/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12676/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12677/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12678/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12679/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12680/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12681/25000, Training Loss: 0.000024, lr : 0.003206\n",
      "Epoch: 12682/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12683/25000, Training Loss: 0.000022, lr : 0.003206\n",
      "Epoch: 12684/25000, Training Loss: 0.000038, lr : 0.003206\n",
      "Epoch: 12685/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12686/25000, Training Loss: 0.000025, lr : 0.003206\n",
      "Epoch: 12687/25000, Training Loss: 0.000023, lr : 0.003206\n",
      "Epoch: 12688/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12689/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12690/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12691/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12692/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12693/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12694/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12695/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12696/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12697/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12698/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12699/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12700/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12701/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12702/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12703/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12704/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12705/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12706/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12707/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12708/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12709/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12710/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12711/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12712/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12713/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12714/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12715/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12716/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12717/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12718/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12719/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12720/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12721/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12722/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12723/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12724/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12725/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12726/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12727/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12728/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12729/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12730/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12731/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12732/25000, Training Loss: 0.000005, lr : 0.003206\n",
      "Epoch: 12733/25000, Training Loss: 0.000005, lr : 0.003206\n",
      "Epoch: 12734/25000, Training Loss: 0.000005, lr : 0.003206\n",
      "Epoch: 12735/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12736/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12737/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12738/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12739/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12740/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12741/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12742/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12743/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12744/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12745/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12746/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12747/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12748/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12749/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12750/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12751/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12752/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12753/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12754/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12755/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12756/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12757/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12758/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12759/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12760/25000, Training Loss: 0.000029, lr : 0.003206\n",
      "Epoch: 12761/25000, Training Loss: 0.000037, lr : 0.003206\n",
      "Epoch: 12762/25000, Training Loss: 0.000021, lr : 0.003206\n",
      "Epoch: 12763/25000, Training Loss: 0.000024, lr : 0.003206\n",
      "Epoch: 12764/25000, Training Loss: 0.000032, lr : 0.003206\n",
      "Epoch: 12765/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12766/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12767/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12768/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12769/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12770/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12771/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12772/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12773/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12774/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12775/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12776/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12777/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12778/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12779/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12780/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12781/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12782/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12783/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12784/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12785/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12786/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12787/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12788/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12789/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12790/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12791/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12792/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12793/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12794/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12795/25000, Training Loss: 0.000020, lr : 0.003206\n",
      "Epoch: 12796/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12797/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12798/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12799/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12800/25000, Training Loss: 0.000025, lr : 0.003206\n",
      "Epoch: 12801/25000, Training Loss: 0.000021, lr : 0.003206\n",
      "Epoch: 12802/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12803/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12804/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12805/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12806/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12807/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12808/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12809/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12810/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12811/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12812/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12813/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12814/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12815/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12816/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12817/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12818/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12819/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12820/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12821/25000, Training Loss: 0.000006, lr : 0.003206\n",
      "Epoch: 12822/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12823/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12824/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12825/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12826/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12827/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12828/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12829/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12830/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12831/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12832/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12833/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12834/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12835/25000, Training Loss: 0.000007, lr : 0.003206\n",
      "Epoch: 12836/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12837/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12838/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12839/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12840/25000, Training Loss: 0.000020, lr : 0.003206\n",
      "Epoch: 12841/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12842/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12843/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12844/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12845/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12846/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12847/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12848/25000, Training Loss: 0.000021, lr : 0.003206\n",
      "Epoch: 12849/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12850/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12851/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12852/25000, Training Loss: 0.000023, lr : 0.003206\n",
      "Epoch: 12853/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12854/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12855/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12856/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12857/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12858/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12859/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12860/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12861/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12862/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12863/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12864/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12865/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12866/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12867/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12868/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12869/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12870/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12871/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12872/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12873/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12874/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12875/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12876/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12877/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12878/25000, Training Loss: 0.000021, lr : 0.003206\n",
      "Epoch: 12879/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12880/25000, Training Loss: 0.000020, lr : 0.003206\n",
      "Epoch: 12881/25000, Training Loss: 0.000060, lr : 0.003206\n",
      "Epoch: 12882/25000, Training Loss: 0.000065, lr : 0.003206\n",
      "Epoch: 12883/25000, Training Loss: 0.000036, lr : 0.003206\n",
      "Epoch: 12884/25000, Training Loss: 0.000022, lr : 0.003206\n",
      "Epoch: 12885/25000, Training Loss: 0.000031, lr : 0.003206\n",
      "Epoch: 12886/25000, Training Loss: 0.000024, lr : 0.003206\n",
      "Epoch: 12887/25000, Training Loss: 0.000036, lr : 0.003206\n",
      "Epoch: 12888/25000, Training Loss: 0.000048, lr : 0.003206\n",
      "Epoch: 12889/25000, Training Loss: 0.000035, lr : 0.003206\n",
      "Epoch: 12890/25000, Training Loss: 0.000041, lr : 0.003206\n",
      "Epoch: 12891/25000, Training Loss: 0.000037, lr : 0.003206\n",
      "Epoch: 12892/25000, Training Loss: 0.000025, lr : 0.003206\n",
      "Epoch: 12893/25000, Training Loss: 0.000020, lr : 0.003206\n",
      "Epoch: 12894/25000, Training Loss: 0.000020, lr : 0.003206\n",
      "Epoch: 12895/25000, Training Loss: 0.000017, lr : 0.003206\n",
      "Epoch: 12896/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12897/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12898/25000, Training Loss: 0.000022, lr : 0.003206\n",
      "Epoch: 12899/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12900/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12901/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12902/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12903/25000, Training Loss: 0.000014, lr : 0.003206\n",
      "Epoch: 12904/25000, Training Loss: 0.000030, lr : 0.003206\n",
      "Epoch: 12905/25000, Training Loss: 0.000035, lr : 0.003206\n",
      "Epoch: 12906/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12907/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12908/25000, Training Loss: 0.000021, lr : 0.003206\n",
      "Epoch: 12909/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12910/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12911/25000, Training Loss: 0.000027, lr : 0.003206\n",
      "Epoch: 12912/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12913/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12914/25000, Training Loss: 0.000019, lr : 0.003206\n",
      "Epoch: 12915/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12916/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12917/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12918/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12919/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12920/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12921/25000, Training Loss: 0.000016, lr : 0.003206\n",
      "Epoch: 12922/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12923/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12924/25000, Training Loss: 0.000033, lr : 0.003206\n",
      "Epoch: 12925/25000, Training Loss: 0.000024, lr : 0.003206\n",
      "Epoch: 12926/25000, Training Loss: 0.000018, lr : 0.003206\n",
      "Epoch: 12927/25000, Training Loss: 0.000015, lr : 0.003206\n",
      "Epoch: 12928/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12929/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12930/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12931/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12932/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12933/25000, Training Loss: 0.000010, lr : 0.003206\n",
      "Epoch: 12934/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12935/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12936/25000, Training Loss: 0.000009, lr : 0.003206\n",
      "Epoch: 12937/25000, Training Loss: 0.000008, lr : 0.003206\n",
      "Epoch: 12938/25000, Training Loss: 0.000011, lr : 0.003206\n",
      "Epoch: 12939/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12940/25000, Training Loss: 0.000012, lr : 0.003206\n",
      "Epoch: 12941/25000, Training Loss: 0.000013, lr : 0.003206\n",
      "Epoch: 12942/25000, Training Loss: 0.000014, lr : 0.002725\n",
      "Epoch: 12943/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 12944/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 12945/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 12946/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 12947/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 12948/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 12949/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 12950/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 12951/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 12952/25000, Training Loss: 0.000009, lr : 0.002725\n",
      "Epoch: 12953/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 12954/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 12955/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12956/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12957/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 12958/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12959/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12960/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12961/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12962/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 12963/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 12964/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 12965/25000, Training Loss: 0.000004, lr : 0.002725\n",
      "Epoch: 12966/25000, Training Loss: 0.000004, lr : 0.002725\n",
      "Epoch: 12967/25000, Training Loss: 0.000004, lr : 0.002725\n",
      "Epoch: 12968/25000, Training Loss: 0.000004, lr : 0.002725\n",
      "Epoch: 12969/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 12970/25000, Training Loss: 0.000004, lr : 0.002725\n",
      "Epoch: 12971/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 12972/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 12973/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 12974/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 12975/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12976/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12977/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12978/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 12979/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 12980/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12981/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12982/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12983/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12984/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12985/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12986/25000, Training Loss: 0.000004, lr : 0.002725\n",
      "Epoch: 12987/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 12988/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 12989/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12990/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 12991/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 12992/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 12993/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 12994/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 12995/25000, Training Loss: 0.000011, lr : 0.002725\n",
      "Epoch: 12996/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 12997/25000, Training Loss: 0.000009, lr : 0.002725\n",
      "Epoch: 12998/25000, Training Loss: 0.000012, lr : 0.002725\n",
      "Epoch: 12999/25000, Training Loss: 0.000012, lr : 0.002725\n",
      "Epoch: 13000/25000, Training Loss: 0.000011, lr : 0.002725\n",
      "Epoch: 13001/25000, Training Loss: 0.000009, lr : 0.002725\n",
      "Epoch: 13002/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 13003/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13004/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13005/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13006/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13007/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13008/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13009/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13010/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13011/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13012/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13013/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13014/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13015/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13016/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13017/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13018/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13019/25000, Training Loss: 0.000004, lr : 0.002725\n",
      "Epoch: 13020/25000, Training Loss: 0.000004, lr : 0.002725\n",
      "Epoch: 13021/25000, Training Loss: 0.000004, lr : 0.002725\n",
      "Epoch: 13022/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13023/25000, Training Loss: 0.000004, lr : 0.002725\n",
      "Epoch: 13024/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13025/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13026/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13027/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13028/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13029/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13030/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13031/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13032/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13033/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13034/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13035/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13036/25000, Training Loss: 0.000009, lr : 0.002725\n",
      "Epoch: 13037/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13038/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13039/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13040/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13041/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13042/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13043/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13044/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13045/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13046/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13047/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13048/25000, Training Loss: 0.000009, lr : 0.002725\n",
      "Epoch: 13049/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13050/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13051/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13052/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13053/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13054/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13055/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13056/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13057/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 13058/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13059/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13060/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13061/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13062/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13063/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13064/25000, Training Loss: 0.000011, lr : 0.002725\n",
      "Epoch: 13065/25000, Training Loss: 0.000009, lr : 0.002725\n",
      "Epoch: 13066/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 13067/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 13068/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13069/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13070/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13071/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13072/25000, Training Loss: 0.000009, lr : 0.002725\n",
      "Epoch: 13073/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13074/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13075/25000, Training Loss: 0.000009, lr : 0.002725\n",
      "Epoch: 13076/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13077/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13078/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13079/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13080/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13081/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13082/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13083/25000, Training Loss: 0.000004, lr : 0.002725\n",
      "Epoch: 13084/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13085/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13086/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13087/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13088/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13089/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13090/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13091/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13092/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13093/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13094/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13095/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13096/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13097/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13098/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13099/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13100/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13101/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13102/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13103/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13104/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13105/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13106/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13107/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13108/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13109/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13110/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13111/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13112/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13113/25000, Training Loss: 0.000004, lr : 0.002725\n",
      "Epoch: 13114/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13115/25000, Training Loss: 0.000004, lr : 0.002725\n",
      "Epoch: 13116/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13117/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13118/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13119/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13120/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13121/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13122/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13123/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13124/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13125/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13126/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13127/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13128/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13129/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13130/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13131/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13132/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 13133/25000, Training Loss: 0.000009, lr : 0.002725\n",
      "Epoch: 13134/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13135/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13136/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13137/25000, Training Loss: 0.000009, lr : 0.002725\n",
      "Epoch: 13138/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 13139/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13140/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13141/25000, Training Loss: 0.000012, lr : 0.002725\n",
      "Epoch: 13142/25000, Training Loss: 0.000011, lr : 0.002725\n",
      "Epoch: 13143/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13144/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13145/25000, Training Loss: 0.000009, lr : 0.002725\n",
      "Epoch: 13146/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13147/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13148/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13149/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13150/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13151/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13152/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13153/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13154/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13155/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13156/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13157/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13158/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13159/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13160/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13161/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13162/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13163/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13164/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13165/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13166/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13167/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13168/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13169/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13170/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13171/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13172/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13173/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13174/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13175/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13176/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13177/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13178/25000, Training Loss: 0.000005, lr : 0.002725\n",
      "Epoch: 13179/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13180/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13181/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13182/25000, Training Loss: 0.000006, lr : 0.002725\n",
      "Epoch: 13183/25000, Training Loss: 0.000012, lr : 0.002725\n",
      "Epoch: 13184/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 13185/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13186/25000, Training Loss: 0.000012, lr : 0.002725\n",
      "Epoch: 13187/25000, Training Loss: 0.000012, lr : 0.002725\n",
      "Epoch: 13188/25000, Training Loss: 0.000012, lr : 0.002725\n",
      "Epoch: 13189/25000, Training Loss: 0.000011, lr : 0.002725\n",
      "Epoch: 13190/25000, Training Loss: 0.000015, lr : 0.002725\n",
      "Epoch: 13191/25000, Training Loss: 0.000012, lr : 0.002725\n",
      "Epoch: 13192/25000, Training Loss: 0.000014, lr : 0.002725\n",
      "Epoch: 13193/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 13194/25000, Training Loss: 0.000012, lr : 0.002725\n",
      "Epoch: 13195/25000, Training Loss: 0.000026, lr : 0.002725\n",
      "Epoch: 13196/25000, Training Loss: 0.000018, lr : 0.002725\n",
      "Epoch: 13197/25000, Training Loss: 0.000013, lr : 0.002725\n",
      "Epoch: 13198/25000, Training Loss: 0.000012, lr : 0.002725\n",
      "Epoch: 13199/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 13200/25000, Training Loss: 0.000014, lr : 0.002725\n",
      "Epoch: 13201/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 13202/25000, Training Loss: 0.000014, lr : 0.002725\n",
      "Epoch: 13203/25000, Training Loss: 0.000011, lr : 0.002725\n",
      "Epoch: 13204/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 13205/25000, Training Loss: 0.000016, lr : 0.002725\n",
      "Epoch: 13206/25000, Training Loss: 0.000011, lr : 0.002725\n",
      "Epoch: 13207/25000, Training Loss: 0.000014, lr : 0.002725\n",
      "Epoch: 13208/25000, Training Loss: 0.000022, lr : 0.002725\n",
      "Epoch: 13209/25000, Training Loss: 0.000013, lr : 0.002725\n",
      "Epoch: 13210/25000, Training Loss: 0.000021, lr : 0.002725\n",
      "Epoch: 13211/25000, Training Loss: 0.000014, lr : 0.002725\n",
      "Epoch: 13212/25000, Training Loss: 0.000013, lr : 0.002725\n",
      "Epoch: 13213/25000, Training Loss: 0.000015, lr : 0.002725\n",
      "Epoch: 13214/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13215/25000, Training Loss: 0.000013, lr : 0.002725\n",
      "Epoch: 13216/25000, Training Loss: 0.000017, lr : 0.002725\n",
      "Epoch: 13217/25000, Training Loss: 0.000024, lr : 0.002725\n",
      "Epoch: 13218/25000, Training Loss: 0.000018, lr : 0.002725\n",
      "Epoch: 13219/25000, Training Loss: 0.000010, lr : 0.002725\n",
      "Epoch: 13220/25000, Training Loss: 0.000012, lr : 0.002725\n",
      "Epoch: 13221/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13222/25000, Training Loss: 0.000014, lr : 0.002725\n",
      "Epoch: 13223/25000, Training Loss: 0.000020, lr : 0.002725\n",
      "Epoch: 13224/25000, Training Loss: 0.000014, lr : 0.002725\n",
      "Epoch: 13225/25000, Training Loss: 0.000015, lr : 0.002725\n",
      "Epoch: 13226/25000, Training Loss: 0.000025, lr : 0.002725\n",
      "Epoch: 13227/25000, Training Loss: 0.000014, lr : 0.002725\n",
      "Epoch: 13228/25000, Training Loss: 0.000016, lr : 0.002725\n",
      "Epoch: 13229/25000, Training Loss: 0.000031, lr : 0.002725\n",
      "Epoch: 13230/25000, Training Loss: 0.000016, lr : 0.002725\n",
      "Epoch: 13231/25000, Training Loss: 0.000015, lr : 0.002725\n",
      "Epoch: 13232/25000, Training Loss: 0.000019, lr : 0.002725\n",
      "Epoch: 13233/25000, Training Loss: 0.000013, lr : 0.002725\n",
      "Epoch: 13234/25000, Training Loss: 0.000027, lr : 0.002725\n",
      "Epoch: 13235/25000, Training Loss: 0.000018, lr : 0.002725\n",
      "Epoch: 13236/25000, Training Loss: 0.000015, lr : 0.002725\n",
      "Epoch: 13237/25000, Training Loss: 0.000012, lr : 0.002725\n",
      "Epoch: 13238/25000, Training Loss: 0.000009, lr : 0.002725\n",
      "Epoch: 13239/25000, Training Loss: 0.000011, lr : 0.002725\n",
      "Epoch: 13240/25000, Training Loss: 0.000011, lr : 0.002725\n",
      "Epoch: 13241/25000, Training Loss: 0.000008, lr : 0.002725\n",
      "Epoch: 13242/25000, Training Loss: 0.000007, lr : 0.002725\n",
      "Epoch: 13243/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13244/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13245/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13246/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13247/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13248/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13249/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13250/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13251/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13252/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13253/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13254/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13255/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13256/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13257/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13258/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13259/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13260/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13261/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13262/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13263/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13264/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13265/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13266/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13267/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13268/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13269/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13270/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13271/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13272/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13273/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13274/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13275/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13276/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13277/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13278/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13279/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13280/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13281/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13282/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13283/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13284/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13285/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13286/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13287/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13288/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13289/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13290/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13291/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13292/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13293/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13294/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13295/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13296/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13297/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13298/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13299/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13300/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13301/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13302/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13303/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13304/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13305/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13306/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13307/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13308/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13309/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13310/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13311/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13312/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13313/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13314/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13315/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13316/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13317/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13318/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13319/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13320/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13321/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13322/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13323/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13324/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13325/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13326/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13327/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13328/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13329/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13330/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13331/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13332/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13333/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13334/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13335/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13336/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13337/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13338/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13339/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13340/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13341/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13342/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13343/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13344/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13345/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13346/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13347/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13348/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13349/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13350/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13351/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13352/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13353/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13354/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13355/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13356/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13357/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13358/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13359/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13360/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13361/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13362/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13363/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13364/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13365/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13366/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13367/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13368/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13369/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13370/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13371/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13372/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13373/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13374/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13375/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13376/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13377/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13378/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13379/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13380/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13381/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13382/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13383/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13384/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13385/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13386/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13387/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13388/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13389/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13390/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13391/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13392/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13393/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13394/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13395/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13396/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13397/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13398/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13399/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13400/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13401/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13402/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13403/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13404/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13405/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13406/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13407/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13408/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13409/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13410/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13411/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13412/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13413/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13414/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13415/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13416/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13417/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 13418/25000, Training Loss: 0.000018, lr : 0.002316\n",
      "Epoch: 13419/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13420/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13421/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13422/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13423/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13424/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13425/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13426/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13427/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13428/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13429/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13430/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13431/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13432/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13433/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13434/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13435/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13436/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13437/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13438/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13439/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13440/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13441/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13442/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13443/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13444/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13445/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13446/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13447/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13448/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13449/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13450/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13451/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13452/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13453/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13454/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13455/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13456/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13457/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13458/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13459/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13460/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13461/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13462/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13463/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13464/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13465/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13466/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13467/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13468/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13469/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13470/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13471/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13472/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13473/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13474/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13475/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13476/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13477/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13478/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13479/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13480/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13481/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13482/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13483/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13484/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13485/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13486/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13487/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13488/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13489/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13490/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13491/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13492/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13493/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13494/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13495/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13496/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13497/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13498/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13499/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13500/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13501/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13502/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13503/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13504/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13505/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13506/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13507/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13508/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13509/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13510/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13511/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13512/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13513/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13514/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13515/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13516/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13517/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13518/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13519/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13520/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13521/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13522/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13523/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13524/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13525/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13526/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13527/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13528/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13529/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13530/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13531/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13532/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13533/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13534/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13535/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13536/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13537/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13538/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13539/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13540/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13541/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13542/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13543/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13544/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13545/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13546/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13547/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13548/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13549/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 13550/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13551/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13552/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13553/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13554/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13555/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 13556/25000, Training Loss: 0.000020, lr : 0.002316\n",
      "Epoch: 13557/25000, Training Loss: 0.000016, lr : 0.002316\n",
      "Epoch: 13558/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13559/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 13560/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 13561/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13562/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13563/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13564/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13565/25000, Training Loss: 0.000016, lr : 0.002316\n",
      "Epoch: 13566/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13567/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13568/25000, Training Loss: 0.000016, lr : 0.002316\n",
      "Epoch: 13569/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13570/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13571/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13572/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13573/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13574/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13575/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13576/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 13577/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13578/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13579/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13580/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13581/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 13582/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13583/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13584/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13585/25000, Training Loss: 0.000016, lr : 0.002316\n",
      "Epoch: 13586/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13587/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 13588/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 13589/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13590/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13591/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13592/25000, Training Loss: 0.000017, lr : 0.002316\n",
      "Epoch: 13593/25000, Training Loss: 0.000018, lr : 0.002316\n",
      "Epoch: 13594/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13595/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13596/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 13597/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13598/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13599/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13600/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13601/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13602/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13603/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13604/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13605/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13606/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13607/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13608/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13609/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13610/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13611/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13612/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13613/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13614/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13615/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13616/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13617/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13618/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13619/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13620/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13621/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13622/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13623/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13624/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13625/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13626/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13627/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13628/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13629/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13630/25000, Training Loss: 0.000003, lr : 0.002316\n",
      "Epoch: 13631/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13632/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13633/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13634/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13635/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13636/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13637/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13638/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13639/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13640/25000, Training Loss: 0.000003, lr : 0.002316\n",
      "Epoch: 13641/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13642/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13643/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13644/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13645/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13646/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13647/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13648/25000, Training Loss: 0.000003, lr : 0.002316\n",
      "Epoch: 13649/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13650/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13651/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13652/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13653/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13654/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13655/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13656/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13657/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13658/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13659/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13660/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13661/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13662/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13663/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13664/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13665/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13666/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13667/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13668/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13669/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13670/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13671/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13672/25000, Training Loss: 0.000019, lr : 0.002316\n",
      "Epoch: 13673/25000, Training Loss: 0.000016, lr : 0.002316\n",
      "Epoch: 13674/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13675/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13676/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13677/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13678/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13679/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13680/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 13681/25000, Training Loss: 0.000023, lr : 0.002316\n",
      "Epoch: 13682/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 13683/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13684/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13685/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13686/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13687/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13688/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 13689/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13690/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13691/25000, Training Loss: 0.000019, lr : 0.002316\n",
      "Epoch: 13692/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 13693/25000, Training Loss: 0.000022, lr : 0.002316\n",
      "Epoch: 13694/25000, Training Loss: 0.000027, lr : 0.002316\n",
      "Epoch: 13695/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13696/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13697/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13698/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13699/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13700/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13701/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13702/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13703/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13704/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13705/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13706/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13707/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13708/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13709/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13710/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13711/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13712/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13713/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13714/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13715/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13716/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13717/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13718/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13719/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13720/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13721/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13722/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13723/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13724/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13725/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13726/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13727/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13728/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13729/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13730/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13731/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13732/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13733/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13734/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13735/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13736/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13737/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13738/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13739/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13740/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13741/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13742/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13743/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13744/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13745/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13746/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13747/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13748/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13749/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13750/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13751/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13752/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13753/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13754/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13755/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13756/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13757/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13758/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13759/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13760/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13761/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13762/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13763/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13764/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13765/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13766/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13767/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13768/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13769/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13770/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13771/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13772/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13773/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13774/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13775/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13776/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13777/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13778/25000, Training Loss: 0.000019, lr : 0.002316\n",
      "Epoch: 13779/25000, Training Loss: 0.000018, lr : 0.002316\n",
      "Epoch: 13780/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13781/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13782/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13783/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 13784/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13785/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13786/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13787/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13788/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13789/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13790/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13791/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13792/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13793/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 13794/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 13795/25000, Training Loss: 0.000027, lr : 0.002316\n",
      "Epoch: 13796/25000, Training Loss: 0.000028, lr : 0.002316\n",
      "Epoch: 13797/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 13798/25000, Training Loss: 0.000018, lr : 0.002316\n",
      "Epoch: 13799/25000, Training Loss: 0.000017, lr : 0.002316\n",
      "Epoch: 13800/25000, Training Loss: 0.000021, lr : 0.002316\n",
      "Epoch: 13801/25000, Training Loss: 0.000018, lr : 0.002316\n",
      "Epoch: 13802/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 13803/25000, Training Loss: 0.000025, lr : 0.002316\n",
      "Epoch: 13804/25000, Training Loss: 0.000021, lr : 0.002316\n",
      "Epoch: 13805/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13806/25000, Training Loss: 0.000020, lr : 0.002316\n",
      "Epoch: 13807/25000, Training Loss: 0.000020, lr : 0.002316\n",
      "Epoch: 13808/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13809/25000, Training Loss: 0.000019, lr : 0.002316\n",
      "Epoch: 13810/25000, Training Loss: 0.000020, lr : 0.002316\n",
      "Epoch: 13811/25000, Training Loss: 0.000018, lr : 0.002316\n",
      "Epoch: 13812/25000, Training Loss: 0.000024, lr : 0.002316\n",
      "Epoch: 13813/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 13814/25000, Training Loss: 0.000023, lr : 0.002316\n",
      "Epoch: 13815/25000, Training Loss: 0.000019, lr : 0.002316\n",
      "Epoch: 13816/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13817/25000, Training Loss: 0.000024, lr : 0.002316\n",
      "Epoch: 13818/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 13819/25000, Training Loss: 0.000018, lr : 0.002316\n",
      "Epoch: 13820/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13821/25000, Training Loss: 0.000016, lr : 0.002316\n",
      "Epoch: 13822/25000, Training Loss: 0.000018, lr : 0.002316\n",
      "Epoch: 13823/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13824/25000, Training Loss: 0.000018, lr : 0.002316\n",
      "Epoch: 13825/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13826/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13827/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13828/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13829/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13830/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13831/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13832/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13833/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13834/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13835/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13836/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13837/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13838/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13839/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13840/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13841/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13842/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13843/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13844/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13845/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13846/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13847/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13848/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13849/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13850/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13851/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13852/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13853/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13854/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13855/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13856/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13857/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13858/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13859/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13860/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13861/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13862/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13863/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13864/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13865/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13866/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13867/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13868/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13869/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13870/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13871/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13872/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13873/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13874/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13875/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13876/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13877/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13878/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13879/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13880/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13881/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13882/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13883/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13884/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13885/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13886/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13887/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13888/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13889/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13890/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13891/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13892/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13893/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13894/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13895/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13896/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13897/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13898/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13899/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13900/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13901/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13902/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13903/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13904/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13905/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13906/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13907/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13908/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13909/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13910/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13911/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13912/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 13913/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13914/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13915/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13916/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13917/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13918/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13919/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13920/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13921/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13922/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 13923/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13924/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13925/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13926/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13927/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13928/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13929/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13930/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13931/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13932/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13933/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13934/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13935/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13936/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13937/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13938/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13939/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13940/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13941/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13942/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13943/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13944/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 13945/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13946/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13947/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13948/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13949/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13950/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13951/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13952/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13953/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13954/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13955/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13956/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13957/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13958/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13959/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13960/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13961/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13962/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13963/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13964/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13965/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13966/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13967/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13968/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13969/25000, Training Loss: 0.000019, lr : 0.002316\n",
      "Epoch: 13970/25000, Training Loss: 0.000024, lr : 0.002316\n",
      "Epoch: 13971/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 13972/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13973/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13974/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13975/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13976/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 13977/25000, Training Loss: 0.000016, lr : 0.002316\n",
      "Epoch: 13978/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13979/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13980/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13981/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13982/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13983/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13984/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13985/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 13986/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13987/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13988/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13989/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13990/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 13991/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13992/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13993/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 13994/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13995/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 13996/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 13997/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 13998/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 13999/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14000/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 14001/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14002/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 14003/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 14004/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 14005/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 14006/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 14007/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 14008/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 14009/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 14010/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 14011/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 14012/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 14013/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 14014/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14015/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 14016/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14017/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 14018/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 14019/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 14020/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 14021/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 14022/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 14023/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14024/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 14025/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 14026/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 14027/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 14028/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14029/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 14030/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14031/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 14032/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14033/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 14034/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 14035/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14036/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14037/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 14038/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 14039/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 14040/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 14041/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 14042/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 14043/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 14044/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 14045/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 14046/25000, Training Loss: 0.000016, lr : 0.002316\n",
      "Epoch: 14047/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 14048/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14049/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 14050/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 14051/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 14052/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14053/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 14054/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 14055/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14056/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 14057/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 14058/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 14059/25000, Training Loss: 0.000018, lr : 0.002316\n",
      "Epoch: 14060/25000, Training Loss: 0.000019, lr : 0.002316\n",
      "Epoch: 14061/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 14062/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 14063/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 14064/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 14065/25000, Training Loss: 0.000028, lr : 0.002316\n",
      "Epoch: 14066/25000, Training Loss: 0.000015, lr : 0.002316\n",
      "Epoch: 14067/25000, Training Loss: 0.000019, lr : 0.002316\n",
      "Epoch: 14068/25000, Training Loss: 0.000018, lr : 0.002316\n",
      "Epoch: 14069/25000, Training Loss: 0.000017, lr : 0.002316\n",
      "Epoch: 14070/25000, Training Loss: 0.000017, lr : 0.002316\n",
      "Epoch: 14071/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 14072/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 14073/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 14074/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 14075/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 14076/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 14077/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 14078/25000, Training Loss: 0.000013, lr : 0.002316\n",
      "Epoch: 14079/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 14080/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 14081/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 14082/25000, Training Loss: 0.000011, lr : 0.002316\n",
      "Epoch: 14083/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 14084/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 14085/25000, Training Loss: 0.000010, lr : 0.002316\n",
      "Epoch: 14086/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 14087/25000, Training Loss: 0.000009, lr : 0.002316\n",
      "Epoch: 14088/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 14089/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 14090/25000, Training Loss: 0.000005, lr : 0.002316\n",
      "Epoch: 14091/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14092/25000, Training Loss: 0.000006, lr : 0.002316\n",
      "Epoch: 14093/25000, Training Loss: 0.000004, lr : 0.002316\n",
      "Epoch: 14094/25000, Training Loss: 0.000008, lr : 0.002316\n",
      "Epoch: 14095/25000, Training Loss: 0.000007, lr : 0.002316\n",
      "Epoch: 14096/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 14097/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 14098/25000, Training Loss: 0.000012, lr : 0.002316\n",
      "Epoch: 14099/25000, Training Loss: 0.000014, lr : 0.002316\n",
      "Epoch: 14100/25000, Training Loss: 0.000009, lr : 0.001969\n",
      "Epoch: 14101/25000, Training Loss: 0.000011, lr : 0.001969\n",
      "Epoch: 14102/25000, Training Loss: 0.000009, lr : 0.001969\n",
      "Epoch: 14103/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14104/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14105/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14106/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14107/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14108/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14109/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14110/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14111/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14112/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14113/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14114/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14115/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14116/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14117/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14118/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14119/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14120/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14121/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14122/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14123/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14124/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14125/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14126/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14127/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14128/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14129/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14130/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14131/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14132/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14133/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14134/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14135/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14136/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14137/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14138/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14139/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14140/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14141/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14142/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14143/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14144/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14145/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14146/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14147/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14148/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14149/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14150/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14151/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14152/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14153/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14154/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14155/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14156/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14157/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14158/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14159/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14160/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14161/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14162/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14163/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14164/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14165/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14166/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14167/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14168/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14169/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14170/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14171/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14172/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14173/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14174/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14175/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14176/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14177/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14178/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14179/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14180/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14181/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14182/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14183/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14184/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14185/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14186/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14187/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14188/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14189/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14190/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14191/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14192/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14193/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14194/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14195/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14196/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14197/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14198/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14199/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14200/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14201/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14202/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14203/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14204/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14205/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14206/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14207/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14208/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14209/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14210/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14211/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14212/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14213/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14214/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14215/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14216/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14217/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14218/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14219/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14220/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14221/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14222/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14223/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14224/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14225/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14226/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14227/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14228/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14229/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14230/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14231/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14232/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14233/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14234/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14235/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14236/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14237/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14238/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14239/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14240/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14241/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14242/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14243/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14244/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14245/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14246/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14247/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14248/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14249/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14250/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14251/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14252/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14253/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14254/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14255/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14256/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14257/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14258/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14259/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14260/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14261/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14262/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14263/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14264/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14265/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14266/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14267/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14268/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14269/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14270/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14271/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14272/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14273/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14274/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14275/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14276/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14277/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14278/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14279/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14280/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14281/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14282/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14283/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14284/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14285/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14286/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14287/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14288/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14289/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14290/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14291/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14292/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14293/25000, Training Loss: 0.000003, lr : 0.001969\n",
      "Epoch: 14294/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14295/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14296/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14297/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14298/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14299/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14300/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14301/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14302/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14303/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14304/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14305/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14306/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14307/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14308/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14309/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14310/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14311/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14312/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14313/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14314/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14315/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14316/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14317/25000, Training Loss: 0.000012, lr : 0.001969\n",
      "Epoch: 14318/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14319/25000, Training Loss: 0.000009, lr : 0.001969\n",
      "Epoch: 14320/25000, Training Loss: 0.000009, lr : 0.001969\n",
      "Epoch: 14321/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14322/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14323/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14324/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14325/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14326/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14327/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14328/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14329/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14330/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14331/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14332/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14333/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14334/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14335/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14336/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14337/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14338/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14339/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14340/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14341/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14342/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14343/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14344/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14345/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14346/25000, Training Loss: 0.000010, lr : 0.001969\n",
      "Epoch: 14347/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14348/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14349/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14350/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14351/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14352/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14353/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14354/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14355/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14356/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14357/25000, Training Loss: 0.000009, lr : 0.001969\n",
      "Epoch: 14358/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14359/25000, Training Loss: 0.000010, lr : 0.001969\n",
      "Epoch: 14360/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14361/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14362/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14363/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14364/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14365/25000, Training Loss: 0.000004, lr : 0.001969\n",
      "Epoch: 14366/25000, Training Loss: 0.000005, lr : 0.001969\n",
      "Epoch: 14367/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14368/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14369/25000, Training Loss: 0.000006, lr : 0.001969\n",
      "Epoch: 14370/25000, Training Loss: 0.000009, lr : 0.001969\n",
      "Epoch: 14371/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14372/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14373/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14374/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14375/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14376/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14377/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14378/25000, Training Loss: 0.000015, lr : 0.001969\n",
      "Epoch: 14379/25000, Training Loss: 0.000009, lr : 0.001969\n",
      "Epoch: 14380/25000, Training Loss: 0.000011, lr : 0.001969\n",
      "Epoch: 14381/25000, Training Loss: 0.000017, lr : 0.001969\n",
      "Epoch: 14382/25000, Training Loss: 0.000011, lr : 0.001969\n",
      "Epoch: 14383/25000, Training Loss: 0.000016, lr : 0.001969\n",
      "Epoch: 14384/25000, Training Loss: 0.000020, lr : 0.001969\n",
      "Epoch: 14385/25000, Training Loss: 0.000010, lr : 0.001969\n",
      "Epoch: 14386/25000, Training Loss: 0.000017, lr : 0.001969\n",
      "Epoch: 14387/25000, Training Loss: 0.000011, lr : 0.001969\n",
      "Epoch: 14388/25000, Training Loss: 0.000018, lr : 0.001969\n",
      "Epoch: 14389/25000, Training Loss: 0.000014, lr : 0.001969\n",
      "Epoch: 14390/25000, Training Loss: 0.000010, lr : 0.001969\n",
      "Epoch: 14391/25000, Training Loss: 0.000015, lr : 0.001969\n",
      "Epoch: 14392/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14393/25000, Training Loss: 0.000014, lr : 0.001969\n",
      "Epoch: 14394/25000, Training Loss: 0.000007, lr : 0.001969\n",
      "Epoch: 14395/25000, Training Loss: 0.000017, lr : 0.001969\n",
      "Epoch: 14396/25000, Training Loss: 0.000012, lr : 0.001969\n",
      "Epoch: 14397/25000, Training Loss: 0.000014, lr : 0.001969\n",
      "Epoch: 14398/25000, Training Loss: 0.000016, lr : 0.001969\n",
      "Epoch: 14399/25000, Training Loss: 0.000008, lr : 0.001969\n",
      "Epoch: 14400/25000, Training Loss: 0.000012, lr : 0.001969\n",
      "Epoch: 14401/25000, Training Loss: 0.000008, lr : 0.001673\n",
      "Epoch: 14402/25000, Training Loss: 0.000010, lr : 0.001673\n",
      "Epoch: 14403/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14404/25000, Training Loss: 0.000008, lr : 0.001673\n",
      "Epoch: 14405/25000, Training Loss: 0.000008, lr : 0.001673\n",
      "Epoch: 14406/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14407/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14408/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14409/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14410/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14411/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14412/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14413/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14414/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14415/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14416/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14417/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14418/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14419/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14420/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14421/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14422/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14423/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14424/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14425/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14426/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14427/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14428/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14429/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14430/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14431/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14432/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14433/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14434/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14435/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14436/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14437/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14438/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14439/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14440/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14441/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14442/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14443/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14444/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14445/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14446/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14447/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14448/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14449/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14450/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14451/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14452/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14453/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14454/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14455/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14456/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14457/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14458/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14459/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14460/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14461/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14462/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14463/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14464/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14465/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14466/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14467/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14468/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14469/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14470/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14471/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14472/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14473/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14474/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14475/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14476/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14477/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14478/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14479/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14480/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14481/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14482/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14483/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14484/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14485/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14486/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14487/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14488/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14489/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14490/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14491/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14492/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14493/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14494/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14495/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14496/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14497/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14498/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14499/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14500/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14501/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14502/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14503/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14504/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14505/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14506/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14507/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14508/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14509/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14510/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14511/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14512/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14513/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14514/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14515/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14516/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14517/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14518/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14519/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14520/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14521/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14522/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14523/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14524/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14525/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14526/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14527/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14528/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14529/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14530/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14531/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14532/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14533/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14534/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14535/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14536/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14537/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14538/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14539/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14540/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14541/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14542/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14543/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14544/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14545/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14546/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14547/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14548/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14549/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14550/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14551/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14552/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14553/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14554/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14555/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14556/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14557/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14558/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14559/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14560/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14561/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14562/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14563/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14564/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14565/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14566/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14567/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14568/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14569/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14570/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14571/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14572/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14573/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14574/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14575/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14576/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14577/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14578/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14579/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14580/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14581/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14582/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14583/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14584/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14585/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14586/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14587/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14588/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14589/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14590/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14591/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14592/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14593/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14594/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14595/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14596/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14597/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14598/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14599/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14600/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14601/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14602/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14603/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14604/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14605/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14606/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14607/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14608/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14609/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14610/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14611/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14612/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14613/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14614/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14615/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14616/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14617/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14618/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14619/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14620/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14621/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14622/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14623/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14624/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14625/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14626/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14627/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14628/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14629/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14630/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14631/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14632/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14633/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14634/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14635/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14636/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14637/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14638/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14639/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14640/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14641/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14642/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14643/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14644/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14645/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14646/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14647/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14648/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14649/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14650/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14651/25000, Training Loss: 0.000008, lr : 0.001673\n",
      "Epoch: 14652/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14653/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14654/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14655/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14656/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14657/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14658/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14659/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14660/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14661/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14662/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14663/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14664/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14665/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14666/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14667/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14668/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14669/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14670/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14671/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14672/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14673/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14674/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14675/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14676/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14677/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14678/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14679/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14680/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14681/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14682/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14683/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14684/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14685/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14686/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14687/25000, Training Loss: 0.000003, lr : 0.001673\n",
      "Epoch: 14688/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14689/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14690/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14691/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14692/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14693/25000, Training Loss: 0.000004, lr : 0.001673\n",
      "Epoch: 14694/25000, Training Loss: 0.000006, lr : 0.001673\n",
      "Epoch: 14695/25000, Training Loss: 0.000005, lr : 0.001673\n",
      "Epoch: 14696/25000, Training Loss: 0.000008, lr : 0.001673\n",
      "Epoch: 14697/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14698/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14699/25000, Training Loss: 0.000008, lr : 0.001673\n",
      "Epoch: 14700/25000, Training Loss: 0.000007, lr : 0.001673\n",
      "Epoch: 14701/25000, Training Loss: 0.000011, lr : 0.001673\n",
      "Epoch: 14702/25000, Training Loss: 0.000010, lr : 0.001422\n",
      "Epoch: 14703/25000, Training Loss: 0.000014, lr : 0.001422\n",
      "Epoch: 14704/25000, Training Loss: 0.000009, lr : 0.001422\n",
      "Epoch: 14705/25000, Training Loss: 0.000013, lr : 0.001422\n",
      "Epoch: 14706/25000, Training Loss: 0.000009, lr : 0.001422\n",
      "Epoch: 14707/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14708/25000, Training Loss: 0.000009, lr : 0.001422\n",
      "Epoch: 14709/25000, Training Loss: 0.000007, lr : 0.001422\n",
      "Epoch: 14710/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14711/25000, Training Loss: 0.000007, lr : 0.001422\n",
      "Epoch: 14712/25000, Training Loss: 0.000008, lr : 0.001422\n",
      "Epoch: 14713/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14714/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14715/25000, Training Loss: 0.000007, lr : 0.001422\n",
      "Epoch: 14716/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14717/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14718/25000, Training Loss: 0.000007, lr : 0.001422\n",
      "Epoch: 14719/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14720/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14721/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14722/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14723/25000, Training Loss: 0.000007, lr : 0.001422\n",
      "Epoch: 14724/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14725/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14726/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14727/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14728/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14729/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14730/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14731/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14732/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14733/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14734/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14735/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14736/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14737/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14738/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14739/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14740/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14741/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14742/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14743/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14744/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14745/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14746/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14747/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14748/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14749/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14750/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14751/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14752/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14753/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14754/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14755/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14756/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14757/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14758/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14759/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14760/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14761/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14762/25000, Training Loss: 0.000007, lr : 0.001422\n",
      "Epoch: 14763/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14764/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14765/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14766/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14767/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14768/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14769/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14770/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14771/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14772/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14773/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14774/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14775/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14776/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14777/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14778/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14779/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14780/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14781/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14782/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14783/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14784/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14785/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14786/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14787/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14788/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14789/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14790/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14791/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14792/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14793/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14794/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14795/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14796/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14797/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14798/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14799/25000, Training Loss: 0.000007, lr : 0.001422\n",
      "Epoch: 14800/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14801/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14802/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14803/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14804/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14805/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14806/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14807/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14808/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14809/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14810/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14811/25000, Training Loss: 0.000007, lr : 0.001422\n",
      "Epoch: 14812/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14813/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14814/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14815/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14816/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14817/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14818/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14819/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14820/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14821/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14822/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14823/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14824/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14825/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14826/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14827/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14828/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14829/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14830/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14831/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14832/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14833/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14834/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14835/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14836/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14837/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14838/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14839/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14840/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14841/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14842/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14843/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14844/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14845/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14846/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14847/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14848/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14849/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14850/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14851/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14852/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14853/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14854/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14855/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14856/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14857/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14858/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14859/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14860/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14861/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14862/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14863/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14864/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14865/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14866/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14867/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14868/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14869/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14870/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14871/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14872/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14873/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14874/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14875/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14876/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14877/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14878/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14879/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14880/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14881/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14882/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14883/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14884/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14885/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14886/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14887/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14888/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14889/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14890/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14891/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14892/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14893/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14894/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14895/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14896/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14897/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14898/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14899/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14900/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14901/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14902/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14903/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14904/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14905/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14906/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14907/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14908/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14909/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14910/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14911/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14912/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14913/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14914/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14915/25000, Training Loss: 0.000007, lr : 0.001422\n",
      "Epoch: 14916/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14917/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14918/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14919/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14920/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14921/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14922/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14923/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14924/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14925/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14926/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14927/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14928/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14929/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14930/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14931/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14932/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14933/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14934/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14935/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14936/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14937/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14938/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14939/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14940/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14941/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14942/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14943/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14944/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14945/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14946/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14947/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14948/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14949/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14950/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14951/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14952/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14953/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14954/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14955/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14956/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14957/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14958/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14959/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14960/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14961/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14962/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14963/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14964/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14965/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14966/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14967/25000, Training Loss: 0.000008, lr : 0.001422\n",
      "Epoch: 14968/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14969/25000, Training Loss: 0.000007, lr : 0.001422\n",
      "Epoch: 14970/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14971/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14972/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14973/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14974/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14975/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14976/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14977/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14978/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14979/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14980/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14981/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14982/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14983/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14984/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14985/25000, Training Loss: 0.000005, lr : 0.001422\n",
      "Epoch: 14986/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14987/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14988/25000, Training Loss: 0.000007, lr : 0.001422\n",
      "Epoch: 14989/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14990/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14991/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14992/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14993/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 14994/25000, Training Loss: 0.000006, lr : 0.001422\n",
      "Epoch: 14995/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14996/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14997/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14998/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 14999/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 15000/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 15001/25000, Training Loss: 0.000003, lr : 0.001422\n",
      "Epoch: 15002/25000, Training Loss: 0.000004, lr : 0.001422\n",
      "Epoch: 15003/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15004/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15005/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15006/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15007/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15008/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15009/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15010/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15011/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15012/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15013/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15014/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15015/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15016/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15017/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15018/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15019/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15020/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15021/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15022/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15023/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15024/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15025/25000, Training Loss: 0.000002, lr : 0.001209\n",
      "Epoch: 15026/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15027/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15028/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15029/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15030/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15031/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15032/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15033/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15034/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15035/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15036/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15037/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15038/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15039/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15040/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15041/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15042/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15043/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15044/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15045/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15046/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15047/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15048/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15049/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15050/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15051/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15052/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15053/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15054/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15055/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15056/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15057/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15058/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15059/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15060/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15061/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15062/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15063/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15064/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15065/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15066/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15067/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15068/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15069/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15070/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15071/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15072/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15073/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15074/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15075/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15076/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15077/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15078/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15079/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15080/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15081/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15082/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15083/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15084/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15085/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15086/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15087/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15088/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15089/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15090/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15091/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15092/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15093/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15094/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15095/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15096/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15097/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15098/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15099/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15100/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15101/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15102/25000, Training Loss: 0.000002, lr : 0.001209\n",
      "Epoch: 15103/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15104/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15105/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15106/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15107/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15108/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15109/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15110/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15111/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15112/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15113/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15114/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15115/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15116/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15117/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15118/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15119/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15120/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15121/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15122/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15123/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15124/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15125/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15126/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15127/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15128/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15129/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15130/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15131/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15132/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15133/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15134/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15135/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15136/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15137/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15138/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15139/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15140/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15141/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15142/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15143/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15144/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15145/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15146/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15147/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15148/25000, Training Loss: 0.000008, lr : 0.001209\n",
      "Epoch: 15149/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15150/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15151/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15152/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15153/25000, Training Loss: 0.000008, lr : 0.001209\n",
      "Epoch: 15154/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15155/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15156/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15157/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15158/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15159/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15160/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15161/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15162/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15163/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15164/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15165/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15166/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15167/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15168/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15169/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15170/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15171/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15172/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15173/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15174/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15175/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15176/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15177/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15178/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15179/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15180/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15181/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15182/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15183/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15184/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15185/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15186/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15187/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15188/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15189/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15190/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15191/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15192/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15193/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15194/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15195/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15196/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15197/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15198/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15199/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15200/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15201/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15202/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15203/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15204/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15205/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15206/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15207/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15208/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15209/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15210/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15211/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15212/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15213/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15214/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15215/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15216/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15217/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15218/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15219/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15220/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15221/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15222/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15223/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15224/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15225/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15226/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15227/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15228/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15229/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15230/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15231/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15232/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15233/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15234/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15235/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15236/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15237/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15238/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15239/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15240/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15241/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15242/25000, Training Loss: 0.000002, lr : 0.001209\n",
      "Epoch: 15243/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15244/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15245/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15246/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15247/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15248/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15249/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15250/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15251/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15252/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15253/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15254/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15255/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15256/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15257/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15258/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15259/25000, Training Loss: 0.000002, lr : 0.001209\n",
      "Epoch: 15260/25000, Training Loss: 0.000002, lr : 0.001209\n",
      "Epoch: 15261/25000, Training Loss: 0.000002, lr : 0.001209\n",
      "Epoch: 15262/25000, Training Loss: 0.000002, lr : 0.001209\n",
      "Epoch: 15263/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15264/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15265/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15266/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15267/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15268/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15269/25000, Training Loss: 0.000002, lr : 0.001209\n",
      "Epoch: 15270/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15271/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15272/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15273/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15274/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15275/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15276/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15277/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15278/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15279/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15280/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15281/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15282/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15283/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15284/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15285/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15286/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15287/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15288/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15289/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15290/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15291/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15292/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15293/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15294/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15295/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15296/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15297/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15298/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15299/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15300/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15301/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15302/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15303/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15304/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15305/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15306/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15307/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15308/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15309/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15310/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15311/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15312/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15313/25000, Training Loss: 0.000008, lr : 0.001209\n",
      "Epoch: 15314/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15315/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15316/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15317/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15318/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15319/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15320/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15321/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15322/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15323/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15324/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15325/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15326/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15327/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15328/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15329/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15330/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15331/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15332/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15333/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15334/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15335/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15336/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15337/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15338/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15339/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15340/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15341/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15342/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15343/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15344/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15345/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15346/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15347/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15348/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15349/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15350/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15351/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15352/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15353/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15354/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15355/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15356/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15357/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15358/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15359/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15360/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15361/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15362/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15363/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15364/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15365/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15366/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15367/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15368/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15369/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15370/25000, Training Loss: 0.000002, lr : 0.001209\n",
      "Epoch: 15371/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15372/25000, Training Loss: 0.000002, lr : 0.001209\n",
      "Epoch: 15373/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15374/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15375/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15376/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15377/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15378/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15379/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15380/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15381/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15382/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15383/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15384/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15385/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15386/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15387/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15388/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15389/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15390/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15391/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15392/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15393/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15394/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15395/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15396/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15397/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15398/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15399/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15400/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15401/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15402/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15403/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15404/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15405/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15406/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15407/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15408/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15409/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15410/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15411/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15412/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15413/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15414/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15415/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15416/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15417/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15418/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15419/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15420/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15421/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15422/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15423/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15424/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15425/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15426/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15427/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15428/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15429/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15430/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15431/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15432/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15433/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15434/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15435/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15436/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15437/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15438/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15439/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15440/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15441/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15442/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15443/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15444/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15445/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15446/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15447/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15448/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15449/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15450/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15451/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15452/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15453/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15454/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15455/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15456/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15457/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15458/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15459/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15460/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15461/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15462/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15463/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15464/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15465/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15466/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15467/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15468/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15469/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15470/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15471/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15472/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15473/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15474/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15475/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15476/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15477/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15478/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15479/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15480/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15481/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15482/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15483/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15484/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15485/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15486/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15487/25000, Training Loss: 0.000008, lr : 0.001209\n",
      "Epoch: 15488/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15489/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15490/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15491/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15492/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15493/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15494/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15495/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15496/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15497/25000, Training Loss: 0.000008, lr : 0.001209\n",
      "Epoch: 15498/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15499/25000, Training Loss: 0.000010, lr : 0.001209\n",
      "Epoch: 15500/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15501/25000, Training Loss: 0.000009, lr : 0.001209\n",
      "Epoch: 15502/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15503/25000, Training Loss: 0.000009, lr : 0.001209\n",
      "Epoch: 15504/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15505/25000, Training Loss: 0.000008, lr : 0.001209\n",
      "Epoch: 15506/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15507/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15508/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15509/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15510/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15511/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15512/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15513/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15514/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15515/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15516/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15517/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15518/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15519/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15520/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15521/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15522/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15523/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15524/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15525/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15526/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15527/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15528/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15529/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15530/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15531/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15532/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15533/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15534/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15535/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15536/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15537/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15538/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15539/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15540/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15541/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15542/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15543/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15544/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15545/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15546/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15547/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15548/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15549/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15550/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15551/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15552/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15553/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15554/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15555/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15556/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15557/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15558/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15559/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15560/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15561/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15562/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15563/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15564/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15565/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15566/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15567/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15568/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15569/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15570/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15571/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15572/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15573/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15574/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15575/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15576/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15577/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15578/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15579/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15580/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15581/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15582/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15583/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15584/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15585/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15586/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15587/25000, Training Loss: 0.000002, lr : 0.001209\n",
      "Epoch: 15588/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15589/25000, Training Loss: 0.000002, lr : 0.001209\n",
      "Epoch: 15590/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15591/25000, Training Loss: 0.000002, lr : 0.001209\n",
      "Epoch: 15592/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15593/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15594/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15595/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15596/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15597/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15598/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15599/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15600/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15601/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15602/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15603/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15604/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15605/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15606/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15607/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15608/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15609/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15610/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15611/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15612/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15613/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15614/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15615/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15616/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15617/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15618/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15619/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15620/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15621/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15622/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15623/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15624/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15625/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15626/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15627/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15628/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15629/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15630/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15631/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15632/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15633/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15634/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15635/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15636/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15637/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15638/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15639/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15640/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15641/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15642/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15643/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15644/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15645/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15646/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15647/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15648/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15649/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15650/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15651/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15652/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15653/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15654/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15655/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15656/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15657/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15658/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15659/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15660/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15661/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15662/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15663/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15664/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15665/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15666/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15667/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15668/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15669/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15670/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15671/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15672/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15673/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15674/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15675/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15676/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15677/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15678/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15679/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15680/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15681/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15682/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15683/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15684/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15685/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15686/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15687/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15688/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15689/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15690/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15691/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15692/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15693/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15694/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15695/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15696/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15697/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15698/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15699/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15700/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15701/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15702/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15703/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15704/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15705/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15706/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15707/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15708/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15709/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15710/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15711/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15712/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15713/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15714/25000, Training Loss: 0.000009, lr : 0.001209\n",
      "Epoch: 15715/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15716/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15717/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15718/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15719/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15720/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15721/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15722/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15723/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15724/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15725/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15726/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15727/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15728/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15729/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15730/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15731/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15732/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15733/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15734/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15735/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15736/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15737/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15738/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15739/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15740/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15741/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15742/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15743/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15744/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15745/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15746/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15747/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15748/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15749/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15750/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15751/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15752/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15753/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15754/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15755/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15756/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15757/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15758/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15759/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15760/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15761/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15762/25000, Training Loss: 0.000009, lr : 0.001209\n",
      "Epoch: 15763/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15764/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15765/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15766/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15767/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15768/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15769/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15770/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15771/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15772/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15773/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15774/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15775/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15776/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15777/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15778/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15779/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15780/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15781/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15782/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15783/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15784/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15785/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15786/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15787/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15788/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15789/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15790/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15791/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15792/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15793/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15794/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15795/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15796/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15797/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15798/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15799/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15800/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15801/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15802/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15803/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15804/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15805/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15806/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15807/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15808/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15809/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15810/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15811/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15812/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15813/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15814/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15815/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15816/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15817/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15818/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15819/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15820/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15821/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15822/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15823/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15824/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15825/25000, Training Loss: 0.000009, lr : 0.001209\n",
      "Epoch: 15826/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15827/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15828/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15829/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15830/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15831/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15832/25000, Training Loss: 0.000006, lr : 0.001209\n",
      "Epoch: 15833/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15834/25000, Training Loss: 0.000007, lr : 0.001209\n",
      "Epoch: 15835/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15836/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15837/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15838/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15839/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15840/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15841/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15842/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15843/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15844/25000, Training Loss: 0.000005, lr : 0.001209\n",
      "Epoch: 15845/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15846/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15847/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15848/25000, Training Loss: 0.000003, lr : 0.001209\n",
      "Epoch: 15849/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15850/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15851/25000, Training Loss: 0.000004, lr : 0.001209\n",
      "Epoch: 15852/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15853/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15854/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15855/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15856/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15857/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15858/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15859/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15860/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15861/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15862/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15863/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15864/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15865/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15866/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15867/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15868/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15869/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15870/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15871/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15872/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15873/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15874/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15875/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 15876/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 15877/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15878/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15879/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15880/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15881/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15882/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15883/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15884/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15885/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15886/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15887/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15888/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15889/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15890/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15891/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15892/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15893/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15894/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15895/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15896/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15897/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15898/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15899/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15900/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15901/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15902/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 15903/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15904/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15905/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15906/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15907/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15908/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15909/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15910/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15911/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15912/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 15913/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15914/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15915/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15916/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15917/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15918/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 15919/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15920/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15921/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15922/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15923/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15924/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15925/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15926/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15927/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15928/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15929/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15930/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15931/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15932/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15933/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 15934/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15935/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15936/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15937/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15938/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15939/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15940/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15941/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15942/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15943/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15944/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15945/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15946/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15947/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15948/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15949/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15950/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 15951/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 15952/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15953/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15954/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15955/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15956/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15957/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15958/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15959/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15960/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 15961/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15962/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 15963/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15964/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15965/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15966/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15967/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15968/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15969/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15970/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15971/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15972/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15973/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15974/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15975/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15976/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15977/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15978/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15979/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 15980/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15981/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15982/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15983/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15984/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15985/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15986/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15987/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15988/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15989/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15990/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15991/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15992/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15993/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15994/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15995/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15996/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15997/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 15998/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 15999/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16000/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16001/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16002/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16003/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16004/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16005/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16006/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16007/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16008/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16009/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16010/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16011/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16012/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16013/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16014/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16015/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16016/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16017/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16018/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16019/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16020/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16021/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16022/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16023/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16024/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16025/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16026/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16027/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16028/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16029/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16030/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16031/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16032/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16033/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16034/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16035/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16036/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16037/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16038/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16039/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16040/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16041/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16042/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16043/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16044/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16045/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16046/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16047/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16048/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16049/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16050/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16051/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16052/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16053/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16054/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16055/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16056/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16057/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16058/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16059/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16060/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16061/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16062/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16063/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16064/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16065/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16066/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16067/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16068/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16069/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16070/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16071/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16072/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16073/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16074/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16075/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16076/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16077/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16078/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16079/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16080/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16081/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16082/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16083/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16084/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16085/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16086/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16087/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16088/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16089/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16090/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16091/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16092/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16093/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16094/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16095/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16096/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16097/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16098/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16099/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16100/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16101/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16102/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16103/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16104/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16105/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16106/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16107/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16108/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16109/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16110/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16111/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16112/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16113/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16114/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16115/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16116/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16117/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16118/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16119/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16120/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16121/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16122/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16123/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16124/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16125/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16126/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16127/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16128/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16129/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16130/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16131/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16132/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16133/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16134/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16135/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16136/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16137/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16138/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16139/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16140/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16141/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16142/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16143/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16144/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16145/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16146/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16147/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16148/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16149/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16150/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16151/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16152/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16153/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16154/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16155/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16156/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16157/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16158/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16159/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16160/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16161/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16162/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16163/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16164/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16165/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16166/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16167/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16168/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16169/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16170/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16171/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16172/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16173/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16174/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16175/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16176/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16177/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16178/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16179/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16180/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16181/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16182/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16183/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16184/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16185/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16186/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16187/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16188/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16189/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16190/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16191/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16192/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16193/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16194/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16195/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16196/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16197/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16198/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16199/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16200/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16201/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16202/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16203/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16204/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16205/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16206/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16207/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16208/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16209/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16210/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16211/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16212/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16213/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16214/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16215/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16216/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16217/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16218/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16219/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16220/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16221/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16222/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16223/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16224/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16225/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16226/25000, Training Loss: 0.000006, lr : 0.001028\n",
      "Epoch: 16227/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16228/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16229/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16230/25000, Training Loss: 0.000006, lr : 0.001028\n",
      "Epoch: 16231/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16232/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16233/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16234/25000, Training Loss: 0.000006, lr : 0.001028\n",
      "Epoch: 16235/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16236/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16237/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16238/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16239/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16240/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16241/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16242/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16243/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16244/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16245/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16246/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16247/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16248/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16249/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16250/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16251/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16252/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16253/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16254/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16255/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16256/25000, Training Loss: 0.000006, lr : 0.001028\n",
      "Epoch: 16257/25000, Training Loss: 0.000009, lr : 0.001028\n",
      "Epoch: 16258/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16259/25000, Training Loss: 0.000008, lr : 0.001028\n",
      "Epoch: 16260/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16261/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16262/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16263/25000, Training Loss: 0.000008, lr : 0.001028\n",
      "Epoch: 16264/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16265/25000, Training Loss: 0.000006, lr : 0.001028\n",
      "Epoch: 16266/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16267/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16268/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16269/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16270/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16271/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16272/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16273/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16274/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16275/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16276/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16277/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16278/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16279/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16280/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16281/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16282/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16283/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16284/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16285/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16286/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16287/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16288/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16289/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16290/25000, Training Loss: 0.000006, lr : 0.001028\n",
      "Epoch: 16291/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16292/25000, Training Loss: 0.000006, lr : 0.001028\n",
      "Epoch: 16293/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16294/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16295/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16296/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16297/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16298/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16299/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16300/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16301/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16302/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16303/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16304/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16305/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16306/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16307/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16308/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16309/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16310/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16311/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16312/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16313/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16314/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16315/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16316/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16317/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16318/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16319/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16320/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16321/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16322/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16323/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16324/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16325/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16326/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16327/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16328/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16329/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16330/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16331/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16332/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16333/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16334/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16335/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16336/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16337/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16338/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16339/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16340/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16341/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16342/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16343/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16344/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16345/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16346/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16347/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16348/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16349/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16350/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16351/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16352/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16353/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16354/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16355/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16356/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16357/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16358/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16359/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16360/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16361/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16362/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16363/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16364/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16365/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16366/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16367/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16368/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16369/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16370/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16371/25000, Training Loss: 0.000002, lr : 0.001028\n",
      "Epoch: 16372/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16373/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16374/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16375/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16376/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16377/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16378/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16379/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16380/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16381/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16382/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16383/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16384/25000, Training Loss: 0.000005, lr : 0.001028\n",
      "Epoch: 16385/25000, Training Loss: 0.000004, lr : 0.001028\n",
      "Epoch: 16386/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16387/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16388/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16389/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16390/25000, Training Loss: 0.000003, lr : 0.001028\n",
      "Epoch: 16391/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16392/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16393/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16394/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16395/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16396/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16397/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16398/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16399/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16400/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16401/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16402/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16403/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16404/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16405/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16406/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16407/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16408/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 16409/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16410/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16411/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16412/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16413/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16414/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16415/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16416/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16417/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16418/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16419/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16420/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16421/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16422/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16423/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16424/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16425/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16426/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16427/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16428/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16429/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16430/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16431/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16432/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16433/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16434/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16435/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16436/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16437/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16438/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16439/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16440/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16441/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16442/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16443/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16444/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16445/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16446/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16447/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16448/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16449/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16450/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16451/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16452/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16453/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16454/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16455/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16456/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16457/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16458/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16459/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16460/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16461/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16462/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16463/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16464/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16465/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16466/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16467/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16468/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16469/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16470/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16471/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16472/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16473/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16474/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16475/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16476/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16477/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16478/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16479/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16480/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16481/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16482/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16483/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16484/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16485/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16486/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16487/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16488/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16489/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16490/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16491/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16492/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16493/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16494/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16495/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16496/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16497/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16498/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16499/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16500/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16501/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16502/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16503/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16504/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16505/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16506/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16507/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16508/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16509/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16510/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16511/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16512/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16513/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16514/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16515/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16516/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16517/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16518/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16519/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16520/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16521/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16522/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16523/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16524/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16525/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16526/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16527/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16528/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16529/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16530/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16531/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16532/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16533/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16534/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16535/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16536/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16537/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16538/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16539/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16540/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16541/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16542/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16543/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16544/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16545/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16546/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16547/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16548/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16549/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16550/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16551/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16552/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16553/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16554/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16555/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16556/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16557/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16558/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16559/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16560/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16561/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16562/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16563/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16564/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16565/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16566/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16567/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16568/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16569/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16570/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16571/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16572/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16573/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16574/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16575/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16576/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16577/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16578/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16579/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16580/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16581/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16582/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16583/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16584/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16585/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16586/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16587/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16588/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16589/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16590/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16591/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16592/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16593/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16594/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16595/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16596/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16597/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16598/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16599/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16600/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16601/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16602/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16603/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16604/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16605/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16606/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16607/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16608/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16609/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16610/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16611/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16612/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16613/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16614/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16615/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16616/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16617/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16618/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16619/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16620/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16621/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16622/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16623/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16624/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16625/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16626/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16627/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16628/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16629/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16630/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16631/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16632/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16633/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16634/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16635/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16636/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16637/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 16638/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16639/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16640/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16641/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16642/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16643/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16644/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16645/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16646/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16647/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16648/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16649/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16650/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16651/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16652/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16653/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16654/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16655/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16656/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16657/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16658/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16659/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16660/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16661/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16662/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16663/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16664/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16665/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16666/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16667/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16668/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16669/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16670/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16671/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16672/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16673/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16674/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16675/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16676/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16677/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16678/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16679/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16680/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16681/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16682/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16683/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16684/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16685/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16686/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16687/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16688/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16689/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16690/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16691/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 16692/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16693/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16694/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16695/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16696/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16697/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16698/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16699/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16700/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16701/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16702/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16703/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16704/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16705/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16706/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16707/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16708/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 16709/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16710/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16711/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16712/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16713/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16714/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16715/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16716/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16717/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16718/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16719/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16720/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16721/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16722/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16723/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16724/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16725/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16726/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16727/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16728/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16729/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16730/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16731/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16732/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16733/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16734/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16735/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16736/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16737/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16738/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16739/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16740/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16741/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16742/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16743/25000, Training Loss: 0.000006, lr : 0.000874\n",
      "Epoch: 16744/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16745/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16746/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16747/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16748/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16749/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16750/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16751/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16752/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16753/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16754/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16755/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16756/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16757/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16758/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16759/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16760/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16761/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16762/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16763/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16764/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16765/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16766/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16767/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16768/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16769/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16770/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16771/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16772/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16773/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16774/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16775/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16776/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16777/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16778/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16779/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16780/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16781/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16782/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16783/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16784/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16785/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16786/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16787/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16788/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16789/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16790/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16791/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16792/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16793/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16794/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 16795/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16796/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16797/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16798/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16799/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16800/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16801/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16802/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 16803/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16804/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16805/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16806/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16807/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16808/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16809/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16810/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16811/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16812/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16813/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16814/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16815/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16816/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 16817/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16818/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16819/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16820/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 16821/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16822/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16823/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16824/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16825/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16826/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16827/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16828/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16829/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16830/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16831/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16832/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16833/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16834/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16835/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16836/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16837/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16838/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16839/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16840/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16841/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16842/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16843/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16844/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16845/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16846/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16847/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16848/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16849/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16850/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16851/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16852/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16853/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16854/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16855/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16856/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16857/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16858/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16859/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16860/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16861/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16862/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16863/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16864/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16865/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16866/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16867/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16868/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16869/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16870/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16871/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16872/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16873/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16874/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16875/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16876/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16877/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16878/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16879/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16880/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16881/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16882/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16883/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16884/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16885/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16886/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16887/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16888/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16889/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16890/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16891/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16892/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16893/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16894/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16895/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16896/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16897/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16898/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16899/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16900/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16901/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16902/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16903/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16904/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16905/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16906/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16907/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16908/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16909/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16910/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16911/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16912/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16913/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16914/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16915/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16916/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16917/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16918/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16919/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16920/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16921/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16922/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16923/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16924/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16925/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16926/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16927/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16928/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 16929/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16930/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16931/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16932/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16933/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16934/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16935/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16936/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16937/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16938/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16939/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16940/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16941/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16942/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16943/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16944/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16945/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16946/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16947/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16948/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16949/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16950/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16951/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16952/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16953/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16954/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16955/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16956/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16957/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16958/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16959/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16960/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16961/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16962/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16963/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16964/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16965/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16966/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16967/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16968/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16969/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16970/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16971/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16972/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16973/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16974/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16975/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16976/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16977/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16978/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16979/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16980/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16981/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16982/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16983/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16984/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16985/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16986/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16987/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16988/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16989/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 16990/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16991/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16992/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16993/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16994/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 16995/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16996/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16997/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16998/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 16999/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17000/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17001/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17002/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17003/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17004/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17005/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17006/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17007/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17008/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17009/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17010/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17011/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17012/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17013/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17014/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17015/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17016/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17017/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17018/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17019/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17020/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17021/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17022/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17023/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17024/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17025/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17026/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17027/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17028/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17029/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17030/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17031/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17032/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17033/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 17034/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17035/25000, Training Loss: 0.000006, lr : 0.000874\n",
      "Epoch: 17036/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17037/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17038/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17039/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17040/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17041/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17042/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17043/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17044/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17045/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17046/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17047/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17048/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17049/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17050/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17051/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17052/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17053/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17054/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17055/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17056/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17057/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17058/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17059/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17060/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17061/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17062/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17063/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17064/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17065/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17066/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17067/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17068/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17069/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17070/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17071/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17072/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17073/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17074/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17075/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17076/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17077/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17078/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17079/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17080/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17081/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17082/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 17083/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17084/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 17085/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17086/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17087/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17088/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17089/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17090/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17091/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17092/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17093/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17094/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17095/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17096/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17097/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17098/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17099/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17100/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17101/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17102/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17103/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17104/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17105/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17106/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17107/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17108/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17109/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17110/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17111/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17112/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17113/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17114/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17115/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17116/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17117/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17118/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17119/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17120/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17121/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17122/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17123/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17124/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17125/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17126/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17127/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17128/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17129/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17130/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17131/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17132/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17133/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17134/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17135/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17136/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17137/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17138/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17139/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17140/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17141/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17142/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17143/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17144/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17145/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17146/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17147/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17148/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17149/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17150/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17151/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17152/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17153/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17154/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17155/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17156/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17157/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17158/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17159/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17160/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17161/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17162/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17163/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17164/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17165/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17166/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17167/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17168/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17169/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17170/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17171/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17172/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17173/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17174/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17175/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17176/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17177/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17178/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17179/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17180/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17181/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17182/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17183/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17184/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17185/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17186/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17187/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17188/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17189/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17190/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17191/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17192/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17193/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17194/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17195/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17196/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 17197/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17198/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17199/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17200/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17201/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17202/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17203/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17204/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17205/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17206/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17207/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17208/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17209/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17210/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17211/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17212/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17213/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17214/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17215/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17216/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17217/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17218/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17219/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17220/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17221/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17222/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17223/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17224/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17225/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17226/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17227/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17228/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17229/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17230/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17231/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17232/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17233/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17234/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17235/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17236/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17237/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17238/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17239/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17240/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17241/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17242/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17243/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17244/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17245/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17246/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17247/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17248/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17249/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17250/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17251/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17252/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17253/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17254/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17255/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17256/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17257/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17258/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17259/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17260/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17261/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17262/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17263/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17264/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17265/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17266/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17267/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17268/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17269/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17270/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17271/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17272/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17273/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17274/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17275/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17276/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17277/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17278/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17279/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17280/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17281/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17282/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17283/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17284/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17285/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17286/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17287/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17288/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17289/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17290/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17291/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17292/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17293/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17294/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17295/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17296/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17297/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17298/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17299/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17300/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17301/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17302/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17303/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17304/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17305/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17306/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17307/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17308/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17309/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17310/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17311/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17312/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17313/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17314/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17315/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17316/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17317/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17318/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17319/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17320/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17321/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17322/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17323/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17324/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17325/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17326/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17327/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17328/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17329/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17330/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17331/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17332/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17333/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17334/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17335/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17336/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17337/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17338/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17339/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17340/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17341/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17342/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17343/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17344/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17345/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17346/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17347/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17348/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17349/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17350/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17351/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17352/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17353/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17354/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17355/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17356/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17357/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17358/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17359/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17360/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17361/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17362/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17363/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17364/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17365/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17366/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17367/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17368/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17369/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17370/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17371/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17372/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17373/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17374/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17375/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17376/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17377/25000, Training Loss: 0.000007, lr : 0.000874\n",
      "Epoch: 17378/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17379/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17380/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17381/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17382/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17383/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17384/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17385/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17386/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17387/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17388/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17389/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17390/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17391/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17392/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17393/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17394/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17395/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17396/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17397/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17398/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17399/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17400/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17401/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17402/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17403/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17404/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17405/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17406/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17407/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17408/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17409/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 17410/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17411/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 17412/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17413/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17414/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17415/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17416/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17417/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17418/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 17419/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17420/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17421/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17422/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17423/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17424/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17425/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17426/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17427/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17428/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 17429/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17430/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 17431/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17432/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17433/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17434/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17435/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17436/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17437/25000, Training Loss: 0.000005, lr : 0.000874\n",
      "Epoch: 17438/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17439/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17440/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17441/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17442/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17443/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17444/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17445/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17446/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17447/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17448/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17449/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17450/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17451/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17452/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17453/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17454/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17455/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17456/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17457/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17458/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17459/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17460/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17461/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17462/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17463/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17464/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17465/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17466/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17467/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17468/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17469/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17470/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17471/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17472/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17473/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17474/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17475/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17476/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17477/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17478/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17479/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17480/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17481/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17482/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17483/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17484/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17485/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17486/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17487/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17488/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17489/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17490/25000, Training Loss: 0.000004, lr : 0.000874\n",
      "Epoch: 17491/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17492/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17493/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17494/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17495/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17496/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17497/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17498/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17499/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17500/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17501/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17502/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17503/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17504/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17505/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17506/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17507/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17508/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17509/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17510/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17511/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17512/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17513/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17514/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17515/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17516/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17517/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17518/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17519/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17520/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17521/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17522/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17523/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17524/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17525/25000, Training Loss: 0.000003, lr : 0.000874\n",
      "Epoch: 17526/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17527/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17528/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17529/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17530/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17531/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17532/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17533/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17534/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17535/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17536/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17537/25000, Training Loss: 0.000002, lr : 0.000874\n",
      "Epoch: 17538/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17539/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17540/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17541/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17542/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17543/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17544/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17545/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17546/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17547/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17548/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17549/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17550/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17551/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17552/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17553/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17554/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17555/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17556/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17557/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17558/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17559/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17560/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17561/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17562/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17563/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17564/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17565/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17566/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17567/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17568/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17569/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17570/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17571/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17572/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17573/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17574/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17575/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17576/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17577/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17578/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17579/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17580/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17581/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17582/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17583/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17584/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17585/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17586/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17587/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17588/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17589/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17590/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17591/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17592/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17593/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17594/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17595/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17596/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17597/25000, Training Loss: 0.000005, lr : 0.000743\n",
      "Epoch: 17598/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17599/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17600/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17601/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17602/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17603/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17604/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17605/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17606/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17607/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17608/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17609/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17610/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17611/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17612/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17613/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17614/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17615/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17616/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17617/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17618/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17619/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17620/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17621/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17622/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17623/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17624/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17625/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17626/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17627/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17628/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17629/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17630/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17631/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17632/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17633/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17634/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17635/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17636/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17637/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17638/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17639/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17640/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17641/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17642/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17643/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17644/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17645/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17646/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17647/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17648/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17649/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17650/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17651/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17652/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17653/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17654/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17655/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17656/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17657/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17658/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17659/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17660/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17661/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17662/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17663/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17664/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17665/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17666/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17667/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17668/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17669/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17670/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17671/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17672/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17673/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17674/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17675/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17676/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17677/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17678/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17679/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17680/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17681/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17682/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17683/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17684/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17685/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17686/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17687/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17688/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17689/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17690/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17691/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17692/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17693/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17694/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17695/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17696/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17697/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17698/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17699/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17700/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17701/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17702/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17703/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17704/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17705/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17706/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17707/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17708/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17709/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17710/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17711/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17712/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17713/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17714/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17715/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17716/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17717/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17718/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17719/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17720/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17721/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17722/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17723/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17724/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17725/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17726/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17727/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17728/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17729/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17730/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17731/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17732/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17733/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17734/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17735/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17736/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17737/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17738/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17739/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17740/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17741/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17742/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17743/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17744/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17745/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17746/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17747/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17748/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17749/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17750/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17751/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17752/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17753/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17754/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17755/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17756/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17757/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17758/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17759/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17760/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17761/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17762/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17763/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17764/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17765/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17766/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17767/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17768/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17769/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17770/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17771/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17772/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17773/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17774/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17775/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17776/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17777/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17778/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17779/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17780/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17781/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17782/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17783/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17784/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17785/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17786/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17787/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17788/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17789/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17790/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17791/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17792/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17793/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17794/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17795/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17796/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17797/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17798/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17799/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17800/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17801/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17802/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17803/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17804/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17805/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17806/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17807/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17808/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17809/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17810/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17811/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17812/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17813/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17814/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17815/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17816/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17817/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17818/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17819/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17820/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17821/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17822/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17823/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17824/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17825/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17826/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17827/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17828/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17829/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17830/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17831/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17832/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17833/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17834/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17835/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17836/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17837/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17838/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17839/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17840/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17841/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17842/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17843/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17844/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17845/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17846/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17847/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17848/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17849/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17850/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17851/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17852/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17853/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17854/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17855/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17856/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17857/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17858/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17859/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17860/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17861/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17862/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17863/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17864/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17865/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17866/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17867/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17868/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17869/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17870/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17871/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17872/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17873/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17874/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17875/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17876/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17877/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17878/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17879/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17880/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17881/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17882/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17883/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17884/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17885/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17886/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17887/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17888/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17889/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17890/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17891/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17892/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17893/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17894/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17895/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17896/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17897/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17898/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17899/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17900/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17901/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17902/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17903/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17904/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17905/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17906/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17907/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17908/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17909/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17910/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17911/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17912/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17913/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17914/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17915/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17916/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17917/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17918/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17919/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17920/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17921/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17922/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17923/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17924/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17925/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17926/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17927/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17928/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17929/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17930/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17931/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17932/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17933/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17934/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17935/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17936/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17937/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17938/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17939/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17940/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17941/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17942/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17943/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17944/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17945/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17946/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17947/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17948/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17949/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17950/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17951/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17952/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17953/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17954/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17955/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17956/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17957/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17958/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17959/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17960/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17961/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17962/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17963/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17964/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17965/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17966/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17967/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17968/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17969/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17970/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17971/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17972/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17973/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17974/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17975/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17976/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17977/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17978/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17979/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17980/25000, Training Loss: 0.000004, lr : 0.000743\n",
      "Epoch: 17981/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17982/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17983/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17984/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17985/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17986/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17987/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17988/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17989/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17990/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17991/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17992/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17993/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17994/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17995/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17996/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17997/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 17998/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 17999/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18000/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18001/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18002/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18003/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18004/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18005/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18006/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18007/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18008/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18009/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18010/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18011/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18012/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18013/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18014/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18015/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18016/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18017/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18018/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18019/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18020/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18021/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18022/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18023/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18024/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18025/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18026/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18027/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18028/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18029/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18030/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18031/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18032/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18033/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18034/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18035/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18036/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18037/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18038/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18039/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18040/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18041/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18042/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18043/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18044/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18045/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18046/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18047/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18048/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18049/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18050/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18051/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18052/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18053/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18054/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18055/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18056/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18057/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18058/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18059/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18060/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18061/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18062/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18063/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18064/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18065/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18066/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18067/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18068/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18069/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18070/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18071/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18072/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18073/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18074/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18075/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18076/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18077/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18078/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18079/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18080/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18081/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18082/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18083/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18084/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18085/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18086/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18087/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18088/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18089/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18090/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18091/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18092/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18093/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18094/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18095/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18096/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18097/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18098/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18099/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18100/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18101/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18102/25000, Training Loss: 0.000002, lr : 0.000743\n",
      "Epoch: 18103/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18104/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18105/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18106/25000, Training Loss: 0.000003, lr : 0.000743\n",
      "Epoch: 18107/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18108/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18109/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18110/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18111/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18112/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18113/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18114/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18115/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18116/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18117/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18118/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18119/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18120/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18121/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18122/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18123/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18124/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18125/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18126/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18127/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18128/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18129/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18130/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18131/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18132/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18133/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18134/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18135/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18136/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18137/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18138/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18139/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18140/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18141/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18142/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18143/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18144/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18145/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18146/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18147/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18148/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18149/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18150/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18151/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18152/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18153/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18154/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18155/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18156/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18157/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18158/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18159/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18160/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18161/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18162/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18163/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18164/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18165/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18166/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18167/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18168/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18169/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18170/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18171/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18172/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18173/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18174/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18175/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18176/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18177/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18178/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18179/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18180/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18181/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18182/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18183/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18184/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18185/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18186/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18187/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18188/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18189/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18190/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18191/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18192/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18193/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18194/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18195/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18196/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18197/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18198/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18199/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18200/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18201/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18202/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18203/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18204/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18205/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18206/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18207/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18208/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18209/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18210/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18211/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18212/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18213/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18214/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18215/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18216/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18217/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18218/25000, Training Loss: 0.000004, lr : 0.000631\n",
      "Epoch: 18219/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18220/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18221/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18222/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18223/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18224/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18225/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18226/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18227/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18228/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18229/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18230/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18231/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18232/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18233/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18234/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18235/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18236/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18237/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18238/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18239/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18240/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18241/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18242/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18243/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18244/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18245/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18246/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18247/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18248/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18249/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18250/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18251/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18252/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18253/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18254/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18255/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18256/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18257/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18258/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18259/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18260/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18261/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18262/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18263/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18264/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18265/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18266/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18267/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18268/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18269/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18270/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18271/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18272/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18273/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18274/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18275/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18276/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18277/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18278/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18279/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18280/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18281/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18282/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18283/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18284/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18285/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18286/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18287/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18288/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18289/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18290/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18291/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18292/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18293/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18294/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18295/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18296/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18297/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18298/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18299/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18300/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18301/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18302/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18303/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18304/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18305/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18306/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18307/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18308/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18309/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18310/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18311/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18312/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18313/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18314/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18315/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18316/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18317/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18318/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18319/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18320/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18321/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18322/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18323/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18324/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18325/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18326/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18327/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18328/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18329/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18330/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18331/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18332/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18333/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18334/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18335/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18336/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18337/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18338/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18339/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18340/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18341/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18342/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18343/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18344/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18345/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18346/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18347/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18348/25000, Training Loss: 0.000004, lr : 0.000631\n",
      "Epoch: 18349/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18350/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18351/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18352/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18353/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18354/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18355/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18356/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18357/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18358/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18359/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18360/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18361/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18362/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18363/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18364/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18365/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18366/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18367/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18368/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18369/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18370/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18371/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18372/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18373/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18374/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18375/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18376/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18377/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18378/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18379/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18380/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18381/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18382/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18383/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18384/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18385/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18386/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18387/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18388/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18389/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18390/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18391/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18392/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18393/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18394/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18395/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18396/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18397/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18398/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18399/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18400/25000, Training Loss: 0.000003, lr : 0.000631\n",
      "Epoch: 18401/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18402/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18403/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18404/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18405/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18406/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18407/25000, Training Loss: 0.000002, lr : 0.000631\n",
      "Epoch: 18408/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18409/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18410/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18411/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18412/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18413/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18414/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18415/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18416/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18417/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18418/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18419/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18420/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18421/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18422/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18423/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18424/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18425/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18426/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18427/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18428/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18429/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18430/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18431/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18432/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18433/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18434/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18435/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18436/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18437/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18438/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18439/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18440/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18441/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18442/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18443/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18444/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18445/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18446/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18447/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18448/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18449/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18450/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18451/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18452/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18453/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18454/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18455/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18456/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18457/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18458/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18459/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18460/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18461/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18462/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18463/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18464/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18465/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18466/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18467/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18468/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18469/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18470/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18471/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18472/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18473/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18474/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18475/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18476/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18477/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18478/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18479/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18480/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18481/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18482/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18483/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18484/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18485/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18486/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18487/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18488/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18489/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18490/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18491/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18492/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18493/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18494/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18495/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18496/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18497/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18498/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18499/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18500/25000, Training Loss: 0.000001, lr : 0.000536\n",
      "Epoch: 18501/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18502/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18503/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18504/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18505/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18506/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18507/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18508/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18509/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18510/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18511/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18512/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18513/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18514/25000, Training Loss: 0.000001, lr : 0.000536\n",
      "Epoch: 18515/25000, Training Loss: 0.000001, lr : 0.000536\n",
      "Epoch: 18516/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18517/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18518/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18519/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18520/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18521/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18522/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18523/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18524/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18525/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18526/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18527/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18528/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18529/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18530/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18531/25000, Training Loss: 0.000001, lr : 0.000536\n",
      "Epoch: 18532/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18533/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18534/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18535/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18536/25000, Training Loss: 0.000001, lr : 0.000536\n",
      "Epoch: 18537/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18538/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18539/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18540/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18541/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18542/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18543/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18544/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18545/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18546/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18547/25000, Training Loss: 0.000001, lr : 0.000536\n",
      "Epoch: 18548/25000, Training Loss: 0.000001, lr : 0.000536\n",
      "Epoch: 18549/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18550/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18551/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18552/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18553/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18554/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18555/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18556/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18557/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18558/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18559/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18560/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18561/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18562/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18563/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18564/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18565/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18566/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18567/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18568/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18569/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18570/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18571/25000, Training Loss: 0.000001, lr : 0.000536\n",
      "Epoch: 18572/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18573/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18574/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18575/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18576/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18577/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18578/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18579/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18580/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18581/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18582/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18583/25000, Training Loss: 0.000001, lr : 0.000536\n",
      "Epoch: 18584/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18585/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18586/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18587/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18588/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18589/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18590/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18591/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18592/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18593/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18594/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18595/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18596/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18597/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18598/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18599/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18600/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18601/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18602/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18603/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18604/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18605/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18606/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18607/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18608/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18609/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18610/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18611/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18612/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18613/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18614/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18615/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18616/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18617/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18618/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18619/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18620/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18621/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18622/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18623/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18624/25000, Training Loss: 0.000001, lr : 0.000536\n",
      "Epoch: 18625/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18626/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18627/25000, Training Loss: 0.000001, lr : 0.000536\n",
      "Epoch: 18628/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18629/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18630/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18631/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18632/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18633/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18634/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18635/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18636/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18637/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18638/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18639/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18640/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18641/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18642/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18643/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18644/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18645/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18646/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18647/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18648/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18649/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18650/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18651/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18652/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18653/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18654/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18655/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18656/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18657/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18658/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18659/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18660/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18661/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18662/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18663/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18664/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18665/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18666/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18667/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18668/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18669/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18670/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18671/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18672/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18673/25000, Training Loss: 0.000004, lr : 0.000536\n",
      "Epoch: 18674/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18675/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18676/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18677/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18678/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18679/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18680/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18681/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18682/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18683/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18684/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18685/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18686/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18687/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18688/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18689/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18690/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18691/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18692/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18693/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18694/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18695/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18696/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18697/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18698/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18699/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18700/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18701/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18702/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18703/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18704/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18705/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18706/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18707/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18708/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18709/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18710/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18711/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18712/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18713/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18714/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18715/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18716/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18717/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18718/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18719/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18720/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18721/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18722/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18723/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18724/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18725/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18726/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18727/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18728/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18729/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18730/25000, Training Loss: 0.000001, lr : 0.000536\n",
      "Epoch: 18731/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18732/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18733/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18734/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18735/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18736/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18737/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18738/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18739/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18740/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18741/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18742/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18743/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18744/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18745/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18746/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18747/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18748/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18749/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18750/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18751/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18752/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18753/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18754/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18755/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18756/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18757/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18758/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18759/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18760/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18761/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18762/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18763/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18764/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18765/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18766/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18767/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18768/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18769/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18770/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18771/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18772/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18773/25000, Training Loss: 0.000004, lr : 0.000536\n",
      "Epoch: 18774/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18775/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18776/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18777/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18778/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18779/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18780/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18781/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18782/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18783/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18784/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18785/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18786/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18787/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18788/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18789/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18790/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18791/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18792/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18793/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18794/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18795/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18796/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18797/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18798/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18799/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18800/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18801/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18802/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18803/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18804/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18805/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18806/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18807/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18808/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18809/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18810/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18811/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18812/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18813/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18814/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18815/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18816/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18817/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18818/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18819/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18820/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18821/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18822/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18823/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18824/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18825/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18826/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18827/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18828/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18829/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18830/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18831/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18832/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18833/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18834/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18835/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18836/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18837/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18838/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18839/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18840/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18841/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18842/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18843/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18844/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18845/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18846/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18847/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18848/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18849/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18850/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18851/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18852/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18853/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18854/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18855/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18856/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18857/25000, Training Loss: 0.000001, lr : 0.000536\n",
      "Epoch: 18858/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18859/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18860/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18861/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18862/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18863/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18864/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18865/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18866/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18867/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18868/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18869/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18870/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18871/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18872/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18873/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18874/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18875/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18876/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18877/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18878/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18879/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18880/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18881/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18882/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18883/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18884/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18885/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18886/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18887/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18888/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18889/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18890/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18891/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18892/25000, Training Loss: 0.000003, lr : 0.000536\n",
      "Epoch: 18893/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18894/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18895/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18896/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18897/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18898/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18899/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18900/25000, Training Loss: 0.000002, lr : 0.000536\n",
      "Epoch: 18901/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18902/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18903/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18904/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18905/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18906/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18907/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18908/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18909/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18910/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18911/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18912/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18913/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18914/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18915/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18916/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18917/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18918/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18919/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 18920/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18921/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18922/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18923/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 18924/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18925/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18926/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18927/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 18928/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18929/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18930/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18931/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18932/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18933/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18934/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18935/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18936/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18937/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18938/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18939/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18940/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18941/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18942/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18943/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18944/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18945/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18946/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18947/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18948/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18949/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18950/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 18951/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18952/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18953/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18954/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18955/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18956/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18957/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18958/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18959/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18960/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18961/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18962/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18963/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18964/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 18965/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18966/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18967/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18968/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18969/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18970/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18971/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18972/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18973/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18974/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18975/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18976/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 18977/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18978/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18979/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18980/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18981/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18982/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18983/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18984/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18985/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18986/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18987/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18988/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18989/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18990/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18991/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18992/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18993/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18994/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18995/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18996/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 18997/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 18998/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 18999/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19000/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19001/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19002/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19003/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19004/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19005/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19006/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19007/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19008/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19009/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19010/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19011/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19012/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19013/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19014/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19015/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19016/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19017/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19018/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19019/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19020/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19021/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19022/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19023/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19024/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19025/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19026/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19027/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19028/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19029/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19030/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19031/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19032/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19033/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19034/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19035/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19036/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19037/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19038/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 19039/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19040/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19041/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19042/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19043/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19044/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19045/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19046/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19047/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19048/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19049/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19050/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19051/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19052/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19053/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19054/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19055/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19056/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19057/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19058/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19059/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19060/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19061/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19062/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19063/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19064/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19065/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19066/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19067/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 19068/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19069/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19070/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 19071/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19072/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19073/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19074/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19075/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19076/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19077/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19078/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19079/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19080/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19081/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19082/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 19083/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 19084/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19085/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19086/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19087/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19088/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19089/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19090/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19091/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19092/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19093/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19094/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19095/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19096/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19097/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19098/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19099/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19100/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19101/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19102/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19103/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19104/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19105/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19106/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19107/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19108/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19109/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19110/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19111/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19112/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19113/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19114/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 19115/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 19116/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19117/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19118/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19119/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 19120/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19121/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19122/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19123/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19124/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19125/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19126/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 19127/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19128/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19129/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19130/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19131/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19132/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19133/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19134/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19135/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19136/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19137/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19138/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19139/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19140/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19141/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19142/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19143/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19144/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19145/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19146/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19147/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19148/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19149/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19150/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19151/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19152/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19153/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19154/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19155/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19156/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19157/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19158/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19159/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19160/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19161/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19162/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19163/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19164/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19165/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19166/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19167/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19168/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19169/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19170/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19171/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19172/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19173/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19174/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19175/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19176/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19177/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19178/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19179/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19180/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19181/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19182/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19183/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19184/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19185/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19186/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19187/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19188/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19189/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19190/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19191/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19192/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 19193/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19194/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19195/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19196/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19197/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19198/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19199/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19200/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19201/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19202/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19203/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19204/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19205/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19206/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19207/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19208/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19209/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19210/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19211/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19212/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19213/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19214/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19215/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19216/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19217/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19218/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19219/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19220/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19221/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19222/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19223/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19224/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19225/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19226/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19227/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19228/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19229/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19230/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19231/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19232/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19233/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19234/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19235/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19236/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19237/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19238/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19239/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19240/25000, Training Loss: 0.000001, lr : 0.000456\n",
      "Epoch: 19241/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19242/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19243/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19244/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19245/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19246/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19247/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19248/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19249/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19250/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19251/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19252/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19253/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19254/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19255/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19256/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19257/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19258/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19259/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19260/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19261/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19262/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19263/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19264/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19265/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19266/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19267/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19268/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19269/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19270/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19271/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19272/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19273/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19274/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19275/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19276/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19277/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19278/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19279/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19280/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19281/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19282/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19283/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19284/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19285/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19286/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19287/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19288/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19289/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19290/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19291/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19292/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19293/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19294/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19295/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19296/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19297/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19298/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19299/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19300/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19301/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19302/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19303/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19304/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19305/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19306/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19307/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19308/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19309/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19310/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19311/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19312/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19313/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19314/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19315/25000, Training Loss: 0.000003, lr : 0.000456\n",
      "Epoch: 19316/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19317/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19318/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19319/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19320/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19321/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19322/25000, Training Loss: 0.000002, lr : 0.000456\n",
      "Epoch: 19323/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19324/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19325/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19326/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19327/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19328/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19329/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19330/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19331/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19332/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19333/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19334/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19335/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19336/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19337/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19338/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19339/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19340/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19341/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19342/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19343/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19344/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19345/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19346/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19347/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19348/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19349/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19350/25000, Training Loss: 0.000003, lr : 0.000388\n",
      "Epoch: 19351/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19352/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19353/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19354/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19355/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19356/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19357/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19358/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19359/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19360/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19361/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19362/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19363/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19364/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19365/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19366/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19367/25000, Training Loss: 0.000003, lr : 0.000388\n",
      "Epoch: 19368/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19369/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19370/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19371/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19372/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19373/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19374/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19375/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19376/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19377/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19378/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19379/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19380/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19381/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19382/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19383/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19384/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19385/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19386/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19387/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19388/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19389/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19390/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19391/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19392/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19393/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19394/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19395/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19396/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19397/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19398/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19399/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19400/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19401/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19402/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19403/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19404/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19405/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19406/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19407/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19408/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19409/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19410/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19411/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19412/25000, Training Loss: 0.000003, lr : 0.000388\n",
      "Epoch: 19413/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19414/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19415/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19416/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19417/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19418/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19419/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19420/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19421/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19422/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19423/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19424/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19425/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19426/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19427/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19428/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19429/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19430/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19431/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19432/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19433/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19434/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19435/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19436/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19437/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19438/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19439/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19440/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19441/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19442/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19443/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19444/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19445/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19446/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19447/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19448/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19449/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19450/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19451/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19452/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19453/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19454/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19455/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19456/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19457/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19458/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19459/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19460/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19461/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19462/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19463/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19464/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19465/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19466/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19467/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19468/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19469/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19470/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19471/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19472/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19473/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19474/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19475/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19476/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19477/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19478/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19479/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19480/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19481/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19482/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19483/25000, Training Loss: 0.000003, lr : 0.000388\n",
      "Epoch: 19484/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19485/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19486/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19487/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19488/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19489/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19490/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19491/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19492/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19493/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19494/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19495/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19496/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19497/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19498/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19499/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19500/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19501/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19502/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19503/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19504/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19505/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19506/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19507/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19508/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19509/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19510/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19511/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19512/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19513/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19514/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19515/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19516/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19517/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19518/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19519/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19520/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19521/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19522/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19523/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19524/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19525/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19526/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19527/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19528/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19529/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19530/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19531/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19532/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19533/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19534/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19535/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19536/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19537/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19538/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19539/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19540/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19541/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19542/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19543/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19544/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19545/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19546/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19547/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19548/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19549/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19550/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19551/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19552/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19553/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19554/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19555/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19556/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19557/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19558/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19559/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19560/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19561/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19562/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19563/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19564/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19565/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19566/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19567/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19568/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19569/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19570/25000, Training Loss: 0.000003, lr : 0.000388\n",
      "Epoch: 19571/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19572/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19573/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19574/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19575/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19576/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19577/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19578/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19579/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19580/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19581/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19582/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19583/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19584/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19585/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19586/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19587/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19588/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19589/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19590/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19591/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19592/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19593/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19594/25000, Training Loss: 0.000003, lr : 0.000388\n",
      "Epoch: 19595/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19596/25000, Training Loss: 0.000004, lr : 0.000388\n",
      "Epoch: 19597/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19598/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19599/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19600/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19601/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19602/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19603/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19604/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19605/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19606/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19607/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19608/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19609/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19610/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19611/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19612/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19613/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19614/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19615/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19616/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19617/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19618/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19619/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19620/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19621/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19622/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19623/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19624/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19625/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19626/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19627/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19628/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19629/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19630/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19631/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19632/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19633/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19634/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19635/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19636/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19637/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19638/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19639/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19640/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19641/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19642/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19643/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19644/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19645/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19646/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19647/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19648/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19649/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19650/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19651/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19652/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19653/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19654/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19655/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19656/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19657/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19658/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19659/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19660/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19661/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19662/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19663/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19664/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19665/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19666/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19667/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19668/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19669/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19670/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19671/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19672/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19673/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19674/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19675/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19676/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19677/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19678/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19679/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19680/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19681/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19682/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19683/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19684/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19685/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19686/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19687/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19688/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19689/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19690/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19691/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19692/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19693/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19694/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19695/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19696/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19697/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19698/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19699/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19700/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19701/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19702/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19703/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19704/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19705/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19706/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19707/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19708/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19709/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19710/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19711/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19712/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19713/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19714/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19715/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19716/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19717/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19718/25000, Training Loss: 0.000003, lr : 0.000388\n",
      "Epoch: 19719/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19720/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19721/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19722/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19723/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19724/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19725/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19726/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19727/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19728/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19729/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19730/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19731/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19732/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19733/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19734/25000, Training Loss: 0.000003, lr : 0.000388\n",
      "Epoch: 19735/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19736/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19737/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19738/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19739/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19740/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19741/25000, Training Loss: 0.000003, lr : 0.000388\n",
      "Epoch: 19742/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19743/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19744/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19745/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19746/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19747/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19748/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19749/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19750/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19751/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19752/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19753/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19754/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19755/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19756/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19757/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19758/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19759/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19760/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19761/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19762/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19763/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19764/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19765/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19766/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19767/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19768/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19769/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19770/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19771/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19772/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19773/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19774/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19775/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19776/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19777/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19778/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19779/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19780/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19781/25000, Training Loss: 0.000003, lr : 0.000388\n",
      "Epoch: 19782/25000, Training Loss: 0.000003, lr : 0.000388\n",
      "Epoch: 19783/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19784/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19785/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19786/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19787/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19788/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19789/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19790/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19791/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19792/25000, Training Loss: 0.000003, lr : 0.000388\n",
      "Epoch: 19793/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19794/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19795/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19796/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19797/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19798/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19799/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19800/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19801/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19802/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19803/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19804/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19805/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19806/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19807/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19808/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19809/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19810/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19811/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19812/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19813/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19814/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19815/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19816/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19817/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19818/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19819/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19820/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19821/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19822/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19823/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19824/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19825/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19826/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19827/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19828/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19829/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19830/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19831/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19832/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19833/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19834/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19835/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19836/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19837/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19838/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19839/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19840/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19841/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19842/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19843/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19844/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19845/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19846/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19847/25000, Training Loss: 0.000003, lr : 0.000388\n",
      "Epoch: 19848/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19849/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19850/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19851/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19852/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19853/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19854/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19855/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19856/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19857/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19858/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19859/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19860/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19861/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19862/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19863/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19864/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19865/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19866/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19867/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19868/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19869/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19870/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19871/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19872/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19873/25000, Training Loss: 0.000001, lr : 0.000388\n",
      "Epoch: 19874/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19875/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19876/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19877/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19878/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19879/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19880/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19881/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19882/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19883/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19884/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19885/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19886/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19887/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19888/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19889/25000, Training Loss: 0.000002, lr : 0.000388\n",
      "Epoch: 19890/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19891/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19892/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19893/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19894/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 19895/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19896/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19897/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19898/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19899/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19900/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19901/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19902/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19903/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19904/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19905/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19906/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19907/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19908/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19909/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19910/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19911/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19912/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19913/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19914/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19915/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 19916/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19917/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19918/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19919/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19920/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19921/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19922/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19923/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19924/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19925/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19926/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19927/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19928/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19929/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19930/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19931/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19932/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19933/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19934/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19935/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19936/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19937/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19938/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19939/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19940/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19941/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19942/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19943/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19944/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19945/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19946/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19947/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19948/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19949/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19950/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19951/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19952/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19953/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19954/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19955/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19956/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19957/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19958/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19959/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19960/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19961/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19962/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19963/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19964/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19965/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19966/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19967/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19968/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19969/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19970/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19971/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19972/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19973/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19974/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19975/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19976/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19977/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19978/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19979/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19980/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19981/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19982/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19983/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19984/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19985/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19986/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19987/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19988/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19989/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19990/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19991/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19992/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19993/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19994/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19995/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19996/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19997/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 19998/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 19999/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20000/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20001/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20002/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20003/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20004/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20005/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20006/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20007/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20008/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20009/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20010/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20011/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20012/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20013/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20014/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20015/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20016/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20017/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20018/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20019/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20020/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20021/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20022/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20023/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20024/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20025/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20026/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20027/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20028/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20029/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20030/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20031/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20032/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20033/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20034/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20035/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20036/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20037/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20038/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20039/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20040/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20041/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20042/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20043/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20044/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20045/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20046/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20047/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20048/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20049/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20050/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20051/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20052/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20053/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20054/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20055/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20056/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20057/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20058/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20059/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20060/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 20061/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20062/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20063/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 20064/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20065/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20066/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20067/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20068/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20069/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 20070/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20071/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20072/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20073/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20074/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20075/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20076/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20077/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20078/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20079/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20080/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20081/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20082/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20083/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20084/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20085/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20086/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20087/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20088/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20089/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20090/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20091/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20092/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20093/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20094/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20095/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20096/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20097/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20098/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20099/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20100/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20101/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20102/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20103/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20104/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20105/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20106/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20107/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20108/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20109/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20110/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20111/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20112/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20113/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20114/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20115/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20116/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20117/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20118/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20119/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20120/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20121/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20122/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20123/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20124/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20125/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20126/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20127/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20128/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20129/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20130/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20131/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20132/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20133/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20134/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20135/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20136/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20137/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20138/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20139/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20140/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20141/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 20142/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20143/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20144/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20145/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20146/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20147/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20148/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20149/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20150/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20151/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20152/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20153/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20154/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20155/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20156/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20157/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20158/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20159/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20160/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20161/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20162/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 20163/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20164/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20165/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20166/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20167/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20168/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20169/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20170/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20171/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20172/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20173/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20174/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20175/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20176/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20177/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20178/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20179/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20180/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20181/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20182/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20183/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20184/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20185/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20186/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20187/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20188/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20189/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20190/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20191/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20192/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20193/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20194/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20195/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20196/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20197/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20198/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20199/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20200/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20201/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20202/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20203/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20204/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20205/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20206/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20207/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20208/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20209/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20210/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20211/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20212/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20213/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 20214/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20215/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20216/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20217/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20218/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20219/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20220/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20221/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20222/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20223/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20224/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20225/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20226/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20227/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20228/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20229/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20230/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20231/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20232/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 20233/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20234/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20235/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20236/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20237/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20238/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20239/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20240/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20241/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20242/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20243/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20244/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20245/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20246/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20247/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20248/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20249/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20250/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20251/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20252/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20253/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20254/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20255/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20256/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20257/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20258/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20259/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20260/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20261/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20262/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 20263/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20264/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20265/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20266/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20267/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20268/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20269/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20270/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20271/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20272/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20273/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20274/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20275/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20276/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20277/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20278/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20279/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20280/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20281/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20282/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20283/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20284/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20285/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20286/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20287/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 20288/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20289/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20290/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20291/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20292/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20293/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20294/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20295/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20296/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20297/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20298/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20299/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20300/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20301/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20302/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20303/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20304/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20305/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20306/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20307/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20308/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20309/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20310/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20311/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 20312/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20313/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20314/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20315/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20316/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20317/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20318/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20319/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20320/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20321/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20322/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20323/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20324/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20325/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20326/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20327/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20328/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20329/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20330/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20331/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20332/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20333/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20334/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 20335/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20336/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20337/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20338/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20339/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20340/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20341/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20342/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20343/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20344/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20345/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20346/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20347/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20348/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20349/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20350/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20351/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20352/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20353/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20354/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20355/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20356/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20357/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20358/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20359/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20360/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20361/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20362/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20363/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20364/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20365/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20366/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20367/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20368/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20369/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20370/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 20371/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20372/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20373/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20374/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20375/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20376/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20377/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20378/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20379/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20380/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20381/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20382/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20383/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20384/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20385/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20386/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20387/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20388/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20389/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20390/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20391/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20392/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20393/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20394/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20395/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20396/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20397/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20398/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20399/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20400/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20401/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20402/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20403/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20404/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20405/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20406/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20407/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20408/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20409/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20410/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20411/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20412/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20413/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20414/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20415/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20416/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20417/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20418/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20419/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20420/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20421/25000, Training Loss: 0.000003, lr : 0.000329\n",
      "Epoch: 20422/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20423/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20424/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20425/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20426/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20427/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20428/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20429/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20430/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20431/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20432/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20433/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20434/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20435/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20436/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20437/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20438/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20439/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20440/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20441/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20442/25000, Training Loss: 0.000002, lr : 0.000329\n",
      "Epoch: 20443/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20444/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20445/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20446/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20447/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20448/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20449/25000, Training Loss: 0.000001, lr : 0.000329\n",
      "Epoch: 20450/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20451/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20452/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20453/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20454/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20455/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20456/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20457/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20458/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20459/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20460/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20461/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20462/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20463/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20464/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20465/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20466/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20467/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20468/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20469/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20470/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20471/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20472/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20473/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20474/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20475/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20476/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20477/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20478/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20479/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20480/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20481/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20482/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20483/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20484/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20485/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20486/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20487/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20488/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20489/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20490/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20491/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20492/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20493/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20494/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20495/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20496/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20497/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20498/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20499/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20500/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20501/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20502/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20503/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20504/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20505/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20506/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20507/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20508/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20509/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20510/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20511/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20512/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20513/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20514/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20515/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20516/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20517/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20518/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20519/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20520/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20521/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20522/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20523/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20524/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20525/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20526/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20527/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20528/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20529/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20530/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20531/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20532/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20533/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20534/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20535/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20536/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20537/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20538/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20539/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20540/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20541/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20542/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20543/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20544/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20545/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20546/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20547/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20548/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20549/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 20550/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20551/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20552/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20553/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20554/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20555/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 20556/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20557/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20558/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20559/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20560/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20561/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20562/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20563/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20564/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20565/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20566/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20567/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20568/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20569/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20570/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20571/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20572/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20573/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20574/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20575/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20576/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20577/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 20578/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20579/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20580/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20581/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20582/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20583/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20584/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20585/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20586/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20587/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20588/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20589/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20590/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20591/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20592/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20593/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20594/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20595/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20596/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20597/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20598/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20599/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20600/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20601/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20602/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20603/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20604/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20605/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20606/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20607/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20608/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20609/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20610/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20611/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20612/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20613/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20614/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20615/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20616/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20617/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20618/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20619/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20620/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20621/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20622/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20623/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20624/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20625/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20626/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20627/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20628/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20629/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20630/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20631/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20632/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20633/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20634/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20635/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20636/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20637/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20638/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20639/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20640/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20641/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20642/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20643/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20644/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20645/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20646/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20647/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20648/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20649/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20650/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20651/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20652/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20653/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20654/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20655/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20656/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20657/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20658/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20659/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20660/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20661/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 20662/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20663/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20664/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20665/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20666/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20667/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20668/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20669/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20670/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20671/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20672/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20673/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20674/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20675/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20676/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20677/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20678/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20679/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20680/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20681/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20682/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20683/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20684/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20685/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20686/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20687/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20688/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20689/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20690/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20691/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20692/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20693/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20694/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20695/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20696/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20697/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20698/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20699/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20700/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20701/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20702/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20703/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20704/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20705/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20706/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20707/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20708/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20709/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20710/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20711/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20712/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20713/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20714/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20715/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20716/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20717/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20718/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20719/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20720/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20721/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20722/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20723/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20724/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20725/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20726/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20727/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20728/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20729/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20730/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20731/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20732/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20733/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20734/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20735/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20736/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20737/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20738/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20739/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20740/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20741/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20742/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20743/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20744/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20745/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20746/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20747/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20748/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20749/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20750/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20751/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20752/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20753/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20754/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20755/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20756/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20757/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20758/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20759/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20760/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20761/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20762/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20763/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20764/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20765/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20766/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20767/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20768/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20769/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20770/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20771/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20772/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20773/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20774/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20775/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20776/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20777/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20778/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20779/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20780/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20781/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20782/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20783/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20784/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20785/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20786/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20787/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20788/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20789/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20790/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20791/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20792/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20793/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20794/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20795/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20796/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20797/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20798/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20799/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20800/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20801/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20802/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20803/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20804/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20805/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20806/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20807/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20808/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20809/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20810/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20811/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20812/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20813/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20814/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20815/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20816/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20817/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20818/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20819/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20820/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20821/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20822/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20823/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20824/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20825/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20826/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20827/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20828/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20829/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20830/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20831/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20832/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 20833/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20834/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20835/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20836/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20837/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20838/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20839/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20840/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20841/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20842/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20843/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20844/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20845/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20846/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20847/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20848/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20849/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20850/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20851/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 20852/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20853/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20854/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20855/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20856/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20857/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20858/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 20859/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20860/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20861/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20862/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20863/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20864/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20865/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20866/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20867/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20868/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20869/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20870/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20871/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20872/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20873/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20874/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20875/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 20876/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20877/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20878/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20879/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20880/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20881/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20882/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20883/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20884/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20885/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20886/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20887/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20888/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20889/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20890/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20891/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20892/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20893/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20894/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20895/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20896/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20897/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20898/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20899/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20900/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20901/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20902/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20903/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20904/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20905/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20906/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20907/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20908/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20909/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20910/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20911/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20912/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20913/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20914/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20915/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20916/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20917/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20918/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20919/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20920/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20921/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20922/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20923/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20924/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20925/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20926/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20927/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20928/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20929/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20930/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20931/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20932/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20933/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20934/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20935/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20936/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20937/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20938/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20939/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20940/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20941/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20942/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 20943/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20944/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20945/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20946/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20947/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20948/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20949/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20950/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20951/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20952/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20953/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20954/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20955/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20956/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 20957/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20958/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20959/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20960/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20961/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20962/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20963/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20964/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20965/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20966/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20967/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20968/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20969/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20970/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20971/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20972/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20973/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20974/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20975/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20976/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20977/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20978/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20979/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20980/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20981/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20982/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20983/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20984/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20985/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20986/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20987/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20988/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20989/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20990/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20991/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20992/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20993/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20994/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20995/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20996/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20997/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 20998/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 20999/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21000/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21001/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21002/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21003/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21004/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21005/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21006/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21007/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21008/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21009/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21010/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21011/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21012/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21013/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21014/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21015/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21016/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21017/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21018/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21019/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21020/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21021/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21022/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21023/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21024/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21025/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21026/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21027/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21028/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21029/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21030/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21031/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21032/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21033/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21034/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21035/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21036/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21037/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21038/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21039/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21040/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21041/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21042/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21043/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21044/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21045/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21046/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21047/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21048/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21049/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21050/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21051/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21052/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21053/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21054/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21055/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21056/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21057/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21058/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21059/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21060/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21061/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21062/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21063/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21064/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21065/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21066/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21067/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21068/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21069/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21070/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21071/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21072/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21073/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21074/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21075/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21076/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21077/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21078/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21079/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21080/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21081/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21082/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21083/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 21084/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21085/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21086/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21087/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21088/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21089/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21090/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21091/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21092/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21093/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21094/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21095/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21096/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21097/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21098/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21099/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21100/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21101/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21102/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21103/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21104/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21105/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21106/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21107/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21108/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21109/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21110/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21111/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21112/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21113/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21114/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21115/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21116/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21117/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21118/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21119/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21120/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21121/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21122/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21123/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21124/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21125/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21126/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21127/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21128/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21129/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21130/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21131/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21132/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21133/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21134/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21135/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21136/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21137/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21138/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21139/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21140/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21141/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21142/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21143/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21144/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21145/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21146/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21147/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21148/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21149/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21150/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21151/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21152/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21153/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21154/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21155/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21156/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21157/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21158/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21159/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21160/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21161/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21162/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21163/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21164/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21165/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21166/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21167/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21168/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21169/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21170/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21171/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21172/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21173/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21174/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21175/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21176/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21177/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21178/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21179/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21180/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21181/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21182/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21183/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21184/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21185/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21186/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21187/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21188/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21189/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21190/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 21191/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21192/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21193/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21194/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21195/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21196/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21197/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21198/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21199/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21200/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21201/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21202/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21203/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21204/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21205/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21206/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21207/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21208/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21209/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21210/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21211/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21212/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21213/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21214/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21215/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21216/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21217/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21218/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21219/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21220/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21221/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21222/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21223/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21224/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21225/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21226/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21227/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21228/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21229/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21230/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21231/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21232/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21233/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21234/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21235/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21236/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21237/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21238/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21239/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21240/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21241/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21242/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21243/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21244/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21245/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21246/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21247/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21248/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21249/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21250/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21251/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21252/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21253/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21254/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21255/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21256/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21257/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21258/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21259/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21260/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21261/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21262/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21263/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21264/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21265/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21266/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21267/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 21268/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21269/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21270/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21271/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21272/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21273/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21274/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21275/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21276/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21277/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21278/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21279/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21280/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21281/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21282/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21283/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21284/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21285/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21286/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21287/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21288/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21289/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21290/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21291/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21292/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21293/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21294/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21295/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21296/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21297/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21298/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21299/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21300/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21301/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21302/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 21303/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21304/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21305/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 21306/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21307/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21308/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21309/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21310/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21311/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21312/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21313/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21314/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21315/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21316/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21317/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21318/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21319/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21320/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21321/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21322/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21323/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21324/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21325/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21326/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21327/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21328/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21329/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21330/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21331/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21332/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21333/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21334/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21335/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21336/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21337/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21338/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21339/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21340/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21341/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21342/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21343/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21344/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21345/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21346/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21347/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21348/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21349/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21350/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21351/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21352/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21353/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21354/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21355/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21356/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21357/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21358/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21359/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21360/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21361/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21362/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21363/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21364/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21365/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21366/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21367/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21368/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21369/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21370/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21371/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21372/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21373/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21374/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21375/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21376/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21377/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21378/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21379/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21380/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21381/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21382/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21383/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21384/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21385/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21386/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21387/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21388/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21389/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21390/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21391/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21392/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21393/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21394/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21395/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21396/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21397/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21398/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21399/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21400/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21401/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21402/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21403/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21404/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21405/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21406/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21407/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21408/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21409/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21410/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21411/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21412/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21413/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21414/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21415/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21416/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21417/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21418/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21419/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21420/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21421/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21422/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21423/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21424/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21425/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21426/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21427/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21428/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21429/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21430/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21431/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21432/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21433/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21434/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21435/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21436/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21437/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21438/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21439/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21440/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21441/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21442/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21443/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21444/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21445/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21446/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21447/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21448/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21449/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21450/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21451/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21452/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21453/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21454/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21455/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21456/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21457/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21458/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21459/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21460/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21461/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21462/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21463/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21464/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21465/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21466/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21467/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21468/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21469/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 21470/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21471/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21472/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21473/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21474/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21475/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21476/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21477/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21478/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21479/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21480/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21481/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21482/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21483/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21484/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21485/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21486/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21487/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21488/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21489/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 21490/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21491/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21492/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21493/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21494/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21495/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21496/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21497/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21498/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21499/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21500/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21501/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21502/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21503/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21504/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21505/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21506/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21507/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21508/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 21509/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21510/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21511/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21512/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21513/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21514/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21515/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21516/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21517/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21518/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21519/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21520/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21521/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21522/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21523/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21524/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21525/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21526/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21527/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21528/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21529/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21530/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21531/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21532/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21533/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 21534/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21535/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21536/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21537/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21538/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21539/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21540/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21541/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21542/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21543/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21544/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21545/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21546/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21547/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21548/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21549/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21550/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21551/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21552/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21553/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21554/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21555/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21556/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21557/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21558/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21559/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21560/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21561/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21562/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21563/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21564/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21565/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21566/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21567/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21568/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21569/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21570/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21571/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21572/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21573/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21574/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21575/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21576/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21577/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21578/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21579/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21580/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21581/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21582/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21583/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21584/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21585/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21586/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21587/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21588/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21589/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21590/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21591/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21592/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21593/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21594/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21595/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21596/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21597/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21598/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21599/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21600/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21601/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21602/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21603/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21604/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21605/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21606/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21607/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21608/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21609/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21610/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21611/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21612/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21613/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21614/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21615/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21616/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21617/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21618/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21619/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21620/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21621/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21622/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21623/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21624/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21625/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21626/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21627/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21628/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 21629/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21630/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21631/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21632/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21633/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21634/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21635/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21636/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21637/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21638/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21639/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21640/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21641/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21642/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21643/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21644/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21645/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21646/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21647/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21648/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21649/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21650/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21651/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21652/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21653/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21654/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21655/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21656/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21657/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21658/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21659/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21660/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21661/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21662/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21663/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21664/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21665/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21666/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21667/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21668/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21669/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21670/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21671/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21672/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21673/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21674/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21675/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21676/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21677/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21678/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21679/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21680/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21681/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21682/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21683/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21684/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21685/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21686/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21687/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21688/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21689/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21690/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21691/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21692/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21693/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21694/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21695/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21696/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21697/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21698/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21699/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21700/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21701/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21702/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21703/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21704/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21705/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21706/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21707/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21708/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21709/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21710/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21711/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21712/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21713/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21714/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 21715/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21716/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21717/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21718/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21719/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21720/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21721/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21722/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21723/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21724/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21725/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21726/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21727/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21728/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21729/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21730/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21731/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21732/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21733/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21734/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21735/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21736/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21737/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21738/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21739/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21740/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21741/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21742/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21743/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21744/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21745/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21746/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21747/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21748/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21749/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21750/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21751/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21752/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21753/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21754/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21755/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21756/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21757/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21758/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21759/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21760/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21761/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21762/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21763/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21764/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21765/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21766/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21767/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21768/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21769/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21770/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21771/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21772/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 21773/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21774/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21775/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21776/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21777/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21778/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21779/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21780/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21781/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21782/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21783/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21784/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21785/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21786/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21787/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21788/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21789/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21790/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21791/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21792/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21793/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21794/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21795/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21796/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21797/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21798/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21799/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21800/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21801/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21802/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21803/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21804/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21805/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21806/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21807/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21808/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21809/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21810/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21811/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21812/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21813/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21814/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21815/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21816/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21817/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21818/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21819/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21820/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21821/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21822/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21823/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21824/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21825/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21826/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21827/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21828/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21829/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21830/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21831/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21832/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21833/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21834/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21835/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21836/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21837/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21838/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21839/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21840/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21841/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21842/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21843/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21844/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21845/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21846/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21847/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21848/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21849/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21850/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21851/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21852/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21853/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21854/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21855/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21856/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21857/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21858/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21859/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21860/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21861/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21862/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21863/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21864/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21865/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21866/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21867/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21868/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21869/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21870/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21871/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21872/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21873/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21874/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21875/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21876/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21877/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21878/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21879/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21880/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21881/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21882/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21883/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21884/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21885/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21886/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21887/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21888/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21889/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21890/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21891/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21892/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21893/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21894/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21895/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21896/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21897/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21898/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21899/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21900/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21901/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21902/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21903/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21904/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21905/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21906/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21907/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21908/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21909/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21910/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21911/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21912/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21913/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21914/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21915/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21916/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21917/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21918/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21919/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21920/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21921/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21922/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21923/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21924/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21925/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21926/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21927/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21928/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21929/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21930/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21931/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21932/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21933/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21934/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21935/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21936/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21937/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21938/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21939/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21940/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21941/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21942/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21943/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21944/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21945/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21946/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21947/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21948/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21949/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21950/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21951/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21952/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21953/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21954/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21955/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21956/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21957/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21958/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21959/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21960/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21961/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21962/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21963/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21964/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21965/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21966/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21967/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21968/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21969/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21970/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21971/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21972/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21973/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21974/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21975/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21976/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21977/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21978/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21979/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21980/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21981/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21982/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21983/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21984/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21985/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21986/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21987/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21988/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21989/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21990/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21991/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21992/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21993/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21994/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 21995/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21996/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21997/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21998/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 21999/25000, Training Loss: 0.000003, lr : 0.000280\n",
      "Epoch: 22000/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 22001/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 22002/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22003/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22004/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22005/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22006/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22007/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22008/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22009/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22010/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22011/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22012/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22013/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22014/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22015/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 22016/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22017/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 22018/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22019/25000, Training Loss: 0.000002, lr : 0.000280\n",
      "Epoch: 22020/25000, Training Loss: 0.000001, lr : 0.000280\n",
      "Epoch: 22021/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22022/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22023/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22024/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22025/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22026/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22027/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22028/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22029/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22030/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22031/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22032/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22033/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22034/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22035/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22036/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22037/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22038/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22039/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22040/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22041/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22042/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22043/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22044/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22045/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22046/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22047/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22048/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22049/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22050/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22051/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22052/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22053/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22054/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22055/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22056/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22057/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22058/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22059/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22060/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22061/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22062/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22063/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22064/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22065/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22066/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22067/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22068/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22069/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22070/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22071/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22072/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22073/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22074/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22075/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22076/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22077/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22078/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22079/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22080/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22081/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22082/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22083/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22084/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22085/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22086/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22087/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22088/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22089/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22090/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22091/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22092/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22093/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22094/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22095/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22096/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22097/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22098/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22099/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22100/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22101/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22102/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22103/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22104/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22105/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22106/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22107/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22108/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22109/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22110/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22111/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22112/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22113/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22114/25000, Training Loss: 0.000003, lr : 0.000238\n",
      "Epoch: 22115/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22116/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22117/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22118/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22119/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22120/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22121/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22122/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22123/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22124/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22125/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22126/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22127/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22128/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22129/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22130/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22131/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22132/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22133/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22134/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22135/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22136/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22137/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22138/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22139/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22140/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22141/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22142/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22143/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22144/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22145/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22146/25000, Training Loss: 0.000003, lr : 0.000238\n",
      "Epoch: 22147/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22148/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22149/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22150/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22151/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22152/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22153/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22154/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22155/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22156/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22157/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22158/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22159/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22160/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22161/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22162/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22163/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22164/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22165/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22166/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22167/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22168/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22169/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22170/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22171/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22172/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22173/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22174/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22175/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22176/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22177/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22178/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22179/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22180/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22181/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22182/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22183/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22184/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22185/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22186/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22187/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22188/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22189/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22190/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22191/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22192/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22193/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22194/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22195/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22196/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22197/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22198/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22199/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22200/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22201/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22202/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22203/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22204/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22205/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22206/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22207/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22208/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22209/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22210/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22211/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22212/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22213/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22214/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22215/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22216/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22217/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22218/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22219/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22220/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22221/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22222/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22223/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22224/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22225/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22226/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22227/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22228/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22229/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22230/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22231/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22232/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22233/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22234/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22235/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22236/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22237/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22238/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22239/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22240/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22241/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22242/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22243/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22244/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22245/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22246/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22247/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22248/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22249/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22250/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22251/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22252/25000, Training Loss: 0.000003, lr : 0.000238\n",
      "Epoch: 22253/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22254/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22255/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22256/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22257/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22258/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22259/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22260/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22261/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22262/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22263/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22264/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22265/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22266/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22267/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22268/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22269/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22270/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22271/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22272/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22273/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22274/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22275/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22276/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22277/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22278/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22279/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22280/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22281/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22282/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22283/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22284/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22285/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22286/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22287/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22288/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22289/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22290/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22291/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22292/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22293/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22294/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22295/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22296/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22297/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22298/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22299/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22300/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22301/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22302/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22303/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22304/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22305/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22306/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22307/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22308/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22309/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22310/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22311/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22312/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22313/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22314/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22315/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22316/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22317/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22318/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22319/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22320/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22321/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22322/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22323/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22324/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22325/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22326/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22327/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22328/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22329/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22330/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22331/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22332/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22333/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22334/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22335/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22336/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22337/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22338/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22339/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22340/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22341/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22342/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22343/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22344/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22345/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22346/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22347/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22348/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22349/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22350/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22351/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22352/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22353/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22354/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22355/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22356/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22357/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22358/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22359/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22360/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22361/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22362/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22363/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22364/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22365/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22366/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22367/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22368/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22369/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22370/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22371/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22372/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22373/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22374/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22375/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22376/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22377/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22378/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22379/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22380/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22381/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22382/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22383/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22384/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22385/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22386/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22387/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22388/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22389/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22390/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22391/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22392/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22393/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22394/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22395/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22396/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22397/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22398/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22399/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22400/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22401/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22402/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22403/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22404/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22405/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22406/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22407/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22408/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22409/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22410/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22411/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22412/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22413/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22414/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22415/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22416/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22417/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22418/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22419/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22420/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22421/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22422/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22423/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22424/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22425/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22426/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22427/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22428/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22429/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22430/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22431/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22432/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22433/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22434/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22435/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22436/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22437/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22438/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22439/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22440/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22441/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22442/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22443/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22444/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22445/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22446/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22447/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22448/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22449/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22450/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22451/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22452/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22453/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22454/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22455/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22456/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22457/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22458/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22459/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22460/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22461/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22462/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22463/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22464/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22465/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22466/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22467/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22468/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22469/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22470/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22471/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22472/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22473/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22474/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22475/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22476/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22477/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22478/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22479/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22480/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22481/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22482/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22483/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22484/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22485/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22486/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22487/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22488/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22489/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22490/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22491/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22492/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22493/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22494/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22495/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22496/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22497/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22498/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22499/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22500/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22501/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22502/25000, Training Loss: 0.000003, lr : 0.000238\n",
      "Epoch: 22503/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22504/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22505/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22506/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22507/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22508/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22509/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22510/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22511/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22512/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22513/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22514/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22515/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22516/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22517/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22518/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22519/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22520/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22521/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22522/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22523/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22524/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22525/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22526/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22527/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22528/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22529/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22530/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22531/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22532/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22533/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22534/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22535/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22536/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22537/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22538/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22539/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22540/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22541/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22542/25000, Training Loss: 0.000003, lr : 0.000238\n",
      "Epoch: 22543/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22544/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22545/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22546/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22547/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22548/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22549/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22550/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22551/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22552/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22553/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22554/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22555/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22556/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22557/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22558/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22559/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22560/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22561/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22562/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22563/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22564/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22565/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22566/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22567/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22568/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22569/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22570/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22571/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22572/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22573/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22574/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22575/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22576/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22577/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22578/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22579/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22580/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22581/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22582/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22583/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22584/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22585/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22586/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22587/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22588/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22589/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22590/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22591/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22592/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22593/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22594/25000, Training Loss: 0.000003, lr : 0.000238\n",
      "Epoch: 22595/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22596/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22597/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22598/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22599/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22600/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22601/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22602/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22603/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22604/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22605/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22606/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22607/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22608/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22609/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22610/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22611/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22612/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22613/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22614/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22615/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22616/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22617/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22618/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22619/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22620/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22621/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22622/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22623/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22624/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22625/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22626/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22627/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22628/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22629/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22630/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22631/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22632/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22633/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22634/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22635/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22636/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22637/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22638/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22639/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22640/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22641/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22642/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22643/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22644/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22645/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22646/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22647/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22648/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22649/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22650/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22651/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22652/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22653/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22654/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22655/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22656/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22657/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22658/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22659/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22660/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22661/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22662/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22663/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22664/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22665/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22666/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22667/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22668/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22669/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22670/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22671/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22672/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22673/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22674/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22675/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22676/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22677/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22678/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22679/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22680/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22681/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22682/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22683/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22684/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22685/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22686/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22687/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22688/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22689/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22690/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22691/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22692/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22693/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22694/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22695/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22696/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22697/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22698/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22699/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22700/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22701/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22702/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22703/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22704/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22705/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22706/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22707/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22708/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22709/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22710/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22711/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22712/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22713/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22714/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22715/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22716/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22717/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22718/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22719/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22720/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22721/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22722/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22723/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22724/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22725/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22726/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22727/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22728/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22729/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22730/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22731/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22732/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22733/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22734/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22735/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22736/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22737/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22738/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22739/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22740/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22741/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22742/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22743/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22744/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22745/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22746/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22747/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22748/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22749/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22750/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22751/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22752/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22753/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22754/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22755/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22756/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22757/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22758/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22759/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22760/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22761/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22762/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22763/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22764/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22765/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22766/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22767/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22768/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22769/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22770/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22771/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22772/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22773/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22774/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22775/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22776/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22777/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22778/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22779/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22780/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22781/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22782/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22783/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22784/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22785/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22786/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22787/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22788/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22789/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22790/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22791/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22792/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22793/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22794/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22795/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22796/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22797/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22798/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22799/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22800/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22801/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22802/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22803/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22804/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22805/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22806/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22807/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22808/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22809/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22810/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22811/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22812/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22813/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22814/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22815/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22816/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22817/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22818/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22819/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22820/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22821/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22822/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22823/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22824/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22825/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22826/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22827/25000, Training Loss: 0.000003, lr : 0.000238\n",
      "Epoch: 22828/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22829/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22830/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22831/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22832/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22833/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22834/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22835/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22836/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22837/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22838/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22839/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22840/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22841/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22842/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22843/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22844/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22845/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22846/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22847/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22848/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22849/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22850/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22851/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22852/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22853/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22854/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22855/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22856/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22857/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22858/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22859/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22860/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22861/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22862/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22863/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22864/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22865/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22866/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22867/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22868/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22869/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22870/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22871/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22872/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22873/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22874/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22875/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22876/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22877/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22878/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22879/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22880/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22881/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22882/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22883/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22884/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22885/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22886/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22887/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22888/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22889/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22890/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22891/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22892/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22893/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22894/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22895/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22896/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22897/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22898/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22899/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22900/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22901/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22902/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22903/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22904/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22905/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22906/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22907/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22908/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22909/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22910/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22911/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22912/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22913/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22914/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22915/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22916/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22917/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22918/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22919/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22920/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22921/25000, Training Loss: 0.000003, lr : 0.000238\n",
      "Epoch: 22922/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22923/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22924/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22925/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22926/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22927/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22928/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22929/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22930/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22931/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22932/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22933/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22934/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22935/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22936/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22937/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22938/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22939/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22940/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22941/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22942/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22943/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22944/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22945/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22946/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22947/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22948/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22949/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22950/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22951/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22952/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22953/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22954/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22955/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22956/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22957/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22958/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22959/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22960/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22961/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22962/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22963/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22964/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22965/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22966/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22967/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22968/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22969/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22970/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22971/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22972/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22973/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22974/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22975/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22976/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22977/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22978/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22979/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22980/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22981/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22982/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22983/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22984/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22985/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22986/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22987/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22988/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22989/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22990/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22991/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22992/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22993/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22994/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22995/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22996/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 22997/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22998/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 22999/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23000/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23001/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23002/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23003/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23004/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23005/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23006/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23007/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23008/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23009/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23010/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23011/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23012/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23013/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23014/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23015/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23016/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23017/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23018/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23019/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23020/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23021/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23022/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23023/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23024/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23025/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23026/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23027/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23028/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23029/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23030/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23031/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23032/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23033/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23034/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23035/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23036/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23037/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23038/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23039/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23040/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23041/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23042/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23043/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23044/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23045/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23046/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23047/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23048/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23049/25000, Training Loss: 0.000003, lr : 0.000238\n",
      "Epoch: 23050/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23051/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23052/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23053/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23054/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23055/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23056/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23057/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23058/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23059/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23060/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23061/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23062/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23063/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23064/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23065/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23066/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23067/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23068/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23069/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23070/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23071/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23072/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23073/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23074/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23075/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23076/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23077/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23078/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23079/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23080/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23081/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23082/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23083/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23084/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23085/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23086/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23087/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23088/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23089/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23090/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23091/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23092/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23093/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23094/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23095/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23096/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23097/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23098/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23099/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23100/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23101/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23102/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23103/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23104/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23105/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23106/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23107/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23108/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23109/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23110/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23111/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23112/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23113/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23114/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23115/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23116/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23117/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23118/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23119/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23120/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23121/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23122/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23123/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23124/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23125/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23126/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23127/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23128/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23129/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23130/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23131/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23132/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23133/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23134/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23135/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23136/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23137/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23138/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23139/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23140/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23141/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23142/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23143/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23144/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23145/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23146/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23147/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23148/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23149/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23150/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23151/25000, Training Loss: 0.000002, lr : 0.000238\n",
      "Epoch: 23152/25000, Training Loss: 0.000001, lr : 0.000238\n",
      "Epoch: 23153/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23154/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23155/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23156/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23157/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23158/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23159/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23160/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23161/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23162/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23163/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23164/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23165/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23166/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23167/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23168/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23169/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23170/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23171/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23172/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23173/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23174/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23175/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23176/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23177/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23178/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23179/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23180/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23181/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23182/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23183/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23184/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23185/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23186/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23187/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23188/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23189/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23190/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23191/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23192/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23193/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23194/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23195/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23196/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23197/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23198/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23199/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23200/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23201/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23202/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23203/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23204/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23205/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23206/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23207/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23208/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23209/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23210/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23211/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23212/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23213/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23214/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23215/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23216/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23217/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23218/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23219/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23220/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23221/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23222/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23223/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23224/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23225/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23226/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23227/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23228/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23229/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23230/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23231/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23232/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23233/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23234/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23235/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23236/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23237/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23238/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23239/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23240/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23241/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23242/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23243/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23244/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23245/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23246/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23247/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23248/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23249/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23250/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23251/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23252/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23253/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23254/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23255/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23256/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23257/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23258/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23259/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23260/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23261/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23262/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23263/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23264/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23265/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23266/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23267/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23268/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23269/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23270/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23271/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23272/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23273/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23274/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23275/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23276/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23277/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23278/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23279/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23280/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23281/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23282/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23283/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23284/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23285/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23286/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23287/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23288/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23289/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23290/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23291/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23292/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23293/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23294/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23295/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23296/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23297/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23298/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23299/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23300/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23301/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23302/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23303/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23304/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23305/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23306/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23307/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23308/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23309/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23310/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23311/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23312/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23313/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23314/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23315/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23316/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23317/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23318/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23319/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23320/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23321/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23322/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23323/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23324/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23325/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23326/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23327/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23328/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23329/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23330/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23331/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23332/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23333/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23334/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23335/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23336/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23337/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23338/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23339/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23340/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23341/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23342/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23343/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23344/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23345/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23346/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23347/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23348/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23349/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23350/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23351/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23352/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23353/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23354/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23355/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23356/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23357/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23358/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23359/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23360/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23361/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23362/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23363/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23364/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23365/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23366/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23367/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23368/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23369/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23370/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23371/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23372/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23373/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23374/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23375/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23376/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23377/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23378/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23379/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23380/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23381/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23382/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23383/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23384/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23385/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23386/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23387/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23388/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23389/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23390/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23391/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23392/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23393/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23394/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23395/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23396/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23397/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23398/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23399/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23400/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23401/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23402/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23403/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23404/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23405/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23406/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23407/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23408/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23409/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23410/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23411/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23412/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23413/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23414/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23415/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23416/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23417/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23418/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23419/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23420/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23421/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23422/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23423/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23424/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23425/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23426/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23427/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23428/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23429/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23430/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23431/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23432/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23433/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23434/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23435/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23436/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23437/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23438/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23439/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23440/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23441/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23442/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23443/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23444/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23445/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23446/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23447/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23448/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23449/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23450/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23451/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23452/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23453/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23454/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23455/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23456/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23457/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23458/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23459/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23460/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23461/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23462/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23463/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23464/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23465/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23466/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23467/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23468/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23469/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23470/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23471/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23472/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23473/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23474/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23475/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23476/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23477/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23478/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23479/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23480/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23481/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23482/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23483/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23484/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23485/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23486/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23487/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23488/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23489/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23490/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23491/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23492/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23493/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23494/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23495/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23496/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23497/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23498/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23499/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23500/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23501/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23502/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23503/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23504/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23505/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23506/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23507/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23508/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23509/25000, Training Loss: 0.000003, lr : 0.000202\n",
      "Epoch: 23510/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23511/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23512/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23513/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23514/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23515/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23516/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23517/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23518/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23519/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23520/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23521/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23522/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23523/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23524/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23525/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23526/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23527/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23528/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23529/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23530/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23531/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23532/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23533/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23534/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23535/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23536/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23537/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23538/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23539/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23540/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23541/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23542/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23543/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23544/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23545/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23546/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23547/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23548/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23549/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23550/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23551/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23552/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23553/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23554/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23555/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23556/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23557/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23558/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23559/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23560/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23561/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23562/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23563/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23564/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23565/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23566/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23567/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23568/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23569/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23570/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23571/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23572/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23573/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23574/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23575/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23576/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23577/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23578/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23579/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23580/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23581/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23582/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23583/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23584/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23585/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23586/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23587/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23588/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23589/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23590/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23591/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23592/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23593/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23594/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23595/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23596/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23597/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23598/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23599/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23600/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23601/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23602/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23603/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23604/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23605/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23606/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23607/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23608/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23609/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23610/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23611/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23612/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23613/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23614/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23615/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23616/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23617/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23618/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23619/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23620/25000, Training Loss: 0.000003, lr : 0.000202\n",
      "Epoch: 23621/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23622/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23623/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23624/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23625/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23626/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23627/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23628/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23629/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23630/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23631/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23632/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23633/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23634/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23635/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23636/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23637/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23638/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23639/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23640/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23641/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23642/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23643/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23644/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23645/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23646/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23647/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23648/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23649/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23650/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23651/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23652/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23653/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23654/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23655/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23656/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23657/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23658/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23659/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23660/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23661/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23662/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23663/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23664/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23665/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23666/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23667/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23668/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23669/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23670/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23671/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23672/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23673/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23674/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23675/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23676/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23677/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23678/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23679/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23680/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23681/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23682/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23683/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23684/25000, Training Loss: 0.000003, lr : 0.000202\n",
      "Epoch: 23685/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23686/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23687/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23688/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23689/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23690/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23691/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23692/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23693/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23694/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23695/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23696/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23697/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23698/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23699/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23700/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23701/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23702/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23703/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23704/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23705/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23706/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23707/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23708/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23709/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23710/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23711/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23712/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23713/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23714/25000, Training Loss: 0.000002, lr : 0.000202\n",
      "Epoch: 23715/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23716/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23717/25000, Training Loss: 0.000001, lr : 0.000202\n",
      "Epoch: 23718/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23719/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23720/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23721/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23722/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23723/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23724/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23725/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23726/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23727/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23728/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23729/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23730/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23731/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23732/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23733/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23734/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23735/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23736/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23737/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23738/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23739/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23740/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23741/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23742/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23743/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23744/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23745/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23746/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23747/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23748/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23749/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23750/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23751/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23752/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23753/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23754/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23755/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23756/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23757/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23758/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23759/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23760/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23761/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23762/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23763/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23764/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23765/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23766/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23767/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23768/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23769/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23770/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23771/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23772/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23773/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23774/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23775/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23776/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23777/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23778/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23779/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23780/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23781/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23782/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23783/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23784/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23785/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23786/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23787/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23788/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23789/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23790/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23791/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23792/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23793/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23794/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23795/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23796/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23797/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23798/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23799/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23800/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23801/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23802/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23803/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23804/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23805/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23806/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23807/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23808/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23809/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23810/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23811/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23812/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23813/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23814/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23815/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23816/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23817/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23818/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23819/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23820/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23821/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23822/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23823/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23824/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23825/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23826/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23827/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23828/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23829/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23830/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23831/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23832/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23833/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23834/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23835/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23836/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23837/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23838/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23839/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23840/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23841/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23842/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23843/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23844/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23845/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23846/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23847/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23848/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23849/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23850/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23851/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23852/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23853/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23854/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23855/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23856/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23857/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23858/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23859/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23860/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23861/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23862/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23863/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23864/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23865/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23866/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23867/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23868/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23869/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23870/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23871/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23872/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23873/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23874/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23875/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23876/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23877/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23878/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23879/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23880/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23881/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23882/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23883/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23884/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23885/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23886/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23887/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23888/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23889/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23890/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23891/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23892/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23893/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23894/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23895/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23896/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23897/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23898/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23899/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23900/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23901/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23902/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23903/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23904/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23905/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23906/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23907/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23908/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23909/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23910/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23911/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23912/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23913/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23914/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23915/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23916/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23917/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23918/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23919/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23920/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23921/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23922/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23923/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23924/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23925/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23926/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23927/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23928/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23929/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23930/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23931/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23932/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23933/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23934/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23935/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23936/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23937/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23938/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23939/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23940/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23941/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23942/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23943/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23944/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23945/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23946/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23947/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23948/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23949/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23950/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23951/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23952/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23953/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23954/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23955/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23956/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23957/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23958/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23959/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23960/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23961/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23962/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23963/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23964/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23965/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23966/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23967/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23968/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23969/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23970/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23971/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23972/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23973/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23974/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23975/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23976/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23977/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23978/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23979/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23980/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23981/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23982/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23983/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23984/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23985/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23986/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23987/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23988/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23989/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23990/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23991/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23992/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23993/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23994/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 23995/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23996/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23997/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23998/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 23999/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24000/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24001/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24002/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24003/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24004/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24005/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24006/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24007/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24008/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24009/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24010/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24011/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24012/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 24013/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24014/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24015/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24016/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24017/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24018/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24019/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24020/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24021/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24022/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24023/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24024/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24025/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24026/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24027/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 24028/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24029/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24030/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24031/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24032/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24033/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 24034/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24035/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24036/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24037/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24038/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24039/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24040/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24041/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24042/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24043/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24044/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24045/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24046/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24047/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24048/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24049/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24050/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24051/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24052/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24053/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24054/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 24055/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24056/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24057/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24058/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24059/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24060/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 24061/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24062/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24063/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24064/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24065/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24066/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24067/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24068/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24069/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24070/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24071/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24072/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24073/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24074/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24075/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24076/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24077/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24078/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24079/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24080/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24081/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24082/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24083/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24084/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24085/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24086/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24087/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24088/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24089/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24090/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24091/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24092/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24093/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24094/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24095/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24096/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24097/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24098/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 24099/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24100/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24101/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24102/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24103/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24104/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24105/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24106/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24107/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24108/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24109/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24110/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24111/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24112/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24113/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24114/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24115/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24116/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24117/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24118/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24119/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24120/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24121/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24122/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24123/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24124/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24125/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24126/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24127/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24128/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24129/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24130/25000, Training Loss: 0.000002, lr : 0.000172\n",
      "Epoch: 24131/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24132/25000, Training Loss: 0.000001, lr : 0.000172\n",
      "Epoch: 24133/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24134/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24135/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24136/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24137/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24138/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24139/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24140/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24141/25000, Training Loss: 0.000003, lr : 0.000146\n",
      "Epoch: 24142/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24143/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24144/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24145/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24146/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24147/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24148/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24149/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24150/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24151/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24152/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24153/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24154/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24155/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24156/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24157/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24158/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24159/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24160/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24161/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24162/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24163/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24164/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24165/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24166/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24167/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24168/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24169/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24170/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24171/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24172/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24173/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24174/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24175/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24176/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24177/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24178/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24179/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24180/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24181/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24182/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24183/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24184/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24185/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24186/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24187/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24188/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24189/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24190/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24191/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24192/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24193/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24194/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24195/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24196/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24197/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24198/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24199/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24200/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24201/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24202/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24203/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24204/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24205/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24206/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24207/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24208/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24209/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24210/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24211/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24212/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24213/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24214/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24215/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24216/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24217/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24218/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24219/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24220/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24221/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24222/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24223/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24224/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24225/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24226/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24227/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24228/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24229/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24230/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24231/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24232/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24233/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24234/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24235/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24236/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24237/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24238/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24239/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24240/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24241/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24242/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24243/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24244/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24245/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24246/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24247/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24248/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24249/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24250/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24251/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24252/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24253/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24254/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24255/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24256/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24257/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24258/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24259/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24260/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24261/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24262/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24263/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24264/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24265/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24266/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24267/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24268/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24269/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24270/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24271/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24272/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24273/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24274/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24275/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24276/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24277/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24278/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24279/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24280/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24281/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24282/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24283/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24284/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24285/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24286/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24287/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24288/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24289/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24290/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24291/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24292/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24293/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24294/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24295/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24296/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24297/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24298/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24299/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24300/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24301/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24302/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24303/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24304/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24305/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24306/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24307/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24308/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24309/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24310/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24311/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24312/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24313/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24314/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24315/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24316/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24317/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24318/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24319/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24320/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24321/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24322/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24323/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24324/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24325/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24326/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24327/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24328/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24329/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24330/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24331/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24332/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24333/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24334/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24335/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24336/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24337/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24338/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24339/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24340/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24341/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24342/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24343/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24344/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24345/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24346/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24347/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24348/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24349/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24350/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24351/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24352/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24353/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24354/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24355/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24356/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24357/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24358/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24359/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24360/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24361/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24362/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24363/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24364/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24365/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24366/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24367/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24368/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24369/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24370/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24371/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24372/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24373/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24374/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24375/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24376/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24377/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24378/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24379/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24380/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24381/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24382/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24383/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24384/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24385/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24386/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24387/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24388/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24389/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24390/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24391/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24392/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24393/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24394/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24395/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24396/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24397/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24398/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24399/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24400/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24401/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24402/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24403/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24404/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24405/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24406/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24407/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24408/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24409/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24410/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24411/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24412/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24413/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24414/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24415/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24416/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24417/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24418/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24419/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24420/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24421/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24422/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24423/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24424/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24425/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24426/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24427/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24428/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24429/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24430/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24431/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24432/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24433/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24434/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24435/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24436/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24437/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24438/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24439/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24440/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24441/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24442/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24443/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24444/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24445/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24446/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24447/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24448/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24449/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24450/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24451/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24452/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24453/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24454/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24455/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24456/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24457/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24458/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24459/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24460/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24461/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24462/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24463/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24464/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24465/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24466/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24467/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24468/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24469/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24470/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24471/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24472/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24473/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24474/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24475/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24476/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24477/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24478/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24479/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24480/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24481/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24482/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24483/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24484/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24485/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24486/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24487/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24488/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24489/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24490/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24491/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24492/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24493/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24494/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24495/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24496/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24497/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24498/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24499/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24500/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24501/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24502/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24503/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24504/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24505/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24506/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24507/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24508/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24509/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24510/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24511/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24512/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24513/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24514/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24515/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24516/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24517/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24518/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24519/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24520/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24521/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24522/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24523/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24524/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24525/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24526/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24527/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24528/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24529/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24530/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24531/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24532/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24533/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24534/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24535/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24536/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24537/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24538/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24539/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24540/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24541/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24542/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24543/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24544/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24545/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24546/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24547/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24548/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24549/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24550/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24551/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24552/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24553/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24554/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24555/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24556/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24557/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24558/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24559/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24560/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24561/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24562/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24563/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24564/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24565/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24566/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24567/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24568/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24569/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24570/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24571/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24572/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24573/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24574/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24575/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24576/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24577/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24578/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24579/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24580/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24581/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24582/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24583/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24584/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24585/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24586/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24587/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24588/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24589/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24590/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24591/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24592/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24593/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24594/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24595/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24596/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24597/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24598/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24599/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24600/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24601/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24602/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24603/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24604/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24605/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24606/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24607/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24608/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24609/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24610/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24611/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24612/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24613/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24614/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24615/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24616/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24617/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24618/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24619/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24620/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24621/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24622/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24623/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24624/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24625/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24626/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24627/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24628/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24629/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24630/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24631/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24632/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24633/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24634/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24635/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24636/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24637/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24638/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24639/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24640/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24641/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24642/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24643/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24644/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24645/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24646/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24647/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24648/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24649/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24650/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24651/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24652/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24653/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24654/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24655/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24656/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24657/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24658/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24659/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24660/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24661/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24662/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24663/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24664/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24665/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24666/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24667/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24668/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24669/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24670/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24671/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24672/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24673/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24674/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24675/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24676/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24677/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24678/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24679/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24680/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24681/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24682/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24683/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24684/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24685/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24686/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24687/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24688/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24689/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24690/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24691/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24692/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24693/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24694/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24695/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24696/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24697/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24698/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24699/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24700/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24701/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24702/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24703/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24704/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24705/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24706/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24707/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24708/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24709/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24710/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24711/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24712/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24713/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24714/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24715/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24716/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24717/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24718/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24719/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24720/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24721/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24722/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24723/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24724/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24725/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24726/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24727/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24728/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24729/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24730/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24731/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24732/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24733/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24734/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24735/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24736/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24737/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24738/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24739/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24740/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24741/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24742/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24743/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24744/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24745/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24746/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24747/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24748/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24749/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24750/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24751/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24752/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24753/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24754/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24755/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24756/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24757/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24758/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24759/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24760/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24761/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24762/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24763/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24764/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24765/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24766/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24767/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24768/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24769/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24770/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24771/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24772/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24773/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24774/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24775/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24776/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24777/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24778/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24779/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24780/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24781/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24782/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24783/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24784/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24785/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24786/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24787/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24788/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24789/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24790/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24791/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24792/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24793/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24794/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24795/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24796/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24797/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24798/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24799/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24800/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24801/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24802/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24803/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24804/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24805/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24806/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24807/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24808/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24809/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24810/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24811/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24812/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24813/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24814/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24815/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24816/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24817/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24818/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24819/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24820/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24821/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24822/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24823/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24824/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24825/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24826/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24827/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24828/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24829/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24830/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24831/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24832/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24833/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24834/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24835/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24836/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24837/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24838/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24839/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24840/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24841/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24842/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24843/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24844/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24845/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24846/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24847/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24848/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24849/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24850/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24851/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24852/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24853/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24854/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24855/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24856/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24857/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24858/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24859/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24860/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24861/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24862/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24863/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24864/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24865/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24866/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24867/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24868/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24869/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24870/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24871/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24872/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24873/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24874/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24875/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24876/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24877/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24878/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24879/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24880/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24881/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24882/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24883/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24884/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24885/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24886/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24887/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24888/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24889/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24890/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24891/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24892/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24893/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24894/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24895/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24896/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24897/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24898/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24899/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24900/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24901/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24902/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24903/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24904/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24905/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24906/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24907/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24908/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24909/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24910/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24911/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24912/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24913/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24914/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24915/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24916/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24917/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24918/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24919/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24920/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24921/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24922/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24923/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24924/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24925/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24926/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24927/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24928/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24929/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24930/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24931/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24932/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24933/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24934/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24935/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24936/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24937/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24938/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24939/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24940/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24941/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24942/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24943/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24944/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24945/25000, Training Loss: 0.000002, lr : 0.000146\n",
      "Epoch: 24946/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24947/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24948/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24949/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24950/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24951/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24952/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24953/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24954/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24955/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24956/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24957/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24958/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24959/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24960/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24961/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24962/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24963/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24964/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24965/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24966/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24967/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24968/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24969/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24970/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24971/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24972/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24973/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24974/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24975/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24976/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24977/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24978/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24979/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24980/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24981/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24982/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24983/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24984/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24985/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24986/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24987/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24988/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24989/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24990/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24991/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24992/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24993/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24994/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24995/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24996/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24997/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24998/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 24999/25000, Training Loss: 0.000001, lr : 0.000146\n",
      "Epoch: 25000/25000, Training Loss: 0.000001, lr : 0.000146\n"
     ]
    }
   ],
   "source": [
    "train_hist = []\n",
    "lr_hist = [] \n",
    "\n",
    "# defining the model, loss function, optimizer, and lr scheduler (same as before)\n",
    "model = RNN_net(n_layers, n_neurons,input_size)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=factor, patience=patience)\n",
    "\n",
    "# Moving to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Function to calculate the loss and learning rate value\n",
    "def evaluate_model(model, val_loader, criterion, optimizer):\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():  # No gradient calculation\n",
    "        for X_batch, Y_batch in val_loader:\n",
    "            X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, Y_batch)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "    # Save the learning rate value\n",
    "    lr_value = optimizer.param_groups[0]['lr']\n",
    "    return val_loss / len(val_loader), lr_value\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()  # Set to training mode\n",
    "    train_loss = 0\n",
    "    \n",
    "    for X_batch, Y_batch in train_loader:\n",
    "        # Move batch to device\n",
    "        X_batch, Y_batch = X_batch.to(device), Y_batch.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        # Loss calculation\n",
    "        loss = loss_fn(outputs, Y_batch)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    train_hist.append(train_loss)  \n",
    "    val_loss, lr_value = evaluate_model(model, val_loader, loss_fn, optimizer)\n",
    "    lr_hist.append(lr_value)  \n",
    "    scheduler.step(val_loss)  # adjust lr based on validation loss\n",
    "       \n",
    "    # Log the training progress\n",
    "    print(f'Epoch: {epoch+1}/{n_epochs}, Training Loss: {train_loss:.6f}, lr : {lr_value:.6f}')\n",
    "\n",
    "torch.save(model.state_dict(), '../models/DHOscillator_RNN.pt')\n",
    "history = {'train_loss': train_hist, 'lr': lr_hist}\n",
    "np.save('../history/RNN_history.npy', history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Load the training history\n",
    "history = np.load('../history/RNN_history.npy', allow_pickle=True).item()\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot the training loss\n",
    "plt.semilogy(history['train_loss'], label='Training Loss')\n",
    "\n",
    "# Plot the validation loss\n",
    "plt.semilogy(history['lr'], label='lr',color = 'r')\n",
    "\n",
    "# Additional plot settings\n",
    "plt.title('Training loss and lr Over Time')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Show the figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "model = RNN_net(n_layers, n_neurons,input_size)\n",
    "model.load_state_dict(torch.load('../models/DHOscillator_RNN.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Get predictions from the model\n",
    "Y_train_pred = model(X_train_batch)\n",
    "Y_test_pred = model(X_test_batch)\n",
    "\n",
    "# Extract the position and velocity from predictions\n",
    "# Now we will use the last step in the sequence for each sample for plotting\n",
    "Y_train_pred_position = Y_train_pred[:, -1, 0].detach().numpy()\n",
    "Y_train_pred_velocity = Y_train_pred[:, -1, 1].detach().numpy()\n",
    "Y_test_pred_position = Y_test_pred[:, 0, 0].detach().numpy()\n",
    "Y_test_pred_velocity = Y_test_pred[:, 0, 1].detach().numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Time axis for the last step of each sequence\n",
    "X_train_batch_np = X_train_batch.detach().numpy()\n",
    "time_steps_train = X_train_batch_np[:, -1 , 0]\n",
    "X_test_batch_np = X_test_batch.detach().numpy()\n",
    "time_steps_test = X_test_batch_np[:, 0, 0]\n",
    "\n",
    "print(Y_train_pred_position.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Now we have the predictions and can plot them alongside the true values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Position vs Time\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(X_train, Y_train[:, 0],'.',markersize = 2, label=\"Train True\")\n",
    "plt.plot(X_test, Y_test[:,0],'.',markersize = 2,   label=\"Test True\")\n",
    "plt.plot(time_steps_train, Y_train_pred_position,'.',markersize = 2,  label=\"Train Pred\")\n",
    "plt.plot(time_steps_test, Y_test_pred_position,'.', markersize = 2)#, label=\"Test Pred\")\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.title(\"Last Step Position\")\n",
    "plt.legend()\n",
    "\n",
    "# Velocity vs Time\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(X_train, Y_train[:, 1],'.',markersize = 2,  label=\"Train True\")\n",
    "plt.plot(X_test, Y_test[:, 1],'.', markersize = 2, label=\"Test True\")\n",
    "plt.plot(time_steps_train, Y_train_pred_velocity,'.', markersize = 2,  label=\"Train Pred\")\n",
    "plt.plot(time_steps_test, Y_test_pred_velocity,'.', markersize = 2,  label=\"Test Pred\")\n",
    "plt.xlabel(\"Time step\")\n",
    "plt.ylabel(\"Velocity\")\n",
    "plt.title(\"Last Step Velocity\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
